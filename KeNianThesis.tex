% uWaterloo Thesis Template for LaTeX
% Last Updated June 14, 2017 by Stephen Carr, IST Client Services
% FOR ASSISTANCE, please send mail to rt-IST-CSmathsci@ist.uwaterloo.ca

% Effective October 2006, the University of Waterloo
% requires electronic thesis submission. See the uWaterloo thesis regulations at
% https://uwaterloo.ca/graduate-studies/thesis.

% DON'T FORGET TO ADD YOUR OWN NAME AND TITLE in the "hyperref" package
% configuration below. THIS INFORMATION GETS EMBEDDED IN THE PDF FINAL PDF DOCUMENT.
% You can view the information if you view Properties of the PDF document.

% Many faculties/departments also require one or more printed
% copies. This template attempts to satisfy both types of output.
% It is based on the standard "book" document class which provides all necessary
% sectioning structures and allows multi-part theses.

% DISCLAIMER
% To the best of our knowledge, this template satisfies the current uWaterloo requirements.
% However, it is your responsibility to assure that you have met all
% requirements of the University and your particular department.
% Many thanks for the feedback from many graduates that assisted the development of this template.

% -----------------------------------------------------------------------

% By default, output is produced that is geared toward generating a PDF
% version optimized for viewing on an electronic display, including
% hyperlinks within the PDF.

% E.g. to process a thesis called "mythesis.tex" based on this template, run:

% pdflatex mythesis	-- first pass of the pdflatex processor
% bibtex mythesis	-- generates bibliography from .bib data file(s)
% makeindex         -- should be run only if an index is used
% pdflatex mythesis	-- fixes numbering in cross-references, bibliographic references, glossaries, index, etc.
% pdflatex mythesis	-- fixes numbering in cross-references, bibliographic references, glossaries, index, etc.

% If you use the recommended LaTeX editor, Texmaker, you would open the mythesis.tex
% file, then click the PDFLaTeX button. Then run BibTeX (under the Tools menu).
% Then click the PDFLaTeX button two more times. If you have an index as well,
% you'll need to run MakeIndex from the Tools menu as well, before running pdflatex
% the last two times.

% N.B. The "pdftex" program allows graphics in the following formats to be
% included with the "\includegraphics" command: PNG, PDF, JPEG, TIFF
% Tip 1: Generate your figures and photos in the size you want them to appear
% in your thesis, rather than scaling them with \includegraphics options.
% Tip 2: Any drawings you do should be in scalable vector graphic formats:
% SVG, PNG, WMF, EPS and then converted to PNG or PDF, so they are scalable in
% the final PDF as well.
% Tip 3: Photographs should be cropped and compressed so as not to be too large.

% To create a PDF output that is optimized for double-sided printing:
%
% 1) comment-out the \documentclass statement in the preamble below, and
% un-comment the second \documentclass line.
%
% 2) change the value assigned below to the boolean variable
% "PrintVersion" from "false" to "true".

% --------------------- Start of Document Preamble -----------------------

% Specify the document class, default style attributes, and page dimensions
% For hyperlinked PDF, suitable for viewing on a computer, use this:
\documentclass[letterpaper,12pt,titlepage,oneside,final]{book}

% For PDF, suitable for double-sided printing, change the PrintVersion variable below
% to "true" and use this \documentclass line instead of the one above:
%\documentclass[letterpaper,12pt,titlepage,openright,twoside,final]{book}

% Some LaTeX commands I define for my own nomenclature.
% If you have to, it's better to change nomenclature once here than in a
% million places throughout your thesis!
\newcommand{\package}[1]{\textbf{#1}} % package names in bold text
\newcommand{\cmmd}[1]{\textbackslash\texttt{#1}} % command name in tt font
\newcommand{\href}[1]{#1} % does nothing, but defines the command so the
    % print-optimized version will ignore \href tags (redefined by hyperref pkg).
%\newcommand{\texorpdfstring}[2]{#1} % does nothing, but defines the command
% Anything defined here may be redefined by packages added below...

% This package allows if-then-else control structures.
\usepackage{ifthen}
\newboolean{PrintVersion}
\setboolean{PrintVersion}{false}
% CHANGE THIS VALUE TO "true" as necessary, to improve printed results for hard copies
% by overriding some options of the hyperref package below.

%\usepackage{nomencl} % For a nomenclature (optional; available from ctan.org)
\usepackage{amsmath,amssymb,amstext} % Lots of math symbols and environments
\usepackage[pdftex]{graphicx} % For including graphics N.B. pdftex graphics driver

% Hyperlinks make it very easy to navigate an electronic document.
% In addition, this is where you should specify the thesis title
% and author as they appear in the properties of the PDF document.
% Use the "hyperref" package
% N.B. HYPERREF MUST BE THE LAST PACKAGE LOADED; ADD ADDITIONAL PKGS ABOVE
\usepackage[pagebackref=false]{hyperref} % with basic options
		% N.B. pagebackref=true provides links back from the References to the body text. This can cause trouble for printing.
\hypersetup{
    plainpages=false,       % needed if Roman numbers in frontpages
    unicode=false,          % non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={uWaterloo\ LaTeX\ Thesis\ Template},    % title: CHANGE THIS TEXT!
%    pdfauthor={Author},    % author: CHANGE THIS TEXT! and uncomment this line
%    pdfsubject={Subject},  % subject: CHANGE THIS TEXT! and uncomment this line
%    pdfkeywords={keyword1} {key2} {key3}, % list of keywords, and uncomment this line if desired
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=blue,         % color of internal links
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}
\ifthenelse{\boolean{PrintVersion}}{   % for improved print quality, change some hyperref options
\hypersetup{	% override some previously defined hyperref options
%    colorlinks,%
    citecolor=black,%
    filecolor=black,%
    linkcolor=black,%
    urlcolor=black}
}{} % end of ifthenelse (no else)

% package adeed
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{fullpage}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage[figuresright]{rotating}
\usepackage[mathscr]{euscript}
\usepackage{amstext}
\usepackage{tabularx}
\usepackage[numbers]{natbib}
\usepackage{lineno,xcolor}
\usepackage{listings}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{fancybox}
\usepackage{listings}
\usepackage{url}
\usepackage{epstopdf}
\usepackage[lined,ruled]{algorithm2e}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{threeparttable}
\numberwithin{equation}{section}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
%\newtheorem{pf}{Proof}

\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
\newcommand*{\Resize}[2]{\resizebox{#1}{!}{$#2$}}%
\newcommand{\model}{\textsc{GRU}_\delta}
\newcommand{\modelT}{\textsc{GRU}_{\textsc{total}}}
\newcommand{\modelN}{\textsc{NN}_\delta}
\newcommand{\vt}[1]{\mathbf{#1}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vb}{\mathbf{b}}
%\newcommand{\pm}{\stackrel{+}{-}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vi}{\mathbf{in}}
\newcommand{\vo}{\mathbf{o}}
\newcommand{\vxt}{\tilde{\mathbf{x}}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\impsigma}{\breve{\sigma}}
\newcommand{\barK}{\overline{K}}
\newcommand{\barC}{\overline{C}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\fnp}{\tilde{f}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\HK}{\mathcal{H}_K}
\newcommand{\XS}{\mathcal{X}}
\newcommand{\DS}{\Delta S}
\newcommand{\Heston}{\textsc{Heston}}
\newcommand{\DVmkt}{\Delta V^{mkt}}
\newcommand{\DT}{\Delta t}
\newcommand{\vuu}{\mathbf{\widetilde u}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\vdot}[2]{{#1}^T{#2}}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\newcommand{\sym}{\textsc{sym}}
\newcommand{\BS}{\textsc{BS}}
\newcommand{\LOF}{\textsc{lof}}
\newcommand{\svm}{\textsc{svm}}
\newcommand{\AMflag}{\text{mFLAG}}
\newcommand{\rw}{\textsc{rw}}
\newcommand{\diag}{\textsc{diag}}
\newcommand{\sign}{\textsc{sign}}
\newcommand{\MeanAbs}{\E(|\DVmkt-\DS \delta |)}
\newcommand{\Cluster}{\textsc{C}}
\newcommand{\bi}{\text{bi}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\valpha}{\pmb{\widehat{\alpha}}}
\newcommand{\vK}{\mathbb{K}}
\newcommand{\vV}{\pmb{\breve{V}}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\vol}{\upsilon}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vW}{\pmb{W}}
\newcommand{\vU}{\pmb{U}}
\newcommand{\np}{\text{np}}
\newcommand{\pt}{^{+\Delta t}}
\newcommand{\norm}{\text{norm}}
\newcommand{\row}{\text{row}}
\newcommand{\Vmkt}{V^{mkt}}
\newcommand{\Smkt}{S}
\newcommand{\vecVmkt}{\mathbb{V}}
\newcommand{\Ncut}{\text{Ncut}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\DKLs}{\bf\textsc{DKL}_{\text{SPL}}}
\newcommand{\DRNNc}{\bf\textsc{DRNN}_{\text{C}}}
\newcommand{\DKLg}{\bf\textsc{DKL}_{\text{RBF}}}
\newcommand{\IKLs}{\bf\textsc{IKL}_{\text{SPL}}}
\newcommand{\IKLg}{\bf\textsc{IKL}_{\text{RBF}}}
\newcommand{\LVF}{\textsc{LVF}}
\newcommand{\Del}{\delta_{\textsc{BS}}}
\newcommand{\SABR}{\bf\textsc{SABR}_{\text{MV}}}
\newcommand{\MV}{\bf \textsc{MV}}

\usepackage{tikz}
\usetikzlibrary{shapes}
\usetikzlibrary{arrows}
\usetikzlibrary{positioning, fit, arrows.meta}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{xparse}
\usetikzlibrary{calc}
\usepackage{tkz-graph}
\usetikzlibrary{backgrounds,automata}
\usepackage{atbegshi}% http://ctan.org/pkg/atbegshi
%\AtBeginDocument{\AtBeginShipoutNext{\AtBeginShipoutDiscard}}



\usepackage[automake,toc,abbreviations]{glossaries-extra} % Exception to the rule of hyperref being the last add-on package
% If glossaries-extra is not in your LaTeX distribution, get it from CTAN (http://ctan.org/pkg/glossaries-extra),
% although it's supposed to be in both the TeX Live and MikTeX distributions. There are also documentation and
% installation instructions there.

% Setting up the page margins...
% uWaterloo thesis requirements specify a minimum of 1 inch (72pt) margin at the
% top, bottom, and outside page edges and a 1.125 in. (81pt) gutter
% margin (on binding side). While this is not an issue for electronic
% viewing, a PDF may be printed, and so we have the same page layout for
% both printed and electronic versions, we leave the gutter margin in.
% Set margins to minimum permitted by uWaterloo thesis regulations:
\setlength{\marginparwidth}{0pt} % width of margin notes
% N.B. If margin notes are used, you must adjust \textwidth, \marginparwidth
% and \marginparsep so that the space left between the margin notes and page
% edge is less than 15 mm (0.6 in.)
\setlength{\marginparsep}{0pt} % width of space between body text and margin notes
\setlength{\evensidemargin}{0.125in} % Adds 1/8 in. to binding side of all
% even-numbered pages when the "twoside" printing option is selected
\setlength{\oddsidemargin}{0.125in} % Adds 1/8 in. to the left of all pages
% when "oneside" printing is selected, and to the left of all odd-numbered
% pages when "twoside" printing is selected
\setlength{\textwidth}{6.375in} % assuming US letter paper (8.5 in. x 11 in.) and
% side margins as above
\raggedbottom

% The following statement specifies the amount of space between
% paragraphs. Other reasonable specifications are \bigskipamount and \smallskipamount.
\setlength{\parskip}{\medskipamount}

% The following statement controls the line spacing.  The default
% spacing corresponds to good typographic conventions and only slight
% changes (e.g., perhaps "1.2"), if any, should be made.
\renewcommand{\baselinestretch}{1} % this is the default line space setting

% By default, each chapter will start on a recto (right-hand side)
% page.  We also force each section of the front pages to start on
% a recto page by inserting \cleardoublepage commands.
% In many cases, this will require that the verso page be
% blank and, while it should be counted, a page number should not be
% printed.  The following statements ensure a page number is not
% printed on an otherwise blank verso page.
\let\origdoublepage\cleardoublepage
\newcommand{\clearemptydoublepage}{%
  \clearpage{\pagestyle{empty}\origdoublepage}}
\let\cleardoublepage\clearemptydoublepage

% Define Glossary terms (This is properly done here, in the preamble. Could be \input{} from a file...)
% Main glossary entries -- definitions of relevant terminology
\newglossaryentry{computer}
{
name=computer,
description={A programmable machine that receives input data,
               stores and manipulates the data, and provides
               formatted output}
}

% Nomenclature glossary entries -- New definitions, or unusual terminology
\newglossary*{nomenclature}{Nomenclature}
\newglossaryentry{dingledorf}
{
type=nomenclature,
name=dingledorf,
description={A person of supposed average intelligence who makes incredibly brainless misjudgments}
}

% List of Abbreviations (abbreviations type is built in to the glossaries-extra package)
\newabbreviation{aaaaz}{AAAAZ}{American Association of Amature Astronomers and Zoologists}

% List of Symbols
\newglossary*{symbols}{List of Symbols}
\newglossaryentry{rvec}
{
name={$\mathbf{v}$},
sort={label},
type=symbols,
description={Random vector: a location in n-dimensional Cartesian space, where each dimensional component is determined by a random process}
}

\makeglossaries

%======================================================================
%   L O G I C A L    D O C U M E N T -- the content of your thesis
%======================================================================
\begin{document}

% For a large document, it is a good idea to divide your thesis
% into several files, each one containing one chapter.
% To illustrate this idea, the "front pages" (i.e., title page,
% declaration, borrowers' page, abstract, acknowledgements,
% dedication, table of contents, list of tables, list of figures,
% nomenclature) are contained within the file "uw-ethesis-frontpgs.tex" which is
% included into the document by the following statement.
%----------------------------------------------------------------------
% FRONT MATERIAL
%----------------------------------------------------------------------
\input{uw-ethesis-frontpgs}

%----------------------------------------------------------------------
% MAIN BODY
%----------------------------------------------------------------------
% Because this is a short document, and to reduce the number of files
% needed for this template, the chapters are not separate
% documents as suggested above, but you get the idea. If they were
% separate documents, they would each start with the \chapter command, i.e,
% do not contain \documentclass or \begin{document} and \end{document} commands.
%======================================================================
\chapter{Introduction}
Options hedging is a critical problem in financial risk management. The state-of-art approaches in financial derivative pricing and hedging rely heavily on parametric assumptions describing the dynamics of underlying asset .  The common practice is to calibrate an option pricing function based on the specific parametric model and compute various sensitivities to hedge the option risk. For example, the sensitivity of the option value function to the underlying price is used in delta hedging. Ideally, the value of an option written on the underlying asset can be perfectly replicated by a hedging portfolio consisting of the underlying asset and the  risk-free asset, when the market is complete \cite{shreve2004stochastic}.  In practice, such perfect scenarios does not exist and we have to rebalance the hedging portfolio discretely instead of continuously due to the existence of the transaction cost.  The practice of adjusting the hedging portfolio discretely is often referred to as discrete hedging.

There are many parametric models proposed to describe the dynamics of underlying asset.
The original and most celebrated parametric  Black-Scholes (BS) model uses a constant volatility \citep{black1973pricing,merton1973theory}, which is  shown
 to produce inaccurate option prices for deeply out-of-the-money options and deeply in-the-money options \citep{genccay2003degree}.
In addition,
the BS model is unable to capture the non-zero correlation between the volatility and the underlying asset price \citep{french1987expected,bollerslev2006leverage}.
 The practitioner's BS delta hedging approach sets the constant volatility in the BS model to the implied volatility calibrated to the market price at the time of re-balancing.  Many alternative parametric models have been proposed to improve the BS model,  including  the Stochastic Volatility (SV) model \cite{hagan2002managing,heston1993closed,hull1987pricing,bakshi1997empirical},  the Local Volatility Function (LVF) model \citep{coleman2001,dumas1998implied,rubinstein1994implied,dupire1994pricing}, and the jump model \citep{He06,kou2002jump}. Unfortunately, all models have been shown to have their limitations in accurately modeling option market prices.

Errors in  the option value model have significant implications in hedging. Consider, for example, when the hedging position is computed from the sensitivity of the  option value function calibrated at the hedging time, the computed hedging position only depends on the assumed underlying price model and the current market option prices. Unless the assumed model for the underlying price is exact and all assumptions that results in the option pricing function are all valid, the option function calibrated at the hedging time cannot predict the future option market price.

Specifically, assume that $V(S,t,T,K,r,q;\theta^*)$ is  the option value function and $\theta^*$ is the vector of model parameters of the assumed option pricing model and, at the hedging time $t$,
option calibration ensures that
\begin{equation} \label{eq:imp}
V(S,t,T,K,r,q; \theta^*)=V^{mkt}_{t,T,K}
\end{equation}
where $\Vmkt$ denotes the actual market option price,  $t$ is the current trading time,  $K$  is  strike price,  $T$ is the expiry time, and $S$ is the underlying price input used in the option pricing function. We use $V^{mkt}_{t,T,K}$ to indicate it is the market price at time $t$ for  the expiry time $T$ and strike $K$.
The option value function $V(S,t,T,K,r,q; \theta^*)$, calibrated at the hedging time $t$,  does not ensure that
 $
\frac{d V}{d S}
$
equals to
$
\frac{d \Vmkt}{d S},
$
which is indeed unknown.
Furthermore, $
\frac{d V}{d S}
$ is difficult to be computed exactly due to the dependence of the calibrated model parameter on the underlying price \citep{knian2017,coleman2001,hulloptimal}. The missing sensitivity $
\frac{\partial \theta^*}{\partial S},
$ is difficult to account for and is often ignored, though
for some models, corrections have been proposed to account for the dependence \citep{hulloptimal,hagan2017bartlett,bartlett2006hedging}.
%Given that it is unlikely that any parametric model can describe the underlying dynamics perfectly for option pricing, hedging positions computed by taking the derivative of the calibrated option value function inevitably suffer from the model parameter dependence issue.


Since machine learning algorithms usually do not impose assumptions on the model to be learned, they have recently been adopted to determine an option value function directly from the market data, with the goal of avoiding the model misspecification issues from the parametric modeling approach e.g.,\citep{gradojevic2009option,garcia2000pricing,hutchinson}.
Unfortunately, using nonparametric learning, hedging positions still need to be computed from the sensitivity of the model value function.
While no assumption is explicitly made for the  dynamics of underlying asset, the option value function is determined by data through cross-validation, leading to training errors.
Since there is no assurance that the  sensitivity of the learned option value function  with regards to underlying asset matches that the sensitivity of the market option price,
the parameters of the model learned directly from data can similarly exhibit dependence on the underlying price. When the hedging position is computed from the partial derivative of the data-driven option value function, e.g.,  \citep{hutchinson}, this dependence cannot be accounted for and again is ignored. Hence, option hedging risk remains insufficiently minimized.

Furthermore, using option delta $\frac{\partial V}{\partial S}$ as the hedging position becomes inadequate when discrete hedging is performed in practice, particularly when rebalancing becomes infrequent.  Instead, optimal discrete hedging strategy can be determined by determining hedging strategy directly using an appropriate objective in the discrete hedging context, e.g., minimizing the variance of the hedging error, needs to be chosen \citep{hulloptimal,Angelini10,Goutte13}.

In hedging, the ultimate goal is to discover a hedging strategy  which minimizes the hedging error which is  measured by the market option and underlying prices. With the increasing availability of market option prices, a timely question arises: is it possible to learn optimal hedging positions directly from market option price and underlying price data?
Up until now, research in learning the hedging position  directly from market data is scarce.
Recently, a data-driven approach  \citep{hulloptimal} is proposed to learn a parametric model for the minimum variance delta hedging based on the analysis of the BS option greeks and underlying market prices. However, the proposed parametric model focuses on the instantaneous hedging error analysis in a parametric model framework.


In this thesis, we study the discrete option hedging problem by  explicitly focusing on the issues arising from  model specification errors and model parameter dependence.
We illustrate that the inability to minimize variance of the hedging error, when determining hedging position from option value function from a parametric model, is also shared by an option model estimated from a nonparametric method. Although a nonparametric modeling approach to option value can potentially lead to smaller mis-specification error, we illustrate that non-parametric model parameters can similarly depend on the underlying. Consequently the sensitivity of the optimally estimated option value function will not lead to minimization of option hedging risk.
Furthermore,  the estimated pricing function inevitably has errors, due to both model mis-specification, discretization, and numerical roundoff errors.
The error in the value function can potentially be substantially magnified in computing partial derivatives as hedging positions.

We explore several direct market data-driven approaches to bypass  challenges mentioned above to achieve effective hedging performance.
We first propose a  data-driven kernel learning approach \cite{knian2017} to learn a local risk minimization hedging model directly from the market data observed at the hedging time $t$.  We learn a  hedging function from the market data by minimizing  the  empirical local hedging  risk with a suitable regularization. The local risk corresponds directly to the variance of the hedging error in the discrete rebalancing period. A novel encoder-decoder  RNN model $\model$ \cite{knian2019}, to extract both sequential and local features at hedging time $t$ from market prices, is later proposed to learn option hedging positions directly from the market. We include a feature weighting procedure to select the most relevant local features at hedging time $t$ and sequential features for the sequential data-driven model $\model$. Lastly, in order to deal with multi-steps discrete total hedging scenarios where we hedge until the expiry of the option \cite{knian2020}, we enhance our sequential local hedging model $\model$ to be $\modelT$. We compared our data-driven approaches with the parametric approaches and demonstrate the effectiveness of the data-driven hedging models in terms of both local hedging risk and total hedging risk.





%======================================================================
%----------------------------------------------------------------------
\section{Contribution}
The contributions with respect to the data-driven kernel hedging model \cite{knian2017} are summarized below:
\begin{itemize}
\item We analyze and discuss implications from model mis-specification in the option value function for discrete option hedging. We illustrate challenges in accounting for the dependence of the calibrated model parameters on the underlying, which arises due to  model mis-specification.

\item We analyze a regularized kernel network for option value estimation and illustrate that  the partial derivative of the estimated value function with respect to the underlying similarly does not minimize variance of the hedging risk in general, even infinitesimally.

\item We propose a data-driven approach to learn a hedging position function directly by minimizing the variance of the local hedging  risk.
    Specifically we implement a regularized spline kernel method $\DKLs$ to nonparametrically estimate the hedging function from the market data.

\item Using synthetic data sets, we compare daily, weekly, and monthly hedging performance using
   the kernel direct data-driven hedging approach with the performance of the indirect approach where hedging positions are computed from the sensitivity of the nonparametric option value function. In particular, we present computational results which demonstrate that the direct spline kernel hedging position learning outperforms the hedging position computed from the sensitivity of the spline kernel option value function.

\item Using  S\&P 500 index option market data for more than a decade ending in August 31, 2015, we demonstrate that the  daily hedging performance of the direct spline kernel hedging function learning method   surpasses that of the   minimum variance quadratic hedging formula proposed in \citep{hulloptimal},  as well as corrective methods based on LVF and SABR implemented in  \citep{hulloptimal}.

\item We also present weekly and monthly hedging results using the  S\&P 500 index option market data and demonstrate significant enhanced performance over the BS implied volatility hedging.
\end{itemize}

The contributions with respect to the data-driven sequential hedging model \cite{knian2019} are summarized below:
\begin{itemize}
	\item  We propose a novel encoder-decoder  RNN model, to extract both sequential and current features from market prices, to learn option hedging positions directly from the market. We include a feature weighting procedure to select the most relevant local features and sequential time series features for the data-driven model.
	\item To ensure robust learning, we use the Huber loss function as the learning objective,  adaptively setting the error resolution parameter to the Black–Scholes hedging error, allowing it to varying from data instance to data instance. Furthermore,    the proposed $\model$ can be updated more frequently than the data-driven model in \cite{knian2017} to account for the market shifts.
	\item Using the S\&P 500 index option market data from January 2, 2004 to  August 31st, 2015, we demonstrate that the weekly and monthly hedging performance of the proposed $\model$ significantly surpasses that
 of the data-driven minimum variance (MV) method in \citep{hulloptimal}, the regularized kernel data-driven model \citep{knian2017}, and the SABR-Bartlett method \cite{bartlett2006hedging}.
	\item Using the S\&P 500 index option market data from January 2, 2004 to  August 31st, 2015, we demonstrate that
	the daily hedging performance of the proposed $\model$ surpasses that of the minimum variance quadratic hedging method  proposed in \cite{hulloptimal}, the corrective methods
	based on LVF and SABR implemented in \cite{hulloptimal},the SABR-Bartlett method \cite{bartlett2006hedging}, as well as the data-driven model in \cite{knian2017}.
	\item To motivate the roles of each major component of the proposed $\model$, we demonstrate performance sensitivity through computational experiments. In addition, we illustrate and analyze the relative importance of selected features.
\end{itemize}


The contributions with respect to the data-driven total hedging model \cite{knian2020} are summarized below:
\begin{itemize}
	\item  We enhance the sequential data-driven local hedging model $\model$ \cite{knian2019} to cope with total hedging scenarios where we rebalance multiple times until the expiries of the options.
	\item  We augment the market data using SABR model and Local Volatility Function to cope with the challenges of lacking market option data.
	\item  Using the S\&P 500 index call option market data from January 2, 1996 to  August 31st, 2015, we demonstrate that the weekly, bi-weekly and monthly hedging performance of the proposed total hedging model $\modelT$  surpasses that of sequential data-driven local hedging model $\model$ \cite{knian2019} and the SABR-Bartlett method \cite{bartlett2006hedging}, when the hedging performance is evaluated on the expiries of the option.
	\item We compare  $\modelT$ based on market option information with $\modelT$ based on purely underlying asset information and indicate the importance of  market option information in determining the total hedging position.
\end{itemize}
%----------------------------------------------------------------------
\section{Outline}
The remainder of the thesis is organized as follows.
Chapter \ref{sec:Background}  reviews basic concept of derivative pricing models, discrete hedging problems, local and total hedging risk and various existing parametric approaches to hedge options.
Chapter \ref{sec:kernel}  discusses the kernel learning framework and introduces the data-driven kernel local hedging model $\DKLs$. 
Chapter \ref{sec:RNNLocal} discusses the Recurrent Neural Network(RNN) framework and introduces the data-driven sequential local hedging model $\model$.
Chapter \ref{sec:LocalComparison} discuss the  empirical results  from  the data-driven sequential local hedging model $\model$ and data-driven kernel local hedging model $\DKLs$.

Chapter \ref{sec:RNNTotal} introduces the data-driven sequential total hedging model $\modelT$ and presents the empirical comparisons between local hedging model $\model$ and total hedging model $\modelT$.
Chapter \ref{sec:RNNTotal}  also discusses the challenges of using market data to build data-driven total hedging models and the data augmentation procedure to cope with the challenges.
We conclude in Chapter \ref{sec:Conclusion}  with summary remarks and potential extensions.
%======================================================================
\chapter{Background}
\label{sec:Background} 
In this chapter, we review the basic concept of option pricing models and discuss the problem of pricing model parameters dependence on underlying asset.  In addition, we specify the discrete hedging problem and define the total and local hedging risk. Most of the discussion in this chapter are drawn from \cite{shreve2004stochastic,heston1993closed,bartlett2006hedging,hagan2017bartlett,hagan2002managing}.
%======================================================================
\section{Option Pricing Model}
\label{sec:pricing}
\subsection{Black-Scholes Model}
\label{sec:bs}
A European style call or put option gives its buyer the right to buy the underlying asset on the option expiry with a strike price. Let the strike price be $K$ and  the $S_T$ be the underlying price at expiry $T$. The payoff of call options is :
\[
\max(S_T-K,0)
\]
The payoff of put options is:
\[
\max(K-S_T,0)
\]

Black and Scholes \cite{black1973pricing} drive the famous closed-form pricing formula for European options. They show that one can construct a riskless portfolio consisting of one option and shares of the underlying asset. The riskless portfolio needs to be continuously adjusted so that the number of shares always equal to the partial derivative of the option pricing function with regards to the underlying asset. No-arbitrage condition implies that the the return of the riskless portfolio must be equal to the risk-free interest rate. This leads to the renowned Black-Scholes (BS) partial differential equation and the closed-form pricing formula.

More specifically, under BS model, it is assumed that the underlying asset price follows a geometric Brownian motion:
\[
dS_t=\mu S_t dt+\sigma S_t dW_t
\]
where $W_t$ is a standard Brownian motion, $\mu$ is the constant drift rate of  the asset and $\sigma$ is the constant volatility of the asset. We can easily show that:
\[
S_t=S_0 e^{(\mu-\frac{\sigma^2}{2})t+\sigma W_t}
\]
Let $C_{BS}$ be the option value function for call option which depends on $t$ and $S$. For notational simplicity, we have:
\[
   \frac{\partial C_{BS}}{\partial t}=\frac{\partial C_{BS}}{\partial t}(t,S_t) \;\;,
   \frac{\partial C_{BS}}{\partial S_t}=\frac{\partial C_{BS}}{\partial S}(t,S_t) \;\;,
   \frac{\partial^2 C_{BS}}{\partial S_t^2}=\frac{\partial^2 C_{BS}}{\partial S^2}(t,S_t) \;\;,
   C^{BS}_t=C_{BS}(t,S_t)
\]
Follows Ito's Lemma \cite{shreve2004stochastic},we have:
\[
dC^{BS}_t=\left(\frac{\partial C_{BS}}{\partial t}+\mu S_t \frac{\partial C_{BS}}{\partial S_t}+\frac{1}{2} \sigma^2 S_t^2 \frac{\partial ^2 C_{BS}}{ \partial S_t^2}\right) dt+\sigma S_t \frac{\partial C_{BS}}{\partial S_t}dW_t
\]
 Now consider a specific portfolio, called the delta-hedge portfolio, consisting of being short one call option and long $\frac{\partial C_{BS}}{\partial S_t}$ shares at time $t$.
The total value of the delta-hedge portfolio $P^{\delta}$ at time $t$ is:
\[
P^{\delta}_t=-C^{BS}_t+S_t \frac{\partial C_{BS}}{\partial S_t}
\]
The instantaneous profit or loss is:
\[
dP^{\delta}_t=-\left(\frac{\partial C_{BS}}{ \partial t} +\frac{1}{2} \sigma^2 S_t^2 \frac{\partial^2 C_{BS}}{ \partial S_t^2 } \right) dt
\]
Assume there is a riskless asset with constant rate of return $r$, which is also known as risk-free interest rate. We can see that the delta-hedge portfolio $P_{\delta}$ is also riskless because the diffusion term associated with $dW_t$ is dropped. Under no-arbitrage condition, two riskless investment must earn the same rate of return so we must have:
\[
dP^{\delta}_{t}=rP^{\delta}_tdt
\]
This leads to the Black-Scholes partial differential equation:
\begin{equation}
\frac{\partial C_{BS}}{\partial t}+\frac{1}{2}\sigma^2 S_t^2 \frac{\partial^2 C_{BS}}{\partial S_t^2}+rS \frac{\partial C_{BS}}{\partial S_t}-rC^{BS}_t=0
\label{eq:bspde}
\end{equation}
The solution with European call option  is the well-known Black-Scholes pricing formula:
\begin{equation}
Call=S  \mathcal{N}(d_1)-e^{-r(T-t)}  K  \mathcal{N}(d_2)
\label{eq:bs}
\end{equation}
where $\mathcal{N}$ is the cumulative density function of the standard normal distribution
\[
d_1=\frac{\ln(S/K)+(r+\sigma^2/2)(T-t)}{\sigma \sqrt{T-t}}, \;\; d_2=d_1-\sigma  \sqrt{T-t}
\]
Similarly, we can derive the Black-Scholes pricing formula for  European put option to be:
\begin{equation}
Put=e^{-r(T-t)}  K   \mathcal{N}(-d_2)-S  \mathcal{N}(-d_1)
\label{eq:bsP}
\end{equation}


\iffalse
Another relevant portfolio is the replicating portfolio.  
We create a portfolio where we hold some shares $\delta$ of the underlying and borrow or loan money from the risk-free asset. The value in the risk-free asset is denoted as $B(t)$. 
\[
    V_{rep}(t,S_t)=\delta S_t+B(t)
\]
We continuously adjust the shares in underlying asset to always hold $\delta=\frac{\partial C}{\partial S}(t,S_t)$ shares at time $t$ 
and adopt a self-financing trading strategy, where any required cash is borrowed, and any excess cash is loaned such that at any time $t$:
\[
    V_{rep}(t,S_t)=V_{BS}(t,S_t)
\]
In other words,  the risk-free asset account is dynamically set to be:
\[
B(t)=\frac{\partial C}{\partial S}(t,S_t) S_t-V_{BS}(t,S_t)
\]
Under Black–Scholes model, we can dynamically create an portfolio with risk-free asset and underlying asset to replicate the value of option at any time $t$.
\fi

Alternatively, we can derive the Black-Scholes formula under the risk-neutral pricing framework. As the name suggests, under a risk-neural measure $Q$, all agents in the economy are neutral to risk, so that they are indifferent between investments with different risk as long as these investments have the same expected return. Under a risk-neutral measure, all tradable assets should have the same expected rate of return as the risk-free asset, which earns the risk-free interest rate $r$. The derivative price can thus derived from the expected payoff, discounted back to the current time at the risk-free rate $r$. It can been shown that \cite{shreve2004stochastic}, following the Black-Scholes assumption, there is a unique risk-neutral probability measure $Q$ that is equivalent to the actual physical probability measure. Under risk-neutral pricing framework, we have:
\begin{equation}
Call=e^{-r(T-t)} E^Q[\max(S_T-K,0)]
\label{eq:bs2}
\end{equation}
\begin{equation}
Put=e^{-r(T-t)} E^Q[\max(K-S_T,0)]
\label{eq:bs2P}
\end{equation}
where $E^Q[\cdot]$ is the expectation under the risk-neutral measure $Q$.
More specifically, define $\Theta_1$ to be the market price of risk:
\begin{equation}
\Theta_1=\frac{\mu-r}{\sigma}
\label{eq:price-risk}
\end{equation}
We change the original Brownian motion $dW_t$ to $\hat{dW_t}$ with
\begin{equation}
\hat{dW_t}=dW_t+\Theta_1 dt
\label{eq:riskS}
\end{equation}
The underlying dynamic will have the drift to be the risk-free interest rate $r$.
\[
dS_t=r S_t dt+\sigma S_t \hat{dW_t}
\]
It can be shown that $\hat{dW_t}$ is the Brownian motion under the risk-neutral measure $Q$ defined through Radon-Nikodym derivative via Girsanov's theorem \cite{shreve2004stochastic}. Using the fact that the drift rate of underlying asset dynamic under risk-neutral measure $Q$ is $r$, following \eqref{eq:bs2} and \eqref{eq:bs2P}, we can arrive at the same pricing formula as \eqref{eq:bs} and \eqref{eq:bsP}. Interest reader can refer to \cite{shreve2004stochastic} for more details about risk-neutral pricing and change from physical measure to risk-neutral measure $Q$.

Since actual drift $\mu$ is irrelevant in determining the option price under Black-Scholes framework, in this thesis, we assume we are dealing with risk-neutral measure $Q$ and the drift of underlying dynamic is always the risk-free interested $r$. 
Lastly, to include the effect of dividend, we can rewrite the $d_1$ and $d_2$ in the Black-Scholes pricing formula as:
\[
d_1=\frac{\ln(S/K)+((r-q)+\sigma^2/2)(T-t)}{\sigma \sqrt{T-t}}, \;\; d_2=d_1-\sigma  \sqrt{T-t}
\]
where $q$ is the given annual dividend yield. In this thesis, we use $V_{BS}(S,t,T,K,r,q;\sigma)$ to denote the European Black-Scholes pricing function regardless of the call or put nature.



Although, Black-Scholes framework provides a nice close-form formula, the real markets are never as ideal as the assumption of Black-Scholes model. Empirical  evidence indicates markets often violate the assumption of   Black-Scholes model. The two major aspects that has been criticized about Black-Scholes model are:
\begin{enumerate}
\item The constant volatility does not hold in real market. In practice, the implied volatility $\sigma^{imp}$, which equates the Black-Scholes option price $V_{BS}(S,t,T,K,r,q;\sigma)$ to market option price $V^{mkt}_{t,T,K}$, is often used to make sure that Black-Scholes price match the market observation. However, one can often find that the implied volatility $\sigma^{imp}$ tends to differ across different strikes and expiries. This breaks down the assumption of a constant volatility
\item Transaction cost exists in real market. Due to the existence of transaction cost, continuously adjusting the shares of underlying can be prohibitively expensive. Therefore, the argument of the Black-Scholes theory falls apart.
\end{enumerate}
The market deviations from the assumption of Black-Scholes model  motivates people to propose various approaches for relaxing the assumptions of Black and Scholes. These attempts include, but not limited to, local volatility models \citep{coleman2001,dumas1998implied,rubinstein1994implied,dupire1994pricing}, stochastic volatility models \cite{hagan2002managing,heston1993closed,hull1987pricing,bakshi1997empirical}, jump diffusion models \citep{He06,kou2002jump} and nonparametric pricing models based on regression \citep{yao2000option,bennell2004black,gradojevic2009option,garcia2000pricing,malliaris1993neural}. In the following section, we discuss two stochastic volatility models relevant to this thesis, Heston model and SABR model, which provide efficient closed-form solutions for the option price similar as Black-Scholes model.
\subsection{Heston Model}
\label{sec:heston}
Heston \cite{heston1993closed} proposed a version of the stochastic volatility model, which has become quite popular to model the volatility smiles.  The volatility smiles refer to the phenomenon that the options in real market whose strike prices differ substantially from the underlying asset price tend to have higher implied volatilities than options whose strike prices are close to the underlying asset price under Black–Scholes models. One of the key reason for its popularity is that European call and put option under Heston model have closed-form solution which makes the calibration of the model computationally efficient and accurate.
The Heston model assumes that the underlying, $S_t$ follows a Black-Scholes type stochastic process, but with a stochastic variance $\upsilon_t$ that follows a Cox, Ingersoll, Ross (CIR) process \cite{cox2005theory}.
\[
\begin{split}
dS_t&=\mu S_t dt + \sqrt{\upsilon} S_t dW_t\\
d\upsilon_t&=\kappa(\overline{\upsilon}-\upsilon_t)dt+\eta \sqrt{\upsilon_t}dZ_t\\
E[dZ_tdW_t]&=\rho dt
\end{split}
\]
These parameters are described as follows:
\begin{itemize}
  \item $\mu$ is the drift coefficient of the underlying asset
  \item $\overline{\upsilon}$ is the long term mean of variance
  \item $\kappa$ is the rate of mean reversion
  \item $\eta$ is the volatility of volatility
  \item $S_t$ is the underlying asset price
  \item $\upsilon_t$ is the  instantaneous variance
  \item $W_t$ and $Z_t$ are correlated Wiener process with correlation coefficient $\rho$
\end{itemize}
Similarly, the Heston dynamics can be transformed to be under a risk-neutral measure $Q$.
Heston \cite{heston1993closed} assumes that the market price of volatility risk is  proportional to the volatility $\sqrt{\upsilon_t}$:
\begin{equation}
\Theta_{2}=\frac{\lambda}{\eta} \sqrt{\upsilon_t}
\label{eq:price-vol-risk}
\end{equation}
$\lambda$ is a parameter used to generate the market price of volatility risk.
Recall that market price of risk is:
\[
\Theta_1=\frac{\mu-r}{\sqrt{\upsilon_t}}
\]
It can be shown that a risk-neutral measure $Q$ can be defined through Radon-Nikodym derivative via Multi-dimensional Girsanov's theorem \cite{shreve2004stochastic} using the $\Theta_1$ and $\Theta_2$.

Therefore, Heston model under a risk-neutral measure $Q$ is:
\begin{equation}
\begin{split}
dS&=r S dt + \sqrt{\upsilon} S \hat{dW}\\
d\upsilon&=\kappa^*(\overline{\upsilon}^*-\upsilon)dt+\eta \sqrt{\upsilon}\hat{dZ}\\
E[\hat{dZ}\hat{dW}]&=\rho dt
\end{split}
\label{eq:hestonNeutral}
\end{equation}
where
\[
\kappa^*=\kappa+\lambda, \overline{\upsilon}^*=\frac{\kappa \overline{\upsilon}}{\kappa+\lambda}
\]
\[
\hat{dW}=dW+\Theta_1 dt
\]
\[
\hat{dZ}=dZ+\Theta_2 dt
\]
Similar to Black-Scholes model, Heston model have closed-form solutions. The closed formed solution for European call option under  the risk-neutral measure $Q$ is
\begin{equation}
	\label{eq:hestonPrice}
Call=S\; N_1-Ke^{-r(T-t)} \; N_2
\end{equation}
Let us define the imaginary unit $\mathcal{I}^2=-1$. Then the $N_1$ and $N_2$ are defined as:
\[
N_j=\frac{1}{2}+\frac{1}{\pi}\int_{0}^{\infty} Re \left[\frac{e^{-\mathcal{I} \varphi \ln{K}} f_j(S,\upsilon,t,T;\varphi)} {\mathcal{I}\varphi} \right] d\varphi;\;\;, j=1,2
\] with $Re[\cdot]$ denoting the real part.
The characteristic function $f_j(S,\upsilon,t,T;\varphi)$ is :
\[
f_j(S,\upsilon,t,T;\varphi)=e^{A_j(t,T,\varphi)+B_j(t,T,\varphi)\upsilon+\mathcal{I} \varphi \ln{S}}\;, j=1,2
\]
Where:
\[
\begin{split}
A_j(t,T,\varphi)&=r \varphi (T-t)+ \frac{\kappa^* \overline{\upsilon}^*}{\eta^2}\left\{(b_j-\rho \eta \varphi \mathcal{I}+ d_j) (T-t)- 2\ln{\left[ \frac{1-g_je^d_j(T-t)}{1-g_j}\right]}     \right\}\\
B_j(t,T,\varphi)&=g_j\left[ \frac{1-e^d_j(T-t)}{1-g_je^{d_j(T-t)}}\right] \\
g_j&=\frac{b_j-\rho\varphi \mathcal{I}+d_j}{b_j-\rho\varphi \mathcal{I}-d_j}\\
d_j&=\sqrt{(\rho \eta \varphi \mathcal{I}-b_j)^2-\eta^2(2 u_j \varphi- \varphi^2)}\\
u_1&=0.5, u_2=-0.5, b_1=\kappa^*-\rho \upsilon, b_2=\kappa^*
\end{split}
\]
European put option price can be derived from the call-put parity:
\[
Put=Call-S+K e^{-r(T-t)}
\]
The parameters to be calibrated from the market option prices are $\{\upsilon,\kappa^*,\overline{\upsilon}^*,\eta,\rho\}$ where $\upsilon$ is the initial instantaneous variance, $\overline{\upsilon}^*$ is the long term mean of variance under risk-neutral measurement $Q$, $\kappa^*$ is the rate of mean reversion under risk-neutral measurement $Q$, $\eta$ is the volatility of volatility, and $\rho$ is correlation.

Under Black-Scholes model, an option is dependent on tradable asset $S_t$. The randomness in option value is solely determined by the randomness of the asset $S_t$. Such uncertainly can be hedged by continuously adjusting the shares of underlying asset as we have discussed in  section \ref{sec:bs}. This implies a complete market \cite{shreve2004stochastic}.
Under a stochastic volatility model such as Heston model, the uncertainty of option value comes from both the underlying asset $S_t$ and the volatility (or variance $\upsilon_t$ as in Heston model). The volatility itself is not tradable which implies an incomplete market under stochastic volatility model. The incompleteness implies the risk-neutral measure is not unique. In other words, $\lambda$ is not unique. Different choices of $\lambda$ will lead to different risk-neutral measurements. However, one can assume a risk-neutral measure $Q$ exists and calibrate the Heston model to match the the market option prices directly using the dynamics in \eqref{eq:hestonNeutral} without specifying the $\lambda$. In this way, $\lambda$ has been implied and embedded into the calibrated model parameters $\kappa^*$ and $\overline{\upsilon}^*$. Interested reader can refer to \cite{heston1993closed,gatheral2011volatility} for more details of risk-neutral pricing under Heston model.

Again, given the annual dividend yield $q$, the above heston pricing formula can be rewritten as:
\[
Call=S e^{-q(T-t)}\; N_1-Ke^{-r(T-t)} \; N_2
\]
\[
N_j=\frac{1}{2}+\frac{1}{\pi}\int_{0}^{\infty} Re \left[\frac{e^{-\mathcal{I} \varphi \ln{K}} f_j(S,\upsilon,t,T;\varphi)} {\mathcal{I}\varphi} \right] d\varphi;\;\;, j=1,2
\] with $Re[\cdot]$ denoting the real part.
The characteristic function $f_j(S,\upsilon,t,T;\varphi)$ is :
\[
f_j(S,\upsilon,t,T;\varphi)=e^{A_j(t,T,\varphi)+B_j(t,T,\varphi)\upsilon+\mathcal{I} \varphi \ln{S}}\;, j=1,2
\]
Where:
\[
\begin{split}
A_j(t,T,\varphi)&=(r-q) \varphi (T-t)+ \frac{\kappa^* \overline{\upsilon}^*}{\eta^2}\left\{(b_j-\rho \eta \varphi \mathcal{I}+ d_j) (T-t)- 2\ln{\left[ \frac{1-g_je^d_j(T-t)}{1-g_j}\right]}     \right\}\\
B_j(t,T,\varphi)&=g_j\left[ \frac{1-e^d_j(T-t)}{1-g_je^{d_j(T-t)}}\right] \\
g_j&=\frac{b_j-\rho\varphi \mathcal{I}+d_j}{b_j-\rho\varphi \mathcal{I}-d_j}\\
d_j&=\sqrt{(\rho \eta \varphi \mathcal{I}-b_j)^2-\eta^2(2 u_j \varphi- \varphi^2)}\\
u_1&=0.5, u_2=-0.5, b_1=\kappa^*-\rho \upsilon, b_2=\kappa^*
\end{split}
\]
European put option price can be derived from the call-put parity:
\[
Put=Call-S e^{-q (T-t)}+K e^{-r(T-t)}
\]

In this thesis, we deal with Heston model under the risk-neural measurement $Q$. For simplicity, in this thesis,  we use $V_{Heston}(S,t,T,K,r,q;\upsilon,\kappa^*,\overline{\upsilon}^*,\eta,\rho)$ to denote the European Heston pricing function regardless of the call or put nature.
\subsection{SABR Model}
\label{sec:SABR}
The SABR model  \cite{hagan2002managing} is another stochastic volatility model, which attempts to capture the volatility smiles in derivatives markets. The name stands for "Stochastic Alpha, Beta, Rho", referring to the parameters of the model.  The SABR model is another popular stochastic volatility model widely used in financial risk management. Its popularity is due to the fact that it can reproduce comparatively well the market-observed volatility smile and that it provides a closed-form
formula for the volatility. 
Given the risk-free interest rate $r$, the annual dividend yield $q$, and the forward $F_t$ with expiry $T$ is: 
\[
F_t=S_t e^{(r-q) (T-t)}
\]
In the SABR stochastic volatility model, the  forward $F_t$ price follows the following stochastic differential equation:
\[
\begin{split}
dF_t=\alpha_t (F_t)^{\beta}dW_t\\
d\alpha_t=\nu \alpha_t dZ_t\\
E[dW_tdZ_t]=\rho dt
\end{split}
\]
These parameters are described as follows:
\begin{itemize}
	\item $\alpha_t$ is the instantaneous  volatility of the forward $F_t$.
	\item $\nu$ is the   volatility of  instantaneous  volatility $\alpha_t$  .
	\item $W_t$ and $Z_t$ are correlated Wiener process with correlation coefficient $\rho$
\end{itemize}

A variant of the Black–Scholes option pricing model, Black model \cite{black1976pricing}, is often used together with SABR model. Under Black model, the  forward $F_t$ price follows the following stochastic differential equation:
\[
dF_t=\sigma_B F_t dW_t\\
\]
where $\sigma_B$ is the volatility. We use $V_B(F,K,r,t,T;\sigma_{B})$ to denote the Black pricing function. For European call option:
\[
V_B(F,t,T,K,r;\sigma_{B})=e^{-r(T-t)}[F \mathcal{N} (d3)-K  \mathcal{N} (d4)]
\]
For European put option:
\[
V_B(F,t,T,K,r;\sigma_{B})=e^{-r(T-t)}[K  \mathcal{N} (-d4)-F  \mathcal{N} (-d3)]
\]
where $\mathcal{N}$ is the cumulative density function of standard normal distribution
\[
d_3=\frac{\ln(F/K)+\sigma^2/2(T-t)}{\sigma \sqrt{T-t}}, \;\; d_4=d_3-\sigma  \sqrt{T-t}
\]
Consider an option on the forward $F$ with expiry $T$ and strike $K$ at time $t$.
If we force the SABR model price of the option into the form of the Black model valuation formula. Then the SABR implied volatility, which is the value of the $\sigma_B$ in Black's model that forces it to match the SABR price, is approximately given by:

\begin{eqnarray*}
	\sigma_{B}(F,t,T,K;\alpha,\beta,\nu,\rho) &\approx&
	\frac{\alpha}{(FK)^{(1-\beta)/2}\left[1+\frac{(1-\beta)^2}{24}\log^2(F/K)
		+ \frac{(1-\beta)^4}{1920}\log^4(F/K) + \cdots\right]} \cdot
	\frac{z}{x(z)}  \\
	& & \cdot \left\{1+\left[\frac{(1-\beta)^2}{24}\frac{\alpha^2}{(FK)^{1-\beta}}
	+ \frac{1}{4} \frac{\rho\beta\nu\alpha}{(FK)^{(1-\beta)/2}} +
	\frac{2-3\rho^2}{24}\nu^2 \right](T-t)+\cdots\right\}
\end{eqnarray*}
where
\[
z = \frac{\nu}{\alpha}(FK)^{(1-\beta)/2}\log (F/K),\; x(z) =
\log\left\{\frac{\sqrt{1-2\rho z + z^2}+z-\rho}{1-\rho}\right\}
\]
For the special case of at-the-money options, options struck at $K=F$,
this formula reduces to
\[
\begin{split}
\sigma_{ATM} &= \sigma_{B}(F,t,T,F;\alpha,\beta,\nu,\rho) \\
&\approx
\frac{\alpha}{F^{(1-\beta)}}\left\{1 +
\left[\frac{(1-\beta)^2}{24}\frac{\alpha^2}{F^{2-2\beta}} +
\frac{1}{4}\frac{\rho\beta\alpha\nu}{F^{(1-\beta)}} +
\frac{2-3\rho^2}{24}\nu^2 \right] (T-t) + \cdots \right\}
\end{split}
\]
Therefore, the European option value under SABR model is given by:
\[
V_{SABR}=V_B(F,t,T,K,r;\sigma_{B}(F,t,T,K;\alpha,\beta,\nu,\rho))
\]

The $\sigma_{B}(F,t,T,K;\alpha,\beta,\nu,\rho)$ under SABR model depends on the forward $F$, the strike $K$, the time to expiry $T-t$, the initial SABR volatility $\alpha$, the power of forward $\beta$, the volatility of volatility $\nu$, and the correlation $\rho$.  Interested readers can refer to \cite{hagan2002managing} for the detailed derivation of the analytical approximation $\sigma_{B}(F,t,T,K;\alpha,\beta,\nu,\rho)$.  We use $V_{SABR}(S,t,T,K,r,q;\alpha,\beta,\nu,\rho)$ to denote the option pricing formula under SABR model regardless of the call and put nature in the latter discussion.



\section{Discrete Hedging Problem}
\label{sec:DiscreteHedgingCriteria}
In this thesis, we are dealing with real market option and underlying prices.  We use  $\Vmkt(t,T,K)$ and $\Smkt(t)$ to denote the time $t$ market price of the option with expiry $T$ and strike $K$ and the underlying price respectively. We propose to learn discrete data-driven hedging position for standard options, calls or puts, based on observations of the option market prices on the same underlying price at a set of trading times.
The goal of the data-driven discrete option hedging in this thesis is to compute a hedging position $\delta$ in the underlying dynamically to minimize certain appropriate measurements of the hedging risk. We are interested in two different types of hedging risk:
\begin{itemize}
   \item  Discrete local hedging risk
   \item  Discrete total hedging risk
\end{itemize}
The definition of the discrete local hedging risk and discrete total hedging risk are explained in the following subsections.

\subsection{Discrete Local Hedging Risk}
\label{sec:DiscreteLocalRisk}
 Let $\DT$ denote the fixed  time interval.
Each observation of a market option price  $V^{mkt}_{t,K,T}$ is uniquely associated with a triplet $\{t,T,K\}$, where $t$ is the trading time of the option price,  $K$ is the strike, and expiry $T$.
  Denote:
\begin{equation}
\begin{split}
\Delta V^{mkt}_{t,K,T}& =V_{mkt}(t+\Delta t,K,T)-V_{mkt}(t,K,T)\\
\Delta \Smkt_{t} &=\Smkt(t+\DT)-\Smkt(t)
\end{split}
\label{eq:DVDS}
\end{equation}
The discrete local hedging risk in the rebalancing interval $\DT$ is:
\begin{equation}
\Delta \Smkt_{t}\delta_{t,T,K} -\Delta V^{mkt}_{t,K,T}
\label{eq:local}
\end{equation}
where $\delta_{t,T,K}$ is a hedging position.

The discrete local hedging risk can be understood as the following.
We set up the following portfolio at time $t$:
\begin{itemize}
\item A short position on option $\Vmkt(t,T,K)$
\item Long $\delta_{t,T,K}$ shares of $\Smkt(t)$
\item An amount in risk-free bank account $B(t)$
\end{itemize}
The bank account  at time $t$ is set to be: 
\[
    B(t)=\Vmkt(t,T,K)-\Smkt(t) \delta_{t,T,K}
\]
We use $P_{H}(t,T,K)$ to denote the hedging portfolio value at $t$ for hedging options with expiry $T$ and strike $K$.
The hedging portfolio value is:
\[
    P_{H}(t,T,K)=-\Vmkt(t,T,K)+\Smkt(t) \delta_{t,T,K} +B(t)=0
\]
Let  $d\Vmkt_{t,T,K}$, $d\Smkt_{t}$, $d B_t$, and $d P^{H}_{t,T,K}$ denote the instantaneous change in the market option price, the underlying price,  bank account and the hedging portfolio value respectively, the instantaneous hedging risk at time $t$ is therefore:
\begin{equation}
    d P^{H}_{t,T,K}=\ d\Smkt_{t} \delta_{t,T,K}-d \Vmkt_{t,T,K}+ d B_t
\end{equation}
Note that $d B_t$ is deterministic:
\[
    d B_t=r B(t) dt
\]
Therefore, if we assume the risk-free interest rate is zero (or omit $d B_t$ since it is deterministic), we have:
\begin{equation}\label{eq:HE}
    d P^{H}_{t}=\ d\Smkt_{t} \delta_{t,T,K}-d \Vmkt_{t,T,K}
\end{equation}
When the market is complete as what we assume in section \ref{sec:bs} for the derivation of Black–Scholes formula, the randomness in this instantaneous hedging risk can theoretically be eliminated by continuously trading the underlying asset and set $\delta=\frac{d \Vmkt_{t,T,K}}{d\Smkt_{t}}$. However in practice, option hedging needs to be implemented in an incomplete market, since hedging can only be done at discrete times.
Besides additional risk, e.g., jump and volatility, cannot be eliminated by trading the underlying asset only \cite{heston1993closed,gatheral2011volatility}. Lastly, in practice, $\frac{d \Vmkt_{t,T,K}}{d\Smkt_{t}}$ is hard to be accurately computed due to the pricing model parameters dependence on underlying asset. The pricing model parameters dependence will be explained in more details in section \ref{sec：dependence}. 
 Therefore, the hedging risk cannot be eliminated even instantaneously. 

In practice, one actually care about the portfolio changes after a discrete time interval $\DT$.
After the fixed interval $\DT$  and assuming the interest rate is zero,
we have:
\begin{equation}
	\label{eq:localPL}
    \Delta P_{H}(t,T,K)=P_{H}(t+\DT,T,K)-P_{H}(t,T,K)=\Delta \Smkt_{t}\delta_{t,T,K} -\Delta V^{mkt}_{t,K,T}
\end{equation}
The local hedging risk is therefore defined to be the one-step hedging error when we assume the risk-free interest risk is zero. The local discrete hedging risk \eqref{eq:local} measures  the changes in the hedging portfolio after a fixed time interval $\DT$ when the hedging position is set to be $\delta_{t,T,K}$.  
 As $\DT \rightarrow 0$, we also have local hedging risk converge to instantaneous hedging risk: 
\[
\Delta \Smkt_{t}\delta_{t,T,K} -\Delta V^{mkt}_{t,K,T}  \rightarrow d\Smkt_{t}\delta_{t,T,K}-d \Vmkt_{t,T,K}
\]

\subsection{Discrete Total Hedging Risk}
In reality, one usually want to hedge until the expiries of the options, which requires rebalancing the hedging portfolio multiple times. Again,
consider a hedging portfolio $P_{H}(t,T,K)$ which is composed of:
\begin{itemize}
	\item A short position on option $\Vmkt(t,T,K)$
	\item Long $\delta_{t,T,K}$ (hedging position) shares of $\Smkt(t)$
	\item An amount in a risk-free bank account $B(t)$
\end{itemize}
For the notational simplicity since the $T$ and $K$ are fixed in following discussion, we denote:
\[
    \Vmkt_{t}=\Vmkt (t,T,K)\;,\Smkt_{t}=\Smkt(t)\;,\delta_{t}=\delta_{t,T,K},B_t=B(t)\;, P^{H}_t=P_H(t,T,K)
\]
The hedging portfolio is rebalanced at discrete times $\{t_0,t_1,\dots,t_{N-1}\}$. 
Initially at $t_0$, we have
\[
P^H_{t_0}=  -V_{t_0}^{mkt}+\delta_{t_0} S_{t_0}+ B_{t_0}=0
\]
And
\[
B_{t_0}=V_{t_0}^{mkt}-\delta_{t_0} S_{t_0}
\]
At each rebalancing time $t_j$, we update our hedging position by changing the share we hold from $\delta_{t_{j-1}}$ to $\delta_{t_j}$ at $t_j$, where any required cash is borrowed, and any excess cash is loaned. Assume $\Delta t=t_{j}-t_{j-1}$ is fixed and risk-free interest rate is zero.
The bank account is updated by:
\[
B_{t_{j}}= B_{t_{j-1}}-S_{t_j}(\delta_{t_j}-\delta_{t_{j-1}})
\]
Let $t_j^+$ and $t_j^-$   be the time immediately after  and immediately before $t_j$. Assume that the performance is measured at the $t_N^-$, where $t_N=T$ is the expiry:
\begin{equation}
\begin{split}
P_{t_N^-}&=B_{t_{N-1}}- V_{t_N}^{mkt}+ S_{t_N} \delta_{t_{N-1}}  \\
&=\sum_{j=0}^{N-1}\left\{ \left(S_{t_{j+1}}-S_{t_{j}}\right) \delta_{t_j} \right\}+ V_{t_0}^{mkt}-V_{t_N}^{mkt}\\
&=\sum_{j=0}^{N-1}\left\{ \left( S_{t_{j+1}}-S_{t_{j}}\right) \delta_{t_j} -(V_{t_{j+1}}^{mkt}-V_{t_j}^{mkt}) \right\}
\end{split}
\label{eq:TotalObj}
\end{equation}
Equation \eqref{eq:TotalObj} is defined as the {\em discrete total hedging risk}.
If we always set $\delta_t=\frac{d V_t^{mkt}}{d S_t}$ and let $\Delta t \rightarrow 0$ (we continuously rebalance the portfolio), then $P_{t_N^-}=0$. In reality, even if we can set the hedging position to be the actual partial derivative of market option price with regards to underlying price $\delta_t=\frac{d V_t^{mkt}}{d S_t}$,  we can only rebalance discretely due to the transaction cost. Thus, $P_{t_N^-}$ can take positive (profit) and negative value (loss).
The  {\em discrete total hedging risk} measures the hedging portfolio profit and loss at the expiry $T$. 

Equation \eqref{eq:TotalObj} can be written as:
\[
\begin{split}
P^H_{t_N^-}&=\sum_{j=0}^{N-1}\left\{ \left( S_{t_{j+1}}-S_{t_{j}}\right) \delta_{t_j} -(V_{t_{j+1}}^{mkt}-V_{t_j}^{mkt}) \right\}\\
&=\sum_{j=0}^{N-1}\left\{ \DS_{t_j} \delta_{t_j} -\Delta V^{mkt}_{t_j} \right\}
\end{split}
\label{eq:TotalObjV2}
\]
with $\Delta V^{mkt}_{t_j}$and  $\DS_{t_j}$ given in \eqref{eq:DVDS}.
Plug in the notation for $T$ and $K$, the discrete total hedging risk is:
\begin{equation}
    P_H(T,T,K)=\sum_{j=0}^{N-1}\left\{ \Delta \Smkt_{t_j} \delta_{t_j,T,K} -\Delta \Vmkt_{t_j,T,K} \right\}
    \label{eq:TotalObjV2}
    \end{equation}
By comparing equation \eqref{eq:local} and \eqref{eq:TotalObjV2}, we can see that the {\em discrete total hedging risk} is the summation of the {\em discrete local hedging risk} evaluated at discrete rebalancing time $\{t_0,t_1,\dots,t_{N-1}\}$. 
\subsection{Interest Rate and Dividend}
In the previous discussion, we assume zero interest rate  when we define the {\em discrete total hedging risk} and the {\em discrete local hedging risk}. In the years of publication of the results presented in \cite{knian2017} and \cite{knian2019}, the zero interest rate  was assumed when measuring the hedging performance under discrete local hedging risk framework. The results presented \cite{knian2017} and \cite{knian2019} were evaluated from 2007 to 2015. During  most of the time from 2007 and 2015, the interest rate is virtually zero. So the zero interest assumption is valid during most of the time.  In \cite{knian2020}, such assumption is relaxed and we include the effect of interest rate when we evaluate the total hedging performance. We assume the interest rate is fixed as a constant during the life time of the hedging portfolio and denote it as $r$, the discrete total hedging risk is therefore:
\begin{equation}
    P_H(T,T,K)=\sum_{j=0}^{N-1}\left\{ \left[e^{r (N-j-1) \Delta t} \Smkt_{t_{j+1}}-e^{r (N-j) \Delta t}\Smkt_{t_{j}}\right] \delta_{t_j,T,K} \right\}+e^{r N \Delta t} \Vmkt_{t_0,T,K}-\Vmkt_{T,T,K}
    \label{eq:TotalObjV3}
\end{equation}

In this thesis, we are dealing with S\&P 500 index option. For S\&P 500 index, the closing index price of each day is already adjusted to capture corporate actions that affect market capitalization such as as additional share insurance, dividends and restructuring events such as mergers or spin-offs\cite{lassance2018comparison}. When we calibrating the  option pricing models such as Black-Scholes model,   an estimate of the dividends to be paid up until the option’s expiration are needed. For dividend-paying indices, we assume that the security pays dividends continuously, according to a continuously-compounded annual dividend yield $q$. 

For the calibration of different pricing models in this thesis, we use the option market data from OptionMetric \cite{optionmetrics2008ivy} database. The OptionMetric \cite{optionmetrics2008ivy} database provides us market option quotes for each trading day from 1996-01 to 2015-08-31. We use the zero curves on each trading day provided by the OptionMetric \cite{optionmetrics2008ivy} database to extract the risk-free interest rate $r$. The zero curve used by the OptionMetric database is derived from ICE
IBA LIBOR rates and settlement prices of CME Eurodollar futures. For a given option, the appropriate interest rate input $r$ corresponds to the zero-coupon rate that has a maturity equal to the option’s expiration $T-t$, and is obtained by linearly interpolating between the two closest zero-coupon rates on the zero curve. In addition, OptionMetric \cite{optionmetrics2008ivy} database provides annual  dividend yield $q$ for S\&P 500 index. The $q$ is recorded daily for the S\&P 500 index and supplied as the input for calibrating the option pricing models.  Details of how OptionMetric compute the zero curves and the annual dividend yield $q$  using market data can be found in \cite{optionmetrics2008ivy}.


\section{Option Hedging Using the Sensitivity from Pricing Model}
In this section, we discuss how we can compute the hedging position $\delta_{t,T,K}$ from the option pricing model, and the drawbacks of this approach. Specifically, we discuss the pricing model parameters dependence issue, which motivates our data-driven model,  and provide several existing ways for correcting the model parameter dependence. 
\subsection{Classical Hedging Position From Pricing Model}
\label{sec:Classical}
In section \ref{sec:pricing}, we have introduced several option pricing models with analytical pricing formula. 
Let $V(S,t,T,K,r,q;\theta)$ denote the arbitrary option pricing model. For Black-Scholes Model,  $\theta$ is the volatility $\sigma$. For Heston model, $\theta=\{\upsilon,\kappa^*,\overline{\upsilon}^*,\eta,\rho \}$. For SABR model, $\theta=\{\alpha,\beta,\nu,\rho \}$. \footnote{Note that, Studies \cite{hulloptimal,hagan2002managing,hagan2017bartlett} shows that $\beta$ can often be fixed a priori. In this case, $\theta=\{\alpha,\nu,\rho \}$.}
We want to calibrate the pricing model to match the market option quotes. 

On each trading time $t$, we have observed market option prices for different strikes $\{K_1,\dots, K_{N_k}\}$ and different expiries $\{T_1,\dots, T_{N_T}\}$, where $N_k$ is the number of strikes and $N_T$ is the number of expiries. Further assume the risk-free interest rate and annual dividend yield are given as $r$ and $q$. The underlying asset price is $S_t$. The calibration can be done by:
\[
\min_{\theta}   \sum_{i=1}^{N_T}  \sum_{j=1}^{N_K} \left(V(S_t,t,T_i,K_j,r,q;\theta)-\Vmkt(t,T_i,K_i)\right)^2
\]

    For Black-Scholes model, there is only one parameter to be calibrated,
so one can usually find that a constant volatility cannot match all market option quotes very well. Alternatively, one can match each market quotes exactly by:
\[
V_{BS}(S_t,t,T_i,K_j,r,q;\sigma^{imp}_{t,T_i,K_j})=\Vmkt(t,T_i,K_j)
\]
The volatility $\sigma^{imp}_{t,T_i,K_j}$ that equals the Black–Scholes price with the market option price is called \textbf{Black-Scholes implied volatility}. (In section \ref{sec:SABR}, we have introduced a term called SABR implied volatility which equals the SABR model price with the Black model price. The SABR implied volatility and  Black-Scholes implied volatility are two different but related terminologies. )
Note that in this way, for different options $\Vmkt(t,T_i,K_j)$, different models are calibrated.  One can often find that the implied volatility $\sigma^{imp}$ tends to differ across different strikes and expiries.

Alternatively, for stochastic volatility models such as Heston and SABR, the calibration is usually done by matching the volatility smile instead of the entire volatility surface \cite{heston1993closed,hagan2002managing,hulloptimal}. In other words, for a fixed expiry $T_i$, we solve
\[
\min_{\theta} \sum_{j=1}^{N_K} \left(V(S_t,t,T_i,K_i,r,q;\theta)-\Vmkt(t,T_i,K_i)\right)^2
\]
Note that in this way, pricing models are calibrated separately for different expiries $T_i$  on the same trading date $t$.

Given the option pricing model $V(S,t,T,K,r,q;\theta^*)$ with the calibrated $\theta^*$, the hedging positive is commonly determined by:
\[
\delta_{t,T,K}=  \frac{\partial V(S,t,T,K,r,q;\theta^*)}{\partial S}
\]
Although a more complex option pricing model such as stochastic volatility models can match the market quotes better and account for the volatility smiles better, it does not necessarily lead to better hedging strategy. 
Several studies \cite{dumas1998implied,bakshi1997empirical,yung2003empirical, lassance2018comparison} detail the hedging performances of option pricing models and show that, while explicitly
modelling volatility smile substantially improves the pricing
accuracy, it frequently decreases the hedging
performance compared to the simpler  Black-Scholes models. A recent empirical study \cite{lassance2018comparison} demonstrates that, for hedging S\&P 500 index options, the simple Black-Scholes model with a constant volatility calibrated on each trading date can even outperform complex stochastic volatility models such as Heston model \cite{heston1993closed} and Heston-Nandi model \cite{heston2000closed}. The phenomenon where better pricing model leads to worse hedging performance is refer to as pricing/hedging conundrum in \cite{lassance2018comparison}. In the following section, we provide one of the potential explanation why such phenomenon exists.

\subsection{Parameter Dependence on Underlying Asset}
\label{sec：dependence}
We notice that determining the hedging position from the partial derivative:
\[
\delta_{t,T,K}=  \frac{\partial V(S,t,T,K,r,q;\theta^*)}{\partial S}
\] of the calibrated pricing function inevitably leads to incomplete elimination of the underlying sensitivity, even in the context of instantaneous hedging. 
To see this,
let us assume that a pricing model allows matching of the market price $\Vmkt$  at $t$ exactly, i.e., $\theta^*$ is determined so that for arbitrary observed $T$ and $K$ in market.
\begin{equation} \label{eq:imp}
     V(S,t,T,K,r,q;\theta^*)=\Vmkt(t,T,K)
\end{equation}
In general, $\{t,T,K\}$ do not depend on $S$. Let us further assume $\{r,q\}$ do not depend on $S$. 
Unfortunately, the calibration process inevitably relates $S$ and $\theta^*$. Therefore, we have:
\[
  \frac{d V}  {d S}=\frac{\partial V}{\partial S} + \frac{\partial V}{\partial \theta^*}\frac{\partial \theta^*}{\partial S}
\]
Thus, unless one can ensure $\frac{\partial \theta^*}{\partial S} = 0$, we will have $dV- \frac{\partial V}{\partial S} \cdot dS \neq 0$.
Several studies have demonstrate that pricing model parameters can be correlated with the underling asset price, so whenever $S$ changes, the pricing model parameters should change too. As an example, negative correlation between Black-Scholes implied volatility and underlying asset price  has been observed  in \cite{french1987expected,bollerslev2006leverage}.
Besides, the calibration equation \eqref{eq:imp} says little about how the sensitivity $\frac{d V}{d S}$ matches $\frac{d \Vmkt}{d S}$. Thus, even if we can compute the sensitivity $\frac{d V}  {d S}$ exactly, we generally have:
\[
    d\Vmkt- \frac{d V}{d S} \cdot dS \neq 0
\]
Even with the perfect case where we can have a pricing model matching all observed market prices, which is often not achievable due to the calibration error, and the unrealistic assumption:
\begin{equation} \label{mkt:d}
    \frac{d \Vmkt}{d S}=\frac{d V}{d S}=\frac{\partial V}{\partial S} + \frac{\partial V}{\partial \theta^*}\frac{\partial \theta^*}{\partial S}
\end{equation}
We still have  the pricing model parameters dependence issue to be addressed. Since it is difficult to account for parameter dependence $\frac{\partial \theta^*}{\partial S}$,  $\frac{d \Vmkt}{d S} $ can be hard to be captured accurately. The classical approach of using $\frac{\partial V}{\partial S}$ as the hedging position does not eliminate the instantaneous hedging risk.

A more complex model usually has more pricing model parameters to be calibrated. The more parameters there are, the more severe the parameters dependence issue possibly is. That may be one of the reason why many studies observe poorer hedging performance from more complex pricing model than simple Black-Scholes model in real market. Even with the simple Black-Scholes model, the Black–Scholes implied volatility still depends on the underlying asset. In the following section, we introduce several corrections under Black-Scholes framework for the parameter dependence. In addition, complex models like Heston model usually can not match the observed market option prices exactly. The calibration error can also potentially increase the hedging errors when the partial derivative is used as the hedging position.  



\subsection{Corrections Under Black-Scholes Framework}
Although the issue of pricing model parameters dependence on underlying asset has been noted, e.g., in \citep{coleman2001,hulloptimal}, correcting for this risk exposure is not straightforward.
Methods have been proposed to compensate for this missing exposure.  Typically methods are based on analysis of some parametric models, e.g., LVF and SV; these methods minimize the variance of the hedging error \citep{hulloptimal,alexander09,Angelini09,Angelini10,Goutte13}. In this section, we present several correction methods under Black-Scholes Framework. The discussion below are majorly drawn from \cite{hulloptimal,hagan2017bartlett,bartlett2006hedging}.
\subsubsection{A Data-Driven Correction For The Black–Scholes Delta Hedging}




Equation \eqref{mkt:d} implies that the sensitivity of the option value function $\frac{\partial V}{\partial S}$ calibrated at the rebalancing time to
satisfy the calibration equation \eqref{eq:imp} cannot completely hedge  
the sensitivity of the market option price to the underlying price since $\frac{\partial \theta^*}{\partial S}$ is not accounted for.
If the hedging risk in each rebalancing period is the performance criteria, the information in the option market price change needs to be explicitly incorporated to determine a better hedging method which minimizes the hedge risk as measured by
market option price change instead of the model option price change.


If sufficient market option and underlying price data exist, a potentially more effective approach is to learn the hedging strategy 
based on historical observations of both  changes in the market option price and the underlying price. This approach is completely different from the existing  parametric underlying price
model based hedging methods.

To the best of our knowledge, the study
\cite{hulloptimal} is the first to determine the hedging strategy based on historical observations of the market option prices and 
underlying prices.  
This particularly form \eqref{eq:HullWhite} is derived based on the following analysis using the Black–Scholes model. 
More specifically, assume we have calibrate the implied volatilities $\sigma^{imp}$ to match all the market option quotes as what we have discussed in section \ref{sec:Classical}.
Let us again assume we  are computing  the hedging position  with a fixed $T$ and $K$ at a specific trading time $t$, so for notational simplicity in the following discussion, we drop ${t,T,K}$ in the subscript. Following \eqref{mkt:d}, to determine the  sensitivity  $\frac{d V_{BS}}{d S} $ accurately, one needs to compute 
\begin{equation} \label{eq:Chain}
\frac{\partial V_\BS}{\partial S} + \frac{\partial V_\BS}{\partial \sigma^{imp}}
\frac{\partial \sigma^{imp}}{\partial S}
\end{equation}
$
\frac{\partial V_\BS}{\partial S}
$ is the Black–Scholes delta with implied volatility and $\frac{\partial V_\BS}{\partial \sigma^{imp}}$ Black–Scholes vega with implied volatility. Both can be computed analytical. The undetermined part is $\frac{\partial \sigma^{imp}}{\partial S}$.
%\citet{hulloptimal} computes the delta $\delta_{\MV}$, to correct the implied volatility risk exposure as
In \cite{hulloptimal}, the authors choose  
$$
\frac{\partial \sigma^{imp}}{\partial S} =
\frac{a+b \cdot\frac{\partial V_\BS}{\partial S}+c \cdot (\frac{\partial V_\BS}{\partial S})^2}{S \sqrt{T-t}}
$$
where $a, b \text{ and  } c$ are the parameters to be fitted using  market option data. Therefore, plug in the notation for $t,T,K$,
the minimum variance delta is therefore:
\begin{equation}
\delta^{MV}_{t,T,K}=\delta^{BS}_{t,T,K}+\text{vega}^{BS}_{t,T,K} \frac{a+b \cdot \delta^{BS}_{t,T,K} +c \cdot (\delta^{BS}_{t,T,K})^2}{S_t\sqrt{T-t}},
\label{eq:HullWhite}
\end{equation}
where $\delta^{BS}_{t,T,K}$ and $\text{vega}^{BS}_{t,T,K}$ are respectively the Black–Scholes delta and vega, using the implied volatility at time $t$ for option with strike $K$ and expiry $T$.

The model parameters $a,b,c$ are determined using historical observations.
Let $M$ denote the number of historical data instances. Each data instance corresponds to a unique combination of $\{t,T,K\}$. For data instance $i$, with time $t_i$, strike $K_i$, and expiry $T_i$, we approximate the instantaneous changes in the market option price and underlying price by the \textbf{daily} changes of the market option prices and underlying prices:
\[
    d\Vmkt_{t_i,T_i,K_i} \approx \Delta \Vmkt_{t_i,T_i,K_i},\;\;
    d\Smkt_{t_i} \approx \Delta S_{t_i}
\]
Thus, we have the following minimization problem:
\[
\min_{a,b,c} \sum_{i=1}^M \left(\Delta S_{t_i} \delta^{MV}_{t_i,T_i,K_i} -\Delta \Vmkt_{t_i,T_i,K_i}   \right)^2
\]
A linear regression is performed to determine the model parameters $a,b,c$.

 Consequently, the minimum variance hedging function $ \delta^{MV}$ is computed based on regression estimation, assuming 
 the quadratic parametric model \eqref{eq:HullWhite}.
 It has been shown \cite{hulloptimal} that the corrective formula \eqref{eq:HullWhite} can significantly improve the daily hedging performance.

 \subsubsection{Correction With LVF Model For  The Black–Scholes Delta Hedging}
 As an improvement over the BS model, the local volatility function (LVF) model,
 \begin{equation}\label{LVF}
\frac{dS_t}{S_t} = (r-q) dt + \sigma(S_t,t)   dW_t
\end{equation}
 has been considered, see e.g., \citep{Dupire94,DermanKani94,DermanKani96},
  and LVF and its extensions remain widely popular in practice. Many methods have been proposed to calibrate a local volatility function $\sigma(S,t)$ from the traded market option prices, see e.g., \citep{JackwerthRubenstein96,Andersen-etal98,CLV98}. In addition, \citet{coleman2001} discuss a relationship between the partial derivatives of  calls and puts in the context of the LVF model, under which a call and put symmetry relation holds, see,  e.g., \citep{Carr94,Carr98}. This relationship is found useful in correcting the dependence of the implied volatility on the underlying for Black–Scholes delta hedging.

In Theorem \ref{pc:symmetry} below, we formalize this relation \citep{coleman2001} to any call and put functions satisfying the call-put-symmetry. We note that, for this relationship to be useful in correcting for MV hedging in practice, hence the relevant price functions  correspond to the market option prices, not model option values.

\begin{thm}\label{pc:symmetry}
Let $Call(S,t,T,K,r,q)$ and $Put(S,t,T,K,r,q)$ be the call option price and put option price with underlying price $S$, strike $K$, expiry $T$, time $t$, interest rate $r$ and dividend yield $q$.
Assume further that there exists a unique implied volatility calibrating to  $Call(S,t,T,K,r,q)$ and $Put(S,t,T,K,r,q)$  respectively and the call-put-symmetry below holds:
\begin{equation} \label{BSLVF}
Call(S, t,T,K,r, q)=Put(K, t,T,S , q, r).
\end{equation}
Then
\begin{equation}\label{PC_imp_sen}
\frac{\partial \impsigma_c(S, t,T,K, r, q)}{\partial S} =\frac{\partial \impsigma_p(K, t,T,S , q, r)}{\partial S}.
\end{equation}
where $\impsigma_c(\cdot)$ is the BS implied volatility calibrated to $C(\cdot)$ and $\impsigma_p(\cdot)$ is the BS implied volatility calibrated to $P(\cdot)$
respectively.
\end{thm}

\begin{proof}
Let $C_{\BS}(\cdot)$ and $P_{\BS}(\cdot)$  denote the BS model option value functions. Put and call symmetry under the Black-Scholes model implies that
\begin{equation}\label{BSdual}
C_{\BS}(S,t,T,K, r, q;\sigma )=P_{\BS}(K,t,T,S, q, r;\sigma),
\end{equation}
where $\sigma$ is any constant volatility.
Let $\impsigma_c(S, t,T,K, r, q)$ and $\impsigma_p(K, t,T,S , q, r)$ be the Black–Scholes implied volatilities calibrated to $Call(S,t,T,K, r, q)$ and $Put(K,t,T,S, q, r)$
Then
\begin{equation}
\begin{array}{l}
    Call(S,t,T,K, r, q)=C_{\BS}(S,t,T,K, r, q;\impsigma_c(S, t,T,K, r, q) )\\
    Put(K,t,T,S, q, r)=P_{\BS}(K,t,T,S, q, r;\impsigma_p(K, t,T,S , q, r)).
\end{array}
\end{equation}
From \eqref{BSLVF} and above, it follows
\begin{equation}\label{eq:imp_cp}
    C_{\BS}(S,t,T,K, r, q;\impsigma_c(S, t,T,K, r, q) )=P_{\BS}(K,t,T,S, q, r;\impsigma_p(K, t,T,S , q, r))
\end{equation}
Using \eqref{BSdual} with $\impsigma_c(\cdot)$,
$$
C_{\BS}(S,t,T,K, r, q;\impsigma_c(S, t,T,K, r, q) )=P_{\BS}(K, S, T, q, r;\impsigma_c(S, t,T,K, r, q))
$$
From above and \eqref{eq:imp_cp}, it follows
$$
P_{\BS}(K, S, T, q, r,\impsigma_c(S, t,T,K, r, q))= P_{\BS}(K,t,T,S, q, r;\impsigma_p(K, t,T,S , q, r))
$$
Assuming that there is a unique implied volatility from the BS formula, we have
\begin{equation}
    \impsigma_c(S, t,T,K, r, q)=\impsigma_p(K, t,T,S , q, r)
\end{equation}
Taking derivative with respect to $S$, we have
$$
\frac{\partial \impsigma_c(S, t,T,K, r, q)}{\partial S} =\frac{\partial \impsigma_p(K, t,T,S , q, r)}{\partial S},
$$
i.e., \eqref{PC_imp_sen} holds.
This completes the proof.
\end{proof}

Assume that the market option prices satisfy put-call symmetry. Then
the relevance of Theorem \ref{pc:symmetry} in accounting for dependence of the implied volatility on the underlying can be appreciated as follows.
On the left hand side of \eqref{PC_imp_sen}, the derivative is with respect to  the underlying price (the first argument). On the right hand side, the derivative is with respect to the strike price (the second argument).
When $q \approx r$ (as in the futures options market), $\frac{\partial \impsigma_p(K, t,T,S , q, r)}{\partial S}$ can be estimated from the observed implied volatility surface. In other words, for at-the-money with $K=S$ option, the rate of change in the implied volatility with respect to changes in the underlying price is equal to the slope of the volatility smile.
Consequently the sensitivity of the implied volatility to the underlying  can be estimated. 

\citet{hulloptimal}  implement a corrective formula for minimum variance hedging  based on  \eqref{PC_imp_sen}, referred to as the LVF minimum variance hedging. \citet{hulloptimal} further assume that the rate of change in the implied volatility with respect to changes in the underlying price is equal to the slope of the volatility smile  for options which are not at-the-money. Therefore, the LVF correction is:

    \begin{equation}
        \delta^{LVF}_{t,T,K}=\delta^{BS}_{t,T,K}+\text{vega}^{BS}_{t,T,K} \frac{\partial \sigma^{imp}}{\partial K}
        \label{eq:HullWhiteLVF}
    \end{equation}  
where  $\frac{\partial \sigma^{imp}}{\partial K}$ can be estimated from the  a quadratic function fitting the volatility smile for each expiry. Other minimum variance delta hedge methods have also been proposed to correct practitioner Black–Scholes delta explicitly, see, e.g., \citep{bakshi1997empirical,crepey2004,barlett2006,poulsen2009,alexander2012}. Interested reader can refer to them for more details.







\subsubsection{Correction With SABR model for the the Black–Scholes Delta Hedging}
Recall that the SABR implied volatility, which is the value of the $\sigma_B$ in Black's model that forces it to match the SABR price, is given in section \ref{sec:SABR}.
Based on SARR model, a correction formula for Black delta hedging is thus given by:
\begin{equation}
\frac{\partial V_{B}}{\partial F} + \frac{\partial V_{B}}{\partial \sigma_B}
\frac{\partial \sigma_B}{\partial F}
\label{eq:sabr}
\end{equation}
Note that, Black delta can be converted to Black–Scholes delta.
One can rewrite the formula to compute the sensitivity with regards to underlying asset price $S$ instead of forward $F$ by:
\begin{equation}
\left(\frac{\partial V_{B}}{\partial F} + \frac{\partial V_{B}}{\partial \sigma_B}
    \frac{\partial \sigma_B}{\partial F}\right)\frac{d F}{d S}
    \label{eq:sabrV2}
    \end{equation}
 Given the risk-free interest rate $r$, the annual dividend yield $q$, and the forward $F_t$ at time $t$ with expiry $T$ is: 
\[
        F_t=S_t e^{(r-q) (T-t)}
 \]
Plug in the notation for $t,T,K$ as what we did in previous sections, we  have:
\begin{equation}
    \delta^{SABR}_{t,T,K}=\left( \delta^{B}_{t,T,K}+\text{vega}^{B}_{t,T,K} \frac{\partial \sigma_B}{\partial F} \right)e^{(r-q) (T-t)}
    \label{eq:sabrV3}
\end{equation}

where 
$\delta^{B}_{t,T,K}$ is the Black delta $\frac{\partial V_{B}}{\partial F}$ at time $t$ for option with strike $K$ and expiry $T$. 
$\text{vega}^{B}_{t,T,K}$ is the Black vega $\frac{\partial V_{B}}{\partial \sigma_{B}}$ at time $t$ for option with strike $K$ and expiry $T$. 
$\frac{\partial \sigma_B}{\partial F}$ is the shortened form  of $\frac{\partial\sigma_{B}(F_t,t,T,K;\alpha,\beta,\nu,\rho)}{\partial F}$.

%Note that, \eqref{eq:sabrV3} is similar to \eqref{eq:HullWhite}. The major difference lies in the how we compute the sensitivity of implied volatility with regards to the underlying ( or forward) price. 

Although, SABR model can be used to correct the dependence of implied volatility on the forward price, the formula \eqref{eq:sabrV3} omits the fact the initial SABR volatility $\alpha$ is correlated with the forward $F$. So whenever the forward $F$ changes,  the initial SABR volatility $\alpha$ changes.  There is still unaccounted parameter dependence  in  equation \eqref{eq:sabrV3}.

Therefore, Bartlett correction \cite{hagan2017bartlett,bartlett2006hedging} was proposed to account for the dependence of $\alpha$ on $F$.
The Bartlett correction is based on the following analysis. Recall that, in SABR model we assume:
\[
\begin{split}
dF_t=\alpha_t (F_t)^{\beta}dW_t\\
d\alpha_t=\nu \alpha_t dZ_t\\
E[dW_tdZ_t]=\rho dt
\end{split}
\]
which can be rewritten as:
\[
\begin{split}
dF_t&=\alpha_t (F_t)^{\beta}dW_t\\
d\alpha_t&=\nu \alpha_t \left(\rho dW_t +\sqrt{1-\rho^2} dZ_t\right)=\frac{\rho \nu }{(F_t)^{\beta}}dF_t+\nu \alpha_t \sqrt{1-\rho^2} dZ_t\\
\end{split}
\]
Therefore, one can readily find that:
\[
   E\left[ \frac{d\alpha_t}{dF_t}\right] =\frac{\rho \nu }{(F_t)^{\beta}}
\]
The formula for Bartlett correction is thus:
\begin{equation}
\delta^{Bartlett}_{t,T,K}=\left[ 
    \delta^{B}_{t,T,K}+\text{vega}^{B}_{t,T,K} 
    \left(\frac{\partial \sigma_B}{\partial F}+ \frac{\partial \sigma_B}{\partial \alpha}\frac{\rho \nu }{(F_t)^{\beta}}\right)
\right]e^{(r-q) (T-t)}
\label{eq:bartlett}
\end{equation}
with $\frac{\partial \sigma_B}{\partial \alpha}$ being the shortened form for 
$\frac{\partial \sigma_B (F_t,t,T,K;\alpha,\beta,\nu,\rho)}{\partial \alpha}$.
The Bartlett correction improves over the SABR delta in two different aspects:
\begin{itemize}
    \item Empirically, the Bartlett correction is less sensitive to the choice of $\beta$ parameter. In practice, the $\beta$ is often fixed instead of being calibrated together with $\alpha,\nu,\rho$ in SABR model. Different choices of $\beta$ can often all fit the market prices but different choices of $\beta$ can often lead to different SABR delta $\delta^{SABR}_{t,T,K}$ position. On the other hand, the $\delta^{Bartlett}_{t,T,K}$ is consistent with different choices of $\beta$. This phenomenon is shown in Figure \ref{fig:Bartlett}. We calibrate a SABR model for S\&P 500 index option with trading time $t$ being 2012-01-04 and expiry being 2012-12-31. We compute and compare delta position and we can see that for SABR delta, different choice of $\beta$ can lead to  significantly different hedging position while the Bartlett delta position is consistent.
    \begin{figure}[htp!]
        \includegraphics[width=\textwidth]{./figures/Bartlett}
        \caption{Comparison based on 2012-01-04 S\&P500 index option}
        \label{fig:Bartlett}
    \end{figure}
    \item  Empirically, the Bartlett correction provides better hedging strategy. As an example, one can observe significant  hedging performance difference between $\delta^{Bartlett}_{t,T,K}$ and  $\delta^{SABR}_{t,T,K}$  on S\&P 500 index options. More specifically, we calibrate SABR model on each trading date from Jan 1st 2007 to August 31st 2015 as what we have discussed in section \ref{sec:pricing} with $\beta=1$. And we compare the local hedging performance for all the traded options from Jan 1st 2007 to August 31st 2015.
    Following \cite{hulloptimal},  we use the Gain ratio below as the evaluating criteria of the hedging performance measure:
    \begin{equation}
    \textsc{Gain}=1-\frac{\sum_{i=1}^m \bigg(\Delta V^{mkt}_{t_i,T_i,K_i}-\delta_{t_i,T_i,K_i} ~ \Delta S_{t_i} \bigg)^2}{\sum_{i=1}^m \bigg(\Delta V^{mkt}_{t_i,T_i,K_i}-\delta^{BS}_{t_i,T_i,K_i} ~ \Delta S_{t_i} \bigg)^2}.
    \label{eq:Gain}
    \end{equation}
    where $m$ is the total number of data instances to be evaluated.  The $\delta^{BS}_{t_i,T_i,K_i}$ is the implied Black–Scholes delta, $\delta_{t_i,T_i,K_i}$  is the hedging position from the method under consideration,  e.g., SABR delta $\delta^{SABR}_{t_i,T_i,K_i}$,  MV delta $\delta^{MV}_{t_i,T_i,K_i}$, and  Bartlett delta $\delta^{Bartlett}_{t_i,T_i,K_i}$. $\Delta V^{mkt}_{t_i,T_i,K_i}$ and $\Delta S_{t_i}$
    are the changes in market option and underlying prices over a fixed time interval. Here we compare the \textbf{daily} local hedging risk with the daily changes of prices. The results are shown in Table \ref{table:Bartlett}:
    \begin{table}[htp!]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            Method& SABR $\delta^{SABR}_{t,T,K}$ & Bartlett $\delta^{Bartlett}_{t,T,K}$\\
            Gain (\%) &-4.2 & 27.1 \\
            \hline
        \end{tabular}
        \caption{Daily Local Hedging Risk Comparison Between  $\delta^{SABR}_{t,T,K}$ and   $\delta^{Bartlett}_{t,T,K}$}
        \label{table:Bartlett}
    \end{table}
    As we can see in Table \ref{table:Bartlett}, $\delta^{Bartlett}_{t,T,K}$ performs much better than $\delta^{SABR}_{t,T,K}$. What is more, $\delta^{SABR}_{t,T,K}$ actually performs worse than Black–Scholes delta $\delta^{BS}_{t,T,K}$ with implied volatility. The similar phenomenon has been observed in \cite{lassance2018comparison} where Heston and Heston-Nandi model performance worse than Black–Scholes delta $\delta^{BS}_{t,T,K}$ with implied volatility.  \textbf{The difference of the hedging performance between $\delta^{Bartlett}_{t,T,K}$ and $\delta^{SABR}_{t,T,K}$ is an example to illustrate how the unaccounted  pricing model parameter dependence on underlying asset affects the hedging performance. }Unfortunately, the pricing model parameter dependence often is not that easy to be accounted for as in SABR model with Bartlett correction.
\end{itemize}






Lastly, we introduce the corrective formula for minimum variance hedging  based on  SABR model implemented by  \citet{hulloptimal} . We use $\delta^{SV}$ to denote the minimum variance hedging  based on  SABR model from \cite{hulloptimal}. Let $\Delta F$ be a small changes in forward $F$, the  $\delta^{SV}$ is given by:
\begin{equation}
	\delta^{SV}_{t,T,K}=\frac{V_B(F_t+\Delta F,t,T,K,r;\sigma_B (F_t+\Delta F,t,T,K;\alpha,\beta,\nu,\rho))-V_B(F_t,t,T,K,r;\sigma_B (,t,T,K;\alpha,\beta,\nu,\rho))}{\Delta F}
	\label{eq:HullWhiteSabr}
\end{equation}
with $V_B$ be the Black pricing formula and $\sigma_B$ be the SABR implied volatility formula discussed in section \ref{sec:SABR}. 

\section{Overview of the Data-Driven Hedging Models}
In the previous sections, we discuss the issue of parameter dependence and present several correction methods under Black–Scholes framework. Although the corrective formula   \eqref{eq:HullWhite}, \eqref{eq:HullWhiteLVF}, \eqref{eq:sabrV3}, and \eqref{eq:bartlett} adopt similar forms, the data-driven MV delta \eqref{eq:HullWhite} is significantly different from other corrective formula for the fact that it is based on historical data while \eqref{eq:HullWhiteLVF} ,\eqref{eq:sabrV3}, and \eqref{eq:bartlett} are based on pricing model calibrated to the spot options and underlying prices. \citet{hulloptimal} indicate that MV delta $\delta^{MV}_{t,T,K}$ estimated based on historical data performs better than $\delta^{LVF}_{t,T,K}$ and $\delta^{SV}_{t,T,K}$ estimated based on spot data. 

Our exploration on the data-driven models for hedging start roughly the same years as the work of \cite{hulloptimal} and we adopt the similar methodology as the MV delta where we learn a hedging position function from historical data. However, our explorations are motivated differently:
\begin{itemize}
    \item The hedging position  $\delta^{MV}_{t,T,K}$, $\delta^{LVF}_{t,T,K}$ and $\delta^{SV}_{t,T,K}$ proposed in \cite{hulloptimal} is to correct for the Black–Scholes implied volatility dependence on underlying asset. Specifically, \citet{hulloptimal} demonstrate how one can estimate the $\frac{\partial \sigma^{imp}}{\partial S}$ empirically.
    \item Noticing the pricing parameters dependence, we try to learn a hedging position directly from the market data.  We are not specifically trying to correct  parameters dependence for certain pricing models. We are trying to avoid it  by obtaining hedging position without a pricing model at all.
\end{itemize}
In addition, the data-driven models proposed in this thesis and MV delta are also different in the following ways:
\begin{itemize}
    \item \citet{hulloptimal} assume a quadratic form to account for the implied volatility dependence on the underlying asset within Black–Scholes framework.  The hedging position from our proposed data-driven models is  purely determined by market data with machine learning algorithms with no specific parametric form.
    \item The MV hedging model \eqref{eq:HullWhite} focuses on instantaneous hedging analysis  \eqref{eq:HE}. For discrete hedging, particularly when re-balancing is done infrequently, e.g., weekly or monthly, \eqref{eq:HullWhite}   may no longer be suitable. Our proposed models, which are not based on computing sensitivity of option pricing functions with regards to the underlying asset, can potentially improve the hedging results when the hedging is done less frequently.
    \item Our proposed data-driven models $\model$ and $\modelT$ can naturally include feature selection and feature extraction which can potentially improve the hedging performance.
    \item Our proposed data-driven model $\modelT$ is enhanced to deal with total hedging scenarios instead of focusing on reducing the instantaneous hedging risk as the   MV hedging model \eqref{eq:HullWhite}.
\end{itemize}
In the following chapters, we will start to formally describe the proposed hedging models and present numerical hedging performance comparison on both synthetic data and real market data.

\chapter{Data-Driven Kernel Learning Framework for Local Hedging Risk}
\label{sec:kernel}

In this chapter, we start the discussion on our proposed data-driven local hedging model from kernel learning framework. The data-driven kernel hedging model can be summarized as the following:
Assume we have $M$ data instances.  Each observation of a market option price  $V^{mkt}_{t_i,K_i,T_i}$ is uniquely associated with a triplet $\{t_i,T_i,K_i\}$, where $t_i$ is the trading time of the option price,  $K_i$ is the strike, and expiry $T_i$, $i=1,\ldots, M$.    The hedging position function is determined by the quadratic data-driven local risk minimization problem:
\[
\min ~\frac{1}{2M}\sum_{i=1}^{M} \left(\Delta S_{t_i} \delta(\vx_{t_i}^{T_i,K_i})- \Delta V^{mkt}_{t_i,K_i,T_i} \right)^2
\]
With $\Delta S_{t_i}$ and $\Delta V^{mkt}_{t_i,K_i,T_i}$ given in  \eqref{eq:DVDS} for a fixed time interval $\DT$.
The hedging position $\delta(\vx_{t_i}^{T_i,K_i})$ is given by a data-driven hedging function $\DKLs(\vx_{t}^{T,K};\valpha)$:
\[
	\delta(\vx_{t_i}^{T_i,K_i})=\DKLs(\vx_{t}^{T,K};\valpha)
\]
with  $\valpha$ being the parameters for the hedging functions and $\vx_{t}^{T,K}$ being the input features for the hedging function. Please note that $\valpha$ in this chapter is the parameter for the regularized kernel network to be learnt from data. The $\alpha$ in section \ref{sec:SABR} is the volatility for SABR. They are two unrelated terminologies.

From chapter \ref{sec:Background}, we have seen various challenges for hedging when the position is computed from the calibrated option value function. Since nonparametric option function estimation does not make specific assumptions and can potentially match option prices more accurately,  it is not unreasonable to  expect that this hedging challenge can potentially be addressed by reducing mis-specification using a data-driven approach to learn an option value function. Here we in this chapter we also discuss this approach and analyze its challenging for option hedging. Similar to discrete hedging under the parametric model, the hedging position can be computed by learning a nonparametric option value model and then determining hedging position from the partial derivatives. Indeed this is the approach adopted in \citep{hutchinson}.
In this chapters, we indicates that the issue of pricing model parameters dependence still exists even if one estimate the pricing model using a machine learning model with no assumption on the dynamic of the underlying asset movement. 

The organization of this chapter is as the following:
We discuss how one can compute a hedging positions from partial derivatives of the optimal regularized kernel functions in section \ref{sec:VF}. The proposed data-driven kernel hedging learning method is presented in section \ref{sec:KernelDirect}. In section \ref{sec:cross}, we discuss how cross-validation can be done efficiently for both kernel pricing and kernel hedging model. In section \ref{sec:Synexperiment}, we present experiments using synthetic data to illustrate the drawbacks of determining hedging position from partial derivative of pricing function estimated using machine learning algorithms and the effectiveness of data-driven direct kernel hedging functions.

\section{Regularized Kernel Pricing Model} 
\label{sec:RegularizedNW}
 Recognizing various challenges in the parametric financial modeling approach,
 nonparametric option pricing has also been studied. The nonparametric option value modeling approach has the distinctive advantage of not relying on specific assumptions about the underlying asset price dynamics. \citet{hutchinson} first propose a nonparametric data-driven approach to price and hedge European options using neural networks, radial basis functions, and projection pursuit regressions. Many other neural network methods for European option pricing have also been proposed, see, e.g., \citep{yao2000option,bennell2004black,gradojevic2009option,garcia2000pricing,malliaris1993neural}.


Although there are  quite a few studies on  nonparametric option pricing models, to our knowledge, there has been little research specifically focusing on discrete hedging using a nonparametric method. Even when the hedging problem is considered, e.g.,  \citep{hutchinson}, it is treated as a byproduct of obtaining a nonparametric  pricing function: The hedging position is obtained from the partial derivative of the option value function. \citet{hutchinson} show that, based on hedging errors on some simulated paths,  this indirect data-driven hedging approach can potentially be an effective alternative to the traditional parametric delta hedging methods.

In this section, we follow the methodology of \cite{hutchinson} and learn a data-driven option pricing model with the regularized kernel network. The hedging position is then given by the partial derivative of the option pricing model with regards to the underlying asset. Our goal is to learn a nonlinear option pricing function $V(\vx;\valpha)$  using a regularized kernel method \cite{evgeniou2000regularization}. 

Assume that we are given a positive definite kernel similarity
 $$
 \mathcal{K}(\vx, \vx^\prime) : \Real^{d} \times \Real^{d} \rightarrow  \Real,
 $$
 which captures  similarity between $\vx$ and $\vx^\prime$ implicitly in a high dimensional feature space.
 Assume that $\HK$ is the  \emph{Reproducing Kernel Hilbert Space} (RKHS) induced by the symmetric positive definite kernel function $ \mathcal{K}(\mathbf{x},\mathbf{y})$ and $\|f\|_{ \mathcal{K}}$ is the norm in RKHS.
 We have the following Representer Theorem \citep{wahba1990spline}:
\begin{thm}\label{pc:representer}
 (The Representer Theorem)
Let $\XS$ be a nonempty set and $k$ a positive definite real-valued kernel on $\XS \times \XS$ with the RKHS $\HK$.  Given the a training sample $
 \{\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_M\} $, a strictly monotonically increasing real valued function $g:[0, \infty) \rightarrow \Real $,  and an arbitrary empirical risk function $E:\XS^M \rightarrow \Real $, then for any $f^* \in \HK$ satisfying
 \[
 f^*=\argmin_{f \in \HK} \;\;E\left(f(\mathbf{x}_1), f(\mathbf{x}_2), \cdots, f(\mathbf{x}_M)\right) + g( \|f\|_{\mathcal{K}})
 \]
 $f^*$ admits a representation of the form:
 \[
 f^*(\cdot)=\sum_{i=1}^M \widehat{\alpha}_i \mathcal{K}(\cdot,\mathbf{x}_i)
 \]
 with $\widehat{\alpha}_i \in \Real$  for all $1 \leq i \leq M$
\end{thm}

A regularized kernel regression problem can be formulated as
\begin{equation}
\min_{f \in \HK}\left(  \sum_{i=1}^M L(f(\vx_i))+\lambda_P \|f\|^2_{\mathcal{K}}\right)
\label{eq:indirect}
\end{equation}
where $L(\cdot)$ is a loss function.
The regularization parameter $\lambda_P>0$  can be determined based on cross validation.

Following the Representer Theorem,  e.g., \citep{wahba1990spline}, a solution of  \eqref{eq:indirect} has the form
\begin{equation}\label{kr1}
f(\mathbf{x})=\sum_{i=1}^M \widehat{\alpha}_i \mathcal{K}(\mathbf{x},\mathbf{x}_i)
\end{equation}
and the regularization term is given by
\begin{equation}\label{kr2}
\|f\|_{\mathcal{K}}^2=\sum_{i=1}^M \sum_{j=1}^M  \widehat{\alpha}_i \widehat{\alpha}_j \mathcal{K}(\mathbf{x}_i,\mathbf{x}_j).
\end{equation}


Assume that a set of $M$ training points $
 \{ (\vx_{t_1}^{T_1,K_1},V^{mkt}_{t_1,K_1,T_1}), \cdots, (\vx_{t_M}^{T_M,K_M},V^{mkt}_{t_M,K_M,T_M})\} $ are given, where
$
\vx_{t}^{T,K}  \in \Real^{d_l}
$ is the input feature for pricing the option with strike $K$ and expiry $T$ at time $t$ and  $V^{mkt}_{t,K,T}=\Vmkt(t,T,K)$ is the market option price at time $t$ with strike $K$ and expiry $T$.  Each data instance corresponds to a unique triplet $\{t,T,K\}$.
We can estimate an option value function $V(\vx;\mathbf{\valpha})$ based on the regularized kernel estimation \eqref{eq:indirect} with $\valpha=\{\widehat{\alpha}_1,\dots,\widehat{\alpha}_M\}$.


Using \eqref{kr1} and \eqref{kr2}, assuming quadratic loss, the option pricing function:
\[
	V(\vx_{t}^{T,K};\mathbf{\widehat{\alpha}})=\sum_{i=1}^N \widehat{\alpha}_i \mathcal{K}(\vx_{t}^{T,K},\vx_{t_i}^{T_i,K_i})
\] can be computed by solving
\begin{equation}
\min_{\mathbf{\widehat{\alpha}}} \left(  \sum_{i=1}^M \left( \Vmkt_{t_i,T_i,K_i}-\sum_{j=1}^M \widehat{\alpha}_j \mathcal{K}(\vx_{t_j}^{T_j,K_j},\vx_{t_i}^{T_i,K_i}) \right)^2+\lambda_P \sum_{i=1}^M\sum_{j=1}^M  \widehat{\alpha}_i \widehat{\alpha}_j \mathcal{K}(\vx_{t_j}^{T_j,K_j},\vx_{t_i}^{T_i,K_i})\right)
\label{eq:indirectf}
\end{equation}


For standard options, the universal RBF kernel
\begin{equation} \label{eq:RBF}
\mathcal{K}(\vx,\tilde{\vx})=e^{-\frac{\|\vx-\tilde{\vx}\|_2^2}{2 \rho^2}}
\end{equation}
is a reasonable kernel choice, since the option value function is very smooth, and
a suitable bandwidth $\rho$ is typically problem dependent and can be determined using cross validation.


\subsection{Indirect Hedging Positions From Kernel Pricing Functions} \label{sec:VF}

Assume that the one dimension  of the attribute vector $\vx_{t}^{T,K} \in \Real^{d_l}$ corresponds to the moneyness $S_t/K$ at time $t$ and other dimensions are not related to underlying prices. Then the delta hedging function option with strike $K$ and expiry $T$ at trading time $t$ is typically determined as:
\begin{equation}\label{eq:dV}
	\delta^{IKL}_{t,T,K}=
\frac{\partial V(\mathbf{x}_{t}^{T,K};\valpha)}{\partial S}=\sum_{i=1}^M  \widehat{\alpha}_i\frac{\partial \mathcal{K}(\mathbf{x}_{t}^{T,K},\vx_{t_i}^{T_i,K_i})}{\partial S} = \sum_{i=1}^M  \frac{\widehat{\alpha}_i}{K} \frac{\partial \mathcal{K}(\mathbf{x}_{t}^{T,K},\vx_{t_i}^{T_i,K_i})}{\partial S/K} 
\end{equation}
\citet{hutchinson} demonstrate that this nonparametric hedging approach,  using the partial derivative of a nonparametric pricing function learned from historical market data, can be a useful alternative for option hedging.

However, using \eqref{eq:dV} as the hedging position similarly does not minimize variance of hedge risk in general and the challenge in accounting for parameter dependence on the underlying remains. We can see that from the  following arguments. We made an unrealistic assumption that the estimated kernel function $V(\mathbf{x}_{t}^{T,K};\valpha)$ matches the target market option price exactly, i.e.,
$$
V(\mathbf{x}_{t}^{T,K};\valpha) = \Vmkt_{t,T,K}
$$
and $$\frac{d V}{d S}(\mathbf{x}_{t}^{T,K};\valpha)=\frac{d  \Vmkt}{d S}$$
Then we have:
$$
\frac{d V}{d S}(\mathbf{x}_{t}^{T,K};\valpha)=\sum_{i=1}^M \widehat{\alpha}_i \frac{\partial \mathcal{K}(\mathbf{x}_{t}^{T,K},\vx_{t_i}^{T_i,K_i})}{\partial S} + \sum_{i=1}^M \frac{\partial \widehat{\alpha}_i}{\partial S}\mathcal{K}(\vx,\vx_i)=\frac{d \Vmkt}{d S}
$$
Hence in general
$$
\frac{\partial \widehat{\alpha}_i}{\partial S} \neq 0
$$
unless
\begin{equation}\label{eq:dep}
\frac{\partial V}{\partial S}(\vx_{t}^{T,K};\valpha) = \frac{d  \Vmkt}{d S}.
\end{equation}
However there is no reason that a solution of the regression problem (\ref{eq:indirectf}) should satisfy \eqref{eq:dep}. Consequently it is similarly difficult to account for all dependence from $\valpha$ on the underlying asset, even infinitesimally, in the estimated kernel model.



Furthermore, error magnification can happen by deriving the hedging position from a estimated kernel function. In general, the estimated kernel pricing function $V(\mathbf{x}_{t}^{T,K};\valpha)$ can not match the all the target market option prices exactly and the  estimated kernel pricing function inevitably will have prediction error when used to predict the price of unobserved testing data instances. The calibration error and prediction can potentially increase the hedging error when using the $\frac{\partial V}{\partial S}(\vx_{t}^{T,K};\valpha)$ as the hedging position.


\section{Regularized Kernel Hedging Model}
\label{sec:KernelDirect}
Let us again assume that a set of $M$ training points 
\[
 \left\{ (\vx_{t_1}^{T_1,K_1},\Delta V^{mkt}_{t_1,K_1,T_1},\Delta \Smkt_{t_1}), \cdots, (\vx_{t_M}^{T_M,K_M},\Delta V^{mkt}_{t_M,K_M,T_M},\Delta \Smkt_{t_M})\right\} \]
 are given, where
$
\vx_{t}^{T,K}  \in \Real^{d_l}
$ is the input feature for \textbf{hedging} the option with strike $K$ and expiry $T$ at time $t$. The $\Delta V^{mkt}_{t,K,T}$ is the change of  market option price at time $t$ with strike $K$ and expiry $T$ over a fixed time intervel $\DT$, e.g., daily, weekly, or monthly. The $\Delta \Smkt_{t}$ is the change of  underling price at time $t$ over a fixed time intervel $\DT$ over the same $\DT$. 
$\Delta V^{mkt}_{t,K,T}$ and  $\Delta \Smkt_{t}$  are given by
equation \eqref{eq:DVDS}.
Again, each data instance corresponds to a unique triplet $\{t,T,K\}$.

We can estimate an option hedging function $\delta(\vx_{t}^{T,K};\mathbf{\valpha})$ based on the regularized kernel network with $\valpha=\{\widehat{\alpha}_1,\dots,\widehat{\alpha}_M\}$. The empirical loss function is chosen to correspond to the square of discrete local hedging risk in section \ref{sec:DiscreteHedgingCriteria}, i.e., 
\begin{equation}\label{eq:HR}
L\left(\delta(\vx_{t}^{T,K};\valpha)\right)=\left(\Delta V^{mkt}_{t,K,T}-\DS_{t} \delta(\vx_{t}^{T,K};\valpha)\right)^2.
\end{equation}
The hedging position function $\delta(\vx_{t}^{T,K};\valpha)$  can be estimated from  the regularized optimization below:
\begin{equation} \label{fhedge}
\min_{\delta \in \HK}\left\{\sum_{i=1}^M L\left(\delta(\vx_{t_i}^{T_i,K_i};\valpha)\right)^2+\lambda_P \|\delta\|^2_\mathcal{K}\right\}
\end{equation}
Note that the Representer Theorem still holds for \eqref{fhedge}.
Using the Representer Theorem \eqref{kr1} and \eqref{kr2}, \eqref{eq:directf} can be computed by solving the following convex quadratic minimization,
\begin{equation}
	\min_{\mathbf{\valpha}} \left(  \sum_{i=1}^M \left( \Delta \Vmkt_{t_i,T_i,K_i}-\Delta S_{t_i} \sum_{j=1}^M \widehat{\alpha}_j \mathcal{K}(\vx_{t_j}^{T_j,K_j},\vx_{t_i}^{T_i,K_i}) \right)^2+\lambda_P \sum_{i=1}^M\sum_{j=1}^M  \widehat{\alpha}_i \widehat{\alpha}_j \mathcal{K}(\vx_{t_j}^{T_j,K_j},\vx_{t_i}^{T_i,K_i})\right)
	\label{eq:directf}
\end{equation}


The loss function for the hedging function in the proposed formulation \eqref{fhedge}
directly corresponds to the sum of square of local hedging risk for a discrete $\DT$-time period. In addition the hedge function is the solution of the optimization problem  and there is no potential error magnification through partial derivative computation. We avoid the pricing model parameters dependence on underlying asset by not computing a prcing model at all.




Since at the expiry the delta of the payoff function is a discontinuous step function, the  delta hedging function of an option changes quickly as the underlying changes near the expiry.
Consequently we choose to use a spline kernel function \citep{vapnik1998statistical} for the hedging function estimation.
The explicit expression for the spline kernel with order $O_d$  for the one-dimensional case is :
\[
\mathcal{K}(x,y)=\sum_{r=0}^{O_d} \frac{\binom{O_d}{r}}{2 O_d-r+1} min(x,y)^{2 O_d-r+1}|x-y|^r +\sum_{r=0}^{O_d} x^ry^r
\]

For multidimensional data, the spline kernel is the product of one-dimensional spline kernel functions with regard to each dimension. Interested readers can referred to \citep{vapnik1998statistical} for more details. The major benefit of using the spline kernel over the gaussian kernel  is that spline kernel does not have a hyperparameter to be tuned while  gaussian kernel has  a bandwidth parameter to be tuned by cross-validation which is often costly. In this thesis, we majorly use spline kernel and, in section 
\ref{sec:Synexperiment}, we will show that spline kernel performs better or equally well when compared with gaussian kernel.  In the latter discussion, we denote $\delta^{DKL}_{t,T,K}= \delta(\vx_{t}^{T,K};\mathbf{\valpha})$ as the hedging position form the directly kernel hedging model.


\section{Cross Validation} \label{sec:cross}
 For our data-driven approach, we need to select an appropriate penalty $\lambda_P$ to control the model complexity. Cross-validation (CV) is a commonly used method for the performance estimation and model selection for the learning algorithms. For example, the Leave-One-Out Cross-Validation (LOOCV) computes the output for each data instance using parameters trained on the remaining data instances. For the regularized kernel methods, we can compute the CV error efficiently without retraining the model in each CV round.


Recall that, for the regularized network or pricing model, the minimization problem is
\[
\min_{\mathbf{\valpha}} \left(  \sum_{i=1}^M \left( \Vmkt_{t_i,T_i,K_i}-\sum_{j=1}^M \widehat{\alpha}_j \mathcal{K}(\vx_{t_j}^{T_j,K_j},\vx_{t_i}^{T_i,K_i}) \right)^2+\lambda_P \sum_{i=1}^M\sum_{j=1}^M  \widehat{\alpha}_i \widehat{\alpha}_j \mathcal{K}(\vx_{t_j}^{T_j,K_j},\vx_{t_i}^{T_i,K_i})\right)
 \]
Let $\vK$ be the kernel matrix with $\vK_{ij}=\mathcal{K}(\mathbf{x}_i,\mathbf{x}_j)$. (Note $\vK$ here denotes the kernel matrix which should not be confused with the strike $K$.) Problem \eqref{eq:indirect} can be rewritten in matrix form:
\begin{equation}
\min_{\valpha \in \Real^M} \; ( \vK \valpha - \vecVmkt)^T ( \vK \valpha -\vecVmkt)+ \lambda_P \valpha^T\vK\valpha
\label{eq:CV2}
\end{equation}
with
\[
	\vecVmkt=\{
	\Vmkt_{t_1,T_1,K_1},\dots,	\Vmkt_{t_M,T_M,K_M}
	\}	\;\;\;,\;\;\; \valpha=\{\widehat{\alpha}_1,\dots,\widehat{\alpha}_M\}
\]
The solution to \eqref{eq:CV2} is:
\[
\valpha  =(\vK+\lambda_P \pmb{I})^{-1}\vecVmkt
\]
Given the eigen-decomposition $\vK=\pmb{Q} \pmb{\Lambda} \pmb{Q}^T$, we can easily see that:
\begin{equation}\label{eq:eig}
(\vK+\lambda_P \pmb{I})^{-1}=\pmb{Q}(\pmb{\Lambda}+\lambda_P \pmb{I})^{-1} \pmb{Q}^T
\end{equation}
Because $(\pmb{\Lambda}+\lambda_P I)$ is a diagonal matrix, given the eigen-decomposition $\vK=\pmb{Q} \pmb{\Lambda} \pmb{Q}^T$, we can get different solutions to \eqref{eq:CV2}  as $\lambda_P$ varies in $O(M^2)$.

Let $V(\vx_{t_j}^{T_j,K_j}; \valpha)$ be the output for data instance $j$ when regularized kernel methods \eqref{eq:indirectf} is trained on all training data instances. Let $V^l(\vx_{t_j}^{T_j,K_j}; \valpha^l)$  be the output for data instance $j$ when regularized kernel methods \eqref{eq:indirectf} is trained on all training data instances except $\vx_{t_l}^{T_l,K_l}$. 
Let $\vecVmkt^l=\{\vecVmkt^l_1,\vecVmkt^l_2,\cdots,\vecVmkt^l_M\}$ be the vector where $\vecVmkt^l_{j} = \Vmkt_{t_j,T_j,K_j}$ for $j \neq l$ and $\vecVmkt^l_l = V^l(\vx_{t_l}^{T_l,K_l}; \valpha^l)$. Since $V^l(\cdot; \valpha^l)$ is the model trained on all example except $\vx_{t_l}^{T_l,K_l}$ , it is easy to see that $V^l(\cdot; \valpha^l)$  minimizes
\begin{equation}
\min_{\valpha \in \Real^M} \; ( \vK \valpha - \vecVmkt^l)^T ( \vK \valpha -\vecVmkt^l)+ \lambda_P \valpha^T\vK\valpha
\label{eq:CV3}
\end{equation}
The solution to \eqref{eq:CV3} is:
\[
\valpha^l  =(\vK+\lambda_P \pmb{I})^{-1}\vecVmkt^l
\]
Let $\pmb{B}=\vK (\vK+\lambda_P \pmb{I})^{-1}$, therefore, we have 
\[\pmb{B}\vecVmkt=\{
	V(\vx_{t_1}^{T_1,K_1}; \valpha),
	\dots,
	V(\vx_{t_M}^{T_M,K_M}; \valpha)
	\}
\]
We can easily show that:
\[
	V^l(\vx_{t}^{T,K}; \valpha^l)-V(\vx_{t}^{T,K}; \valpha)=\sum_{i=1}^M B_{li}(\vecVmkt_i^l-\vecVmkt_i)=B_{ll}(V^l(\vx_{t_l}^{T_l,K_l}; \valpha^l)-\vecVmkt_l)=B_{ll}(V^l(\vx_{t_l}^{T_l,K_l}; \valpha^l)-\Vmkt_{t_l,T_l,K_l})
\]
Thus,
\begin{equation} \label{eq:fastCV}
	V^l(\vx_{t_l}^{T_l,K_l}; \valpha^l)=
	\frac{V(\vx_{t_l}^{T_l,K_l}; \valpha^l)-B_{ll}\Vmkt_{t_l,T_l,K_l}}
	{1-B_{ll}}
\end{equation}
Therefore, we can get the $V^l(\vx_{t_l}^{T_l,K_l}; \valpha^l)$  without actually retraining the model. The leave-one-out estimations for all data instance are:
\begin{equation} \label{eq:fastCV2}
\vecVmkt_{loo}=(I-B_{LL})^{-1}(B\vecVmkt-B_{LL}\vecVmkt)
\end{equation}
and the leave-one-out errors for all data instance are:
\begin{equation} \label{eq:fastCV3}
\vecVmkt-\vecVmkt_{loo}=\vecVmkt-(\pmb{I}-B_{LL})^{-1}(\pmb{B}\vecVmkt-\pmb{B}_{LL}\vecVmkt)=(I-\pmb{B}_{LL})^{-1}(\vecVmkt-\pmb{B}\vecVmkt)
\end{equation}
where $B_{LL}$ is a diagonal matrix with $B_{ii},\;\; i=1,\dots,M$ on its diagonal.

We can further simplify the expression by noting the fact that:
\[\begin{split}
\pmb{B}&=\vK (\vK+\lambda_P \pmb{I})^{-1} \\
       &=\pmb{Q}\pmb{\Lambda}(\pmb{\Lambda}+\lambda_P \pmb{I})^{-1} \pmb{Q}^T\\
       &=\pmb{Q}(\pmb{\Lambda}+\lambda_P \pmb{I}-\lambda_P \pmb{I})(\pmb{\Lambda}+\lambda_P \pmb{I})^{-1} \pmb{Q}^T\\
       &= \pmb{I}-\lambda_P Q (\pmb{\Lambda}+\lambda_P \pmb{I})^{-1} \pmb{Q}^T=\pmb{I} -\lambda_P  (\vK+\lambda_P \pmb{I})^{-1}
  \end{split}
\]
Therefore, we can get:
\[
(\pmb{I}-\pmb{B}_{LL})=diag(\pmb{I}-\pmb{B})=\lambda_P \; diag((\vK+\lambda_P \pmb{I})^{-1})
\]
\[
\vecVmkt-B\vecVmkt=\lambda_P (\vK+\lambda_P \pmb{I})^{-1} \vecVmkt =\lambda_P \valpha
\]
Finally, we can get:
\begin{equation}  \label{eq:fastCV4}
\vecVmkt-\vecVmkt_{loo}=diag((\vK+\lambda_P \pmb{I})^{-1})^{-1} \valpha
\end{equation}

Given the eigen-decomposition $\vK=\pmb{Q} \pmb{\Lambda} \pmb{Q}^T$, we can compute  $\valpha$ as $\lambda_P$ varies in $O(M^2)$. Similarly, given the eigen-decomposition $\vK=\pmb{Q} \pmb{\Lambda} \pmb{Q}^T$, we can compute the diagonal of $(\vK+\lambda_P \pmb{I})^{-1}$ in $O(M^2)$. Then  using \eqref{eq:fastCV4}, the LOOCV errors can be computed in $O(M^2)$.  It can been further shown \cite{pahikkala2006fast} that, given the eigen-decomposition $\vK=\pmb{Q} \pmb{\Lambda} \pmb{Q}^T$, the computational complexity for the $n$-fold cross-validation ($n$FCV) is $O(M^3/n)$. Interested readers can referred to \citep{pahikkala2006fast, wahba1990spline} for more details.

Let $\Delta \vecVmkt=\{\Delta \Vmkt_{t_1,T_1,K_1},\cdots,\Delta \Vmkt_{t_M,T_M,K_M}\}$ and  let $\pmb{D}$ be a diagonal matrix with $D_{ii}=\Delta S_{t_i}. \;i=1,\cdots,M.$ on its diagonal.
Similarly, we can rewrite  the minimization problem \eqref{eq:directf} in matrix form:
\begin{equation}
\min_{\valpha \in \Real^m} \; ( \pmb{D} \vK \valpha - \Delta \vecVmkt)^T (\pmb{D}  \vK \valpha -\Delta \vecVmkt)+ \lambda_P \valpha^T\vK\valpha
\label{eq:directfM}
\end{equation}
Let $\widetilde{\vK}=\pmb{D}\vK$, the solution to \eqref{eq:directfM} can be obtained by:
\[
\valpha = (\widetilde{\vK}^T\widetilde{\vK}  + \lambda_P \vK)^{-1} \widetilde{\vK}^T \Delta \vecVmkt
\]
Unfortunately, we can not directly use the fast cross validation algorithms from \cite{pahikkala2006fast, wahba1990spline} for the minimization problem \eqref{eq:directfM}.
Because $(\pmb{\widetilde{K}}^T\pmb{\widetilde{K}}  + \lambda_P \vK)^{-1}$ can not be easily inverted in $O(M^2)$ as $\lambda_P$ varies.

However if we change the penalty term for \eqref{eq:directfM} from $\lambda_P \valpha^T\vK\valpha$ to $\lambda_P \valpha^T\valpha$, the minimization problem becomes:
\begin{equation}
\min_{\valpha \in \Real^m} \; ( \pmb{D} \vK \valpha - \Delta\vV)^T (\pmb{D}  \vK \valpha -\Delta\vV)+ \lambda_P \valpha^T\valpha
\label{eq:directfMV2}
\end{equation}
The solution to \eqref{eq:directfMV2} can be obtained by:
\[
\valpha = (\widetilde{\vK}^T\widetilde{\vK}  + \lambda_P \pmb{I})^{-1} \widetilde{\vK}^T \Delta \vecVmkt
\]
Utilizing the ideas from \cite{pahikkala2006fast, wahba1990spline}, we can similarly show that,  given the singular value decomposition $\pmb{\widetilde{K}}=\pmb{U} \pmb{\Sigma} \pmb{V}^T$, for the problem \eqref{eq:directfMV2},  the computational complexity for the LOOCV is still $O(M^2)$ and the computational complexity for the $n$FCV is still $O(M^3/n)$. In practice, changing the penalty term from $\lambda_P \valpha^T\vK\valpha$ to $\lambda_P \valpha^T\valpha$ will not affect the actual performance too much. Therefore, in order to improve the computation efficiency for the direct data driven approach, we are solving the problem \eqref{eq:directfMV2}.






\section{Comparison using synthetic data}\label{sec:Synexperiment}
Following the S\&P 500  market option specifications in \citep{hull2006options},  we first synthetically generate option data assuming that the underlying price follows a Heston model \citep{heston1993closed}.
We  compare hedging effectiveness of direct hedging function learning \eqref{fhedge}  and indirect hedging function estimation  \eqref{eq:indirectf}. In addition, we compare their performance to that of using  analytic delta under the  Heston model, which can be regarded as a best case benchmark, at least for daily hedging. Note that there is no model mis-specification  for the underlying price in this synthetic case.

The experiments in the this section is to demonstrate the following aspects:
\begin{itemize}
\item Computing the hedging position as the partial derivative of an nonparametric option pricing model with regards to underlying asset still suffers from the issue of parameter dependence on underlying asset.
\item The hedging position directly learnt from data as in \eqref{eq:directf} can be very close to the  best case benchmark in daily hedging.
\item The hedging position directly learnt from data using spline kernel performs better when compared with gaussian kernel.
\end{itemize}

Since the loss function is  quadratic, solutions to \eqref{fhedge} and \eqref{eq:indirectf}
can be easily computed from linear equation solvers.
In addition we also compare hedging performance using a RBF kernel \eqref{eq:RBF} versus a spline kernel.
Specifically, using the generated synthetic data, we compare here hedging performance  of the following hedging computation methods:
 \begin{itemize}
 \item  $\delta^{BS}_{t,T,K}$:  implied  volatility BS delta
 \item $\delta^{Heston}_{t,T,K}$:   analytical Heston delta
 \item $\DKLs$ : direct learning a spline kernel hedging  function based on \eqref{fhedge}
 \item $\DKLg$ : directly learning a RBF kernel hedging  function directly based on \eqref{fhedge}
 \item ${\IKLs}$: determining hedging position indirectly  as the partial derivative \eqref{eq:dV} of the  option value function   estimated from \eqref{eq:indirectf} using a spline kernel

\item ${\IKLg}$: determining hedging position indirectly  as the partial derivative \eqref{eq:dV} of the  option value function   estimated from \eqref{eq:indirectf} using a RBF kernel

\end{itemize}

Training data consists of simulated daily underlying price $S_t$ for two years\footnote{We assume that there are 252 trading days in a year.}, $t=1,\cdots, 2\times 252$, assuming a  risk-neutral Heston model below:
\[
	\begin{split}
	dS&=(r-q) S dt + \sqrt{\upsilon} S \hat{dW}\\
	d\upsilon&=\kappa^*(\overline{\upsilon}^*-\upsilon)dt+\eta \sqrt{\upsilon}\hat{dZ}\\
	E[\hat{dZ}\hat{dW}]&=\rho dt
	\end{split}
\]
In addition,  a vector of simulated (traded)  market call option prices  are  generated,  on each day $t$, with different strikes and time to expiry, following  the CBOE specifications of stock options described in  \cite{hull2006options}.
 The option prices $\Vmkt$ are  computed using  the analytical option 
 formula \eqref{eq:hestonPrice} under the Heston model \citep{heston1993closed} in section \ref{sec:heston} ,
 using the the parameters  from \citep{bakshi1997empirical}, which are given in Table \ref{para}. In other words, we assume in this section:
 \[
\Vmkt(t,T,K)=V_{Heston}(S_t,t,T,K,r,q;\upsilon,\kappa^*,\overline{\upsilon}^*,\eta,\rho)
 \]

 Please note that the heston delta position under this synthetic scenarios does not have the parameter dependence issue, since for the synthetic case all heston parameter is fixed, and can be used as the benchmark.
 \[
	 \delta^{Heston}_{t,T,K}=\frac{ \partial V_{Heston}(S_t,t,T,K,r,q;\upsilon,\kappa^*,\overline{\upsilon}^*,\eta,\rho)}{\partial S}=\frac{d \Vmkt}{d S}
 \] 
 The Black-Scholes delta position from implied volatility still has the the issue of implied volatility depending on underlying asset:
 \[
	 \delta^{BS}_{t,T,K}=\frac{ \partial V_{BS}(S_t,t,T,K,r,q;\sigma^{imp}_{t,T,K})}{\partial S} \neq \frac{d \Vmkt}{d S}
 \] 
 where $\sigma^{imp}_{t,T,K}$ is the volatility that equals Black–Scholes price and Heston price
 \[
	 V_{BS}(S_t,t,T,K,r,q;\sigma^{imp}_{t,T,K})=V_{Heston}(S_t,t,T,K,r,q;\upsilon,\kappa^*,\overline{\upsilon}^*,\eta,\rho)
 \]

 \begin{table}[htp!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
 $r$ &$q$& $\overline{\upsilon}^*$  & $\kappa^*$ & $\eta$ & $\rho$ & $S_0$ & $\upsilon_0$  \\ \hline
 0.02&0.0&0.04                  & 1.15   & 0.39  & -0.64 & 100 &0.04 \\    \hline
\end{tabular}
\end{center}

\caption{Parameters for the Heston Model}
\label{para}
\end{table}



Testing data consists of 100 daily underlying price paths and corresponding option prices, spanning a six month period. The Heston parameter is still specified as in Table \ref{para}. We report average performance measures over 10 random training-test-data sets generated as described. \footnote{Following CBOE option specification rules, the size of a training or testing data set can vary slightly for each simulation run.}



We train a regularized option price kernel model for the indirect hedging $\IKLs$ and $\IKLg$. For $\IKLs$ and $\IKLg$,
the hedging position is  the partial derivative of the estimated pricing function  \eqref{eq:dV},  as in \citep{hutchinson}. We note that the simulation hedging analysis in \cite{hutchinson} considers only the Black–Scholes model and the results in \citep{hutchinson} indicate that a lower hedging error can be achieved on a subset of simulated paths.


Following a common practice, we use the standard deviation of the pairwise Euclidean distance of the training data as the bandwidth $\rho$ for the RBF kernel. The regularization parameter $\lambda_P$  is however selected using a 5-fold cross-validation. We also consider weekly hedging and monthly hedging, which correspond to hedging over a 5-business-days period and 20-business-days period respectively.

To investigate impact of the feature choice,  we evaluate hedging performance using
\begin{itemize}
\item
$\text{Feature Set \#1} = \{\textsc{moneyness}\; (S_t/K),\;\;\textsc{time-to-expiry}\; (T-t)\}$.
\item
$\text{Feature Set \#2} = \{\textsc{moneyness} \;(S_t/K),\;\;\textsc{time-to-expiry} \;(T-t) ,\;\;~ \delta^{BS}_{t,T,K}\}$.
\end{itemize}
In the second feature set,  the Black–Scholes delta $\delta^{BS}_{t,T,K}$ using the implied volatility  is used as an additional feature in determining the hedging position. 

Let the number of data instances to be evaluated be $m$.
We have the profit and loss \eqref{eq:localPL} of the local hedging portfolio in section \ref{sec:DiscreteLocalRisk} over the fixed time interval $\DT$ for each data instance, which corresponds to a unique triplet $\{t,T,K\}$:
\[
	\Delta P_{H}(t,T,K)=\Delta \Smkt_{t} \delta_{t,T,K} -\Delta V^{mkt}_{t,K,T}
\]
where $\delta_{t,T,K}$ is the hedging position from different approaches, e.g., Black–Scholes delta $\delta^{BS}_{t,T,K}$, Heston delta $\delta^{Heston}_{t,T,K}$, direct data-driven hedging position $\delta^{DKL}_{t,T,K}$, and  indirect data-driven hedging position$\delta^{IKL}_{t,T,K}$.
We evaluate the performance in four different ways:
\begin{enumerate}
	\item Gain \eqref{eq:Gain} over Black-Scholes $\delta^{BS}_{t,T,K}$ as in \cite{hulloptimal}.
	\item The mean absolute value of $\Delta P_{H}(t,T,K)$
	\[
		\MeanAbs=\frac{1}{m}\sum_{i=1}^m |\Delta P_{H}(t_i,T_i,K_i)|
	\]

	\item The 95\% Value-at-Risk (VaR) of $\{\Delta P_{H}(t_i,T_i,K_i)| i=1,\dots, m\}$
	\item The 95\% Conditional-Value-at-Risk (CVaR) of $\{\Delta P_{H}(t_i,T_i,K_i)| i=1,\dots, m\}$
\end{enumerate}

\subsubsection{Feature Set \#1:$ \{\textsc{moneyness} \;(S_t/K),\;\;\textsc{time-to-expiry} \;(T-t)\}$}
For the synthetic data, it is known that the option price is a function of
 the moneyness $ \frac{S}{K}$ and time to expiry $T-t$, which are the attributes $\vx_{t}^{T,K}$ in Feature Set \#1. Table \ref{Daily}, \ref{Weekly} and \ref{Monthly} report results for daily, weekly, and monthly hedging respectively.

 \begin{table}[htp!]
\begin{center}
\begin{threeparttable}
\begin{tabular}{|l|r c c c c|}
\hline
Method & Gain (\%)& $\MeanAbs$ & Std& VaR & CVaR   \\ \hline
$\Del$ & 0.0 & 0.185 & 0.286 & 0.380 & 0.574 \\
$\IKLg$  & -3.3 & 0.171 & 0.291 & 0.356 & 0.566 \\
$\IKLs$  & -183.3 & 0.291 & 0.482 & 0.669 & 1.105 \\
$\DKLg$  & 63.1 & 0.120 & 0.174 & 0.251 & 0.352 \\
$\DKLs$  & \textbf{64.9} & \textbf{0.121} & \textbf{0.170} & \textbf{0.255} & \textbf{0.345} \\
$\Heston$ & 63.6 & 0.121 & 0.173 & 0.266 & 0.360 \\
%$\DKLgd$  & 62.3 & 0.114 & 0.176 & 0.238 & 0.349 \\
%$\DKLsd$  & \textbf{70.9} &\textbf{ 0.110 }& \textbf{0.154} & \textbf{0.234} & \textbf{0.322} \\
\hline
\end{tabular}
\caption{Daily Hedging Comparison}
\label{Daily}
 \begin{tablenotes}
    \small
  \item[1] FS \#1: $\vx=\{\textsc{moneyness,~time-to-expiry}\}$
  \item[2] Bold entry indicating best Gain
  \end{tablenotes}
  \end{threeparttable}
\end{center}
\end{table}

\begin{table}[htp!]
\begin{center}
\begin{threeparttable}
%\begin{tabular}{|l|r|c|c|c|c|}
\begin{tabular}{|l|r c c c c|}
\hline
Method & Gain (\%) & $\MeanAbs$ & Std& VaR & CVaR   \\ \hline
$\Del$ & 0.0 & 0.414 & 0.620 & 0.776 & 1.009 \\
$\IKLg$  & -197.6 & 0.406 & 1.070 & 0.741 & 1.254 \\
$\IKLs$  & -94.7 & 0.548 & 0.866 & 1.114 & 1.738 \\
$\DKLg$  & 47.0 & 0.312 & 0.451 & 0.620 & 0.825 \\
$\DKLs$  & \textbf{50.8} & \textbf{0.312} & \textbf{0.435} & \textbf{0.622} & \textbf{0.797} \\
$\Heston$ & 45.7 & 0.319 & 0.456 & 0.651 & 0.840 \\
\hline
\end{tabular}
\caption{Weekly Hedging Comparison}
\label{Weekly}
 \begin{tablenotes}
    \small
  \item[1] FS \#1: $\vx=\{\textsc{moneyness,~time-to-expiry}\}$
  \item[2] Bold entry indicating best Gain
  \end{tablenotes}
  \end{threeparttable}
\end{center}
\end{table}


\begin{table}[htp!]
\begin{center}
\begin{threeparttable}
\begin{tabular}{|l|r c c c c|}
\hline
Method & Gain (\%)& $\MeanAbs$ & Std& VaR & CVaR   \\ \hline
$\Del$ & 0.0 & 0.941 & 1.484 & 1.516 & 1.808 \\
$\IKLg$  & 1.0 & 0.888 & 1.470 & 1.516 & 1.829 \\
$\IKLs$  & -36.9 & 1.135 & 1.729 & 2.033 & 2.894 \\
$\DKLg$  & 33.6 & 0.860 & 1.181 & 1.612 & 1.949 \\
$\DKLs$  & {35.4} & {0.858} & {1.165} & {1.610} & {1.922} \\
$\Heston$ & \textbf{38.7} & \textbf{0.814} & \textbf{1.136} & \textbf{1.544} & \textbf{1.829} \\
\hline
\end{tabular}
\caption{Monthly Hedging Comparison}
\label{Monthly}
\begin{tablenotes}
    \small
  \item[1] FS \#1: $\vx=\{\textsc{moneyness,~time-to-expiry}\}$
  \item[2] Bold entry indicating best Gain
  \end{tablenotes}
  \end{threeparttable}
  \end{center}
\end{table}


Table \ref{Daily} and \ref{Weekly}  demonstrate that the direct hedging function learning $\DKLs$ \& $\DKLg$  significantly outperform the indirect hedging learning $\IKLs$ \& $\IKLg$ in Gain and different risk measures considered. Indeed, $\DKLs$ \& $\DKLg$ slightly outperform the benchmark of using the analytic Heston delta.
The indirect hedging function learning performs more poorly than the implied BS delta hedging. In addition,  the spline kernel performs better than the RBF kernel (with the standard deviation as the bandwidth parameter). The RBF kernel yields larger risk measures and smaller Gain  for both the direct and indirect hedging learning methods.

Table \ref{Monthly} reports hedging comparison for monthly hedging. We  observe that  $\DKLs$ \& $\DKLg$  significantly outperform the indirect hedging learning $\IKLs$ \& $\IKLg$ in  Gain and various risk measures. In addition, $\DKLs$ \& $\DKLg$ continue to achieve enhanced performance over $\Del$, with the spline kernel $\DKLs$ yielding better results than $\DKLg$.
Not surprisingly, hedging performance of each method also deteriorates as the length of the hedging period increases,  with larger mean absolute hedging error and larger standard deviation for monthly hedging than for daily and weekly hedging.

Table \ref{Monthly} also illustrates that, unlike daily and weekly hedging, $\DKLs$ \& $\DKLg$ slightly underperform the analytic Heston delta benchmark  for monthly hedging.
Given that the analytic delta is for instantaneous hedging while the direct hedging learning $\DKLs$ minimizes quadratic hedging error, one would expect better performance from the direct hedging learning $\DKLs$. We suspect that this is due to the effect of the specific combination of choices of features and kernel. Next we show that,  with a different feature set,   performance of direct hedging is improved, which suggests the possibility of surpassing  analytic Heston delta benchmark, using a more suitable feature set, for a longer period hedging.


\subsubsection{Feature Set \#2: $ \{\textsc{moneyness} \;(S_t/K),\;\;\textsc{time-to-expiry} \;(T-t) ,\;\;~ \delta^{BS}_{t,T,K}\}$}
We add the Black–Scholes delta $\delta^{BS}_{t,T,K}$ using the implied volatility as an additional feature in the direct hedging learning, since \citet{hulloptimal} indicate that a better minimum  variance hedge can be calculated based on the implied volatility delta.  Table \ref{DailyD}, \ref{WeeklyD}, and \ref{MonthlyD}
present hedging results for  $\DKLs$ \& $\DKLg$ for  $\{\textsc{moneyness} \;(S_t/K),\;\;\textsc{time-to-expiry} \;(T-t) ,\;\;~ \delta^{BS}_{t,T,K}\}$. For clarity, we also include the results for FS \#1 $\{\textsc{moneyness} \;(S_t/K),\;\;\textsc{time-to-expiry} \;(T-t) \}$ and Heston delta for ease of comparison.

\begin{table}[htp!]
\begin{center}
\begin{threeparttable}
\begin{tabular}{|cl| r c c c c|}
\hline
\multicolumn{2}{|c|}{Method} & Gain (\%) & $\MeanAbs$ & Std& VaR & CVaR   \\ \hline
 \multirow{2}{*}{$\DKLg$}& FS \#1 & 63.1 & 0.120 & 0.174 & 0.251 & 0.352 \\
 &FS \#2& 62.3 & 0.114 & 0.176 & 0.238 & 0.349 \\
 \multirow{2}{*}{$\DKLs$}& FS \#1 & 64.9 & 0.121 & 0.170 & 0.255 & 0.345 \\
 &FS \#2& \textbf{70.9} &\textbf{ 0.110 }& \textbf{0.154} & \textbf{0.234} & \textbf{0.322} \\
 \multicolumn{2}{|c|}{ $\Heston$ }& 63.6 & 0.121 & 0.173 & 0.266 & 0.360 \\
\hline
\end{tabular}
\caption{ Daily Hedging Comparison }
\label{DailyD}
\begin{tablenotes}
    \small
  \item[1] FS \#2:  $\vx=\{\textsc{moneyness,~time-to-expiry},~ \Del\}$
  \item[2] Bold entry indicating best Gain
  \end{tablenotes}
  \end{threeparttable}
\end{center}
\end{table}

\begin{table}[htp!]
\begin{center}
\begin{threeparttable}
\begin{tabular}{|cl| r c c c c|}
%\begin{tabular}{|cl|r|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{Method} & Gain (\%) & $\MeanAbs$ & Std& VaR & CVaR   \\ \hline
 \multirow{2}{*}{$\DKLg$}& FS \#1 & 47.0 & 0.312 & 0.451 & 0.620 & 0.825 \\
 &FS \#2 & 51.4 & 0.301 & 0.432 & 0.611 & 0.816 \\
 \multirow{2}{*}{$\DKLs$}&FS \#1 & 50.8 & 0.312 & 0.435 & 0.622 & 0.797 \\
 &FS \#2& \textbf{53.5} & \textbf{0.299} & \textbf{0.422} & \textbf{0.606} & \textbf{0.794} \\
 \multicolumn{2}{|c|} {$\Heston$} & 45.7 & 0.319 & 0.456 & 0.651 & 0.840 \\
\hline
\end{tabular}
\caption{ Weekly Hedging Comparison}
\label{WeeklyD}
\begin{tablenotes}
    \small
  \item[1] FS \#2:  $\vx=\{\textsc{moneyness,~time-to-expiry},~ \Del\}$
  \item[2] Bold entry indicating best Gain
  \end{tablenotes}
  \end{threeparttable}
\end{center}
\end{table}

\begin{table}[htp!]
\begin{center}
 \begin{threeparttable}
%\begin{tabular}{|cl|r|c|c|c|c|}
\begin{tabular}{|cl| r c c c c|}
\hline
\multicolumn{2}{|c|}{Method} & Gain (\%) & $\MeanAbs$ & Std& VaR & CVaR   \\ \hline
 \multirow{2}{*}{$\DKLg$}& FS \#1 & 33.6 & 0.860 & 1.181 & 1.612 & 1.949 \\
 &FS \#2 & 30.1 & 0.863 & 1.217 & 1.652 & 2.104 \\
 \multirow{2}{*}{$\DKLs$}&FS \#1 & 35.4 & 0.858 & 1.165 & 1.610 & {1.922} \\
 &FS \#2 & {36.6} & {0.836} & {1.156} & {1.609} & 1.953 \\
 \multicolumn{2}{|c|} {$\Heston$} & \textbf{38.7} & \textbf{0.814} & \textbf{1.136} & \textbf{1.544} & \textbf{1.829} \\
\hline
\end{tabular}
\caption{ Monthly Hedging Comparison}
\label{MonthlyD}
\begin{tablenotes}
    \small
  \item[1] FS \#2:  $\vx=\{\textsc{moneyness,~time-to-expiry},~ \Del\}$
  \item[2] Bold entry indicating best Gain
  \end{tablenotes}
  \end{threeparttable}
\end{center}
\end{table}
From Table \ref{DailyD}, \ref{WeeklyD} and \ref{MonthlyD}, we observe that, for daily and weekly hedging, including the BS delta further improves the performance of the direct hedging learning methods, which outperforms analytical delta hedging. For monthly hedging, however, the performance is similar to what we obtain with that of the feature set \#1. Overall, including the BS delta as an attribute is beneficial for the direct hedging function learning. 
\section{Enhancement Over the Kernel Local Hedging Model}
In this chapter, we illustrate that, even in a nonparametric kernel approach to model the option value function,  dependence on the underlying can exist for the estimated kernel parameters. Consequently, using the partial derivatives of the model option pricing function, parametrically or nonparametrically estimated, will fail to minimize hedging error, even instantaneously.

Thus, we propose to directly learn nonparametric kernel hedging functions by minimizing the sum of square of the discrete local hedging risk, bypassing the intermediate step of the option value function estimation. Using synthetic data, we first demonstrate that the proposed direct hedging function learning significantly outperforms hedging based on the sensitivity of the model option function learned nonparametrically. In addition, we demonstrate that spline kernel yields better hedging performance in comparison to that of the RBF kernel. The exploratory research in this chapter clearly demonstrates the potential role of a market data-driven approach for financial derivative modeling and risk management. 

However, we also notice the positive gain ratios \eqref{eq:Gain} are  smaller for monthly hedging in comparison to daily and weekly hedging for the synthetic data. 
In addition, when testing on real market S\&P 500 index option with the $\DKLs$ model and the  feature set \#2, we observed the similar phenomenon as it is for the synthetic scenario: the gain ratio decreases when we move from daily hedging to weekly and monthly hedging. The details of the real data comparison can be found in \cite{knian2017} and will be discussed in details in chapter \ref{sec:LocalComparison}.

We suspect that neither the  feature set \#1 nor the  feature set \#2 is enough to learn a sufficient data-driven local hedging model for longer period such as weekly  and monthly.  We are thus motivated to enhance the data-driven local hedging model using  the RNN framework by adding the following components to include more features in computing the hedging position:
\begin{itemize}
\item  Feature selection process.
\item  Sequential feature extraction process.
\end{itemize}
We demonstrate using real S\&P 500 index option data that such enhancement greatly improve the local hedging performance.
Besides, the $\model$ proposed in Chapter \ref{sec:RNNLocal} is more computational efficient enabling  us to update the model more frequently to incorporate market changes.
Details of the enhanced data-driven local hedging model $\model$ will be discussed in chapter \ref{sec:RNNLocal}. The experimental results of $\DKLs$ and $\model$ on real $S\&P 500$ index option data will be discussed in chapter \ref{sec:LocalComparison}.

\chapter{Data-Driven Sequential Learning Framework for Local Hedging Risk}
\label{sec:RNNLocal}
In chapter \ref{sec:kernel}, we have discussed the exploratory data-driven kernel hedging  model $\DKLs$ \cite{knian2017}. However, we believe that the data-driven hedging learning approach in \cite{knian2017} can potentially benefit from further improvements in a few directions.
Firstly,  \citet{knian2017}  only use moneyness, time-to-expiry, and Black–Scholes delta as features to predict the hedging position. In practice, since both the underlying and options markets have complex price dynamics,
 feature extraction and feature selection can potentially further enhance hedging performance.
Secondly, when predicting the hedging position at the rebalancing time $t$, only attributes observed at $t$ are used as features in \cite{knian2017}.  However, a financial market exhibits volatility clustering,   describing a positive, significant, and slowly decaying volatility autocorrelation  \citep{mandelbrot1963variation}.  For example,  a generalized autoregressive conditionally heteroskedastic (GARCH) model has been proposed for option pricing because of its capability in better characterizing asset returns \citep{heston2000closed} .  It has been shown that the option pricing function under a GARCH model depends not only on the current underlying price but also on the observed underlying price history \citep{duan2006approximating,heston2000closed}.   This suggests that the information immediately prior to the rebalancing time should be relevant in determining the hedging position at the rebalancing time.
Thirdly,  the mean squared error is used as the loss function in \cite{knian2017} but
 %which may not be the best choice: this measure disproportionately focuses on the largest hedging error (in magnitude) even though the market is known to experience crisis and normal regimes.
%If we include data instances observed during the financial crisis and market crashes in the training data set, the model performance may be negatively affected since we treat each data instance equally and minimize the sum of squared errors.
%Therefore, it is important to devise
a more appropriate robust objective function and framework may lead to a more stable optimal learning as the market shifts between crisis and normal regimes.
Lastly, when a kernel data-driven model needs to be updated frequently, e.g., daily, it is computationally prohibitive to conduct backtesting over a long time period (e.g., a decade). 
Consequently the regularized spline kernel network is only updated monthly in  \cite{knian2017}.  To accurately assess the potential of a data-driven hedging approach,  a model which is sufficiently computationally efficient needs to be considered to allow more frequent updating in a back-testing study of a long time horizon.


%Following its inception in the 1980s, it has been widely used in time series analysis.
To incorporate sequential information in the hedging model, we use
RNN,  a feed-forward neural networks augmented with edges that connect adjacent  steps.
In the early 1980s, \citet{hopfield1982neural} introduced RNN  for sequential pattern recognitions.
\citet{jordan1986serial} and \citet{elman1990finding} developed  basic architectures for RNN with a group of neural networks that have a "memory" to capture past information. This can be beneficial in time series applications.

Training RNN is similar to training the traditional neural network, with
the Stochastic Gradient Descent (SGD) as the primary optimization tool.
The Back Propagation Through Time (BPTT) algorithm is used to calculate gradients.
However, vanishing and exploding gradients  \citep{hochreiter2001gradient} can occur when back-propagating errors across many  steps,
which pose challenges for standard RNN architectures to learn long-term dependence between  steps.
The Long Short-Term Memory (LSTM) \citep{hochreiter1997long} model and the Gated Recurrent Unit (GRU) model \citep{cho2014learning} are
subsequently proposed to address issues of vanishing and exploding gradients.
%LSTM  and GRU replace a standard RNN cell, which yields the value of the current hidden state $\vh_t$,
%with an LSTM memory cell or a GRU cell respectively.
LSTM  and GRU utilize a gate structure with element-wise operations, which  %enabling the hidden state to change slowly
can retain information in memory for a longer  period.  This alleviates the problem of vanishing and exploding gradients \citep{hochreiter1997long}. %Thus, GRU usually outperforms standard RNN experimentally \cite{chung2014empirical}.
%Performances of the
GRU and  LSTM models are shown to perform better than the standard RNN model \citep{cho2014learning}, although
performances of GRU and  LSTM are comparable.
Since GRU  has fewer parameters than LSTM  and usually require less training data \citep{yin2017comparative},
we use the GRU in the proposed encode-decoder hedging model $\model$.


\section{ The Proposed  $\model$ for market data-driven hedging}
\label{sec:model}
\subsection{Local Discrete Hedging GRU Model}
Figure \ref{fig:RNNModel} depicts the proposed local discrete GRU  hedging  model $\model$,
 which uses an encoder-decoder structure to combine the local and sequential features,  similar to the sequence to sequence (seq2seq) models \citep{cho2014learning}.  This model uses both the  local features at the hedging time and the sequential features, which encode information immediately before the hedging time.

 Consider a data instance at the hedging time $t$, strike $K$, and expiry $T$. Let the associated change of the market option price  be $\Delta V^{mkt}_{t,T,K}$ and underlying price change be $\Delta S_t$, as in \eqref{eq:DVDS}.
Assume that at the hedging time $t$, we have local features vector $\vx^{T,K}_{t} \in \Real^{d_l}$ which records local information at the hedging time $t$ for hedging the option with expiry $T$ and strike $K$.

Let $\DT_{d}$ denote the time interval for sequential information recording. In  the subsequent empirical study, the interval $\DT_{d}$ equals one-day.  We denote the sequential features recording the daily history for hedging the option with expiry $T$ and strike $K$ as
\[
\mathbf{Y}_{t}^{T,K}=\left[\vy^{T,K}_{t-N \DT_{d}},\dots,\vy^{T,K}_{t}\right]
\]
For notational simplicity, we denote $\tau_i=t-(N+1-i)\DT_d$ with $i=1, \dots,N+1$, we thus have:
\[
\mathbf{Y}_{t}^{T,K}=\left[\vy^{T,K}_{\tau_{1}},\dots,\vy^{T,K}_{\tau_{N+1}}\right]
\]
The vector $\vy^{T,K}_{\tau_{i}} \in \Real^{d_s}$ has  $d_s$ features at time $\tau_{i}$ in the input sequential feature.
Thus $d_l$ is the dimension for the local feature $\vx_{t}^{T,K}$, $d_s$ is the dimension for the sequential feature $\mathbf{Y}_{t}^{T,K}$, and
$N+1$ is the length of the sequential feature sequence.


The encoder transforms information from the sequential feature $\mathbf{Y}_{t}^{T,K}$ to  a fixed-sized vector
$\mathbf{\widehat{h}}_E$   and the decoder makes the final prediction based on both $\mathbf{\widehat{h}}_E$  and the local feature $\vx^{T,K}_{t}$.
The overall structure of the proposed model is illustrated in Figure \ref{fig:RNNModel}. Next we discuss
 each component of the proposed encoder-decoder model in details.
\begin{figure}[htp!]
	\centering
	\resizebox{0.65\textwidth}{!}{
		\begin{tikzpicture}[
		prod/.style={circle, draw, inner sep=0pt},
		ct/.style={circle, draw, inner sep=5pt, ultra thick, minimum width=10mm},
		ft/.style={circle, draw, minimum width=8mm, inner sep=1pt},
		filter/.style={circle, draw, minimum width=7mm, inner sep=1pt, path picture={\draw[thick, rounded corners] (path picture bounding box.center)--++(65:2mm)--++(0:1mm);
				\draw[thick, rounded corners] (path picture bounding box.center)--++(245:2mm)--++(180:1mm);}},
		mylabel/.style={font=\scriptsize\sffamily},
		>=LaTeX
		]
		
		
		\node[draw,rectangle]  (s1) at (4.5, -3) {softmax};
		\node[draw,rectangle]  (s3) at (7.5, -3) {softmax};
		\node [inner sep=0pt] (rp1) at (3*1, -3) {$\odot$};
		\node  (rp2) at (3*2, -1.5) {$\dots$};
		\node [inner sep=0pt] (rp3) at (3*3, -3) {$\odot$};
		
		\foreach \i [count=\step from 1] in {$\vy^{T,K}_{\tau_{1}}$,$\dots$,$\vy^{T,K}_{\tau_{N+1}}$}
		\node (ri\step) at (3*\step, -4) {\i};
		\node  (sw1) at (4.5, -4) {$\omega^S$ };
		\node  (sw3) at (7.5, -4) {$\omega^S$};
		\draw[->] (s1.west) to node[below]{} (rp1.east);
		\draw[->] (s3.east) to node[below]{} (rp3.west);
		\draw[->] (ri1.north) to (rp1.south);
		\draw[->] (ri3.north) to (rp3.south);
		\draw[->] (sw1.north) to (s1.south);
		\draw[->] (sw3.north) to (s3.south);
		\node (h2) at (3*2, 0.0) {$\dots$};
		\foreach \step in {1,3} {
			\node[draw,rectangle] (h\step) at (3*\step, 0.0) {GRU};
		}
		\draw[->] (rp1.north) to node[right]{$\widehat{\vy}^{T,K}_{\tau_{1}}$} (h1.south);
		\draw[->] (rp3.north) to node[left]{$\widehat{\vy}^{T,K}_{\tau_{N+1}}$} (h3.south);
		
		%\draw[->] (i4) -> (h4.south);
		\draw[->] (h1.east) to node[below]{$\vh_1$} (h2.west);
		\draw[->] (h2.east) to node[below]{$\vh_{N}$} (h3.west);
		%\foreach \step in {1,...,2} {
		%	\pgfmathtruncatemacro{\next}{add(\step,1)}
		%	\draw[->] (h\step.east) -> (h\next.west);
		%}
		\node[fit=(ri1) (ri3) (s1) (s3) (h1) (h3), draw, inner sep=0pt] (fit1) { };
		\node[align=center, outer sep=0] (encoder) [right=of fit1] {Encoder};
		
		
		
		\node[filter] (oe2) at (9, 4.5) {};
		\node[filter] (oe3) at (6, 4.5) {};
		\node [inner sep=0pt] (oe4) at (6, 6.5) {$\odot$};
		\node [inner sep=0pt] (oe5) at (9, 6.5) {$\odot$};
		\node [draw,circle,inner sep=0pt] (oe6) at (7.5, 7.5) {$+$};
		\node (oe7) at (7.5, 8.5) {$\delta^M_{t,T,K}$};
		\node  (bs) at (4.5, 6.5) {$\delta^{BS}_{t,T,K}$};
		\draw[->] (oe3.north) to node[left]{$1-W_{\delta} $} (oe4.south);
		\draw[->] (oe3.north) to node[above]{$W_{\delta}$} (oe5.west);
		\draw[->] (oe2.north) to node[left]{$\widehat{\delta}^M_{t,T,K}$} (oe5.south);
		\draw[->] (oe4.north) to node[left]{} (oe6.west);
		\draw[->] (oe5.north) to node[left]{} (oe6.east);
		\draw[->] (oe6.north) to node[left]{} (oe7.south);
		\draw[->] (bs.east) to node[left]{} (oe4.west);
		
		
		
		
		\node[inner sep=0pt]  (l1) at (6, 2) {$\odot$};
		\node[draw,rectangle]  (l3) at (6, 1) {softmax};
		\node  (l2) at (4.5, 2) {$\vx^{T,K}_{t}$};
		\node  (l4) at (4.5, 1) {$\omega^L$};
		\draw[->] (l4.east) to (l3.west);
		\draw[->] (l2.east) to (l1.west);
		\draw[->] (l3.north) to node[left]{} (l1.south);
		
		
		\node (he2) at (6, 3) {$\widehat{\vx}^{T,K}_{t}$};
		\node (he3) at (9, 3) {$\widehat{\mathbf{h}}_E$};
		\draw[->] (l1.north) to (he2.south);
		\draw[->] (h3.north) to  (he3.south);
		\draw[->] (he3.north) to  (oe2.south);
		\draw[->] (he3.north) to  (oe3.south);
		
		\draw[->] (he2.north) to (oe2.south);
		\draw[->] (he2.north) to (oe3.south);
		
		\node[fit=(oe2) (oe3) (oe5) (oe6)  (oe7)  (bs) (he2) (he3), draw, inner sep=0pt] (fit3) { };
		\node[align=center, outer sep=0] (encoder) [left=of fit3] {Decoder};
		\end{tikzpicture}
	}
	\caption{$\model$: GRU Encode-Decoder Hedging Model  }
	\label{fig:RNNModel}
\end{figure}

\subsection{Feature Selection via Embedded Feature Weighting}
 Feature selection improves machine learning performance by eliminating noise and providing better interpretability. While various feature selection frameworks have  been proposed,  we consider the feature weighting method \citep{tayal2014primal}, which embeds feature selection  in the SVM training.
  This  embedded feature weighting method is shown to outperform other state-of-the-art embedded feature selection methods \citep{tayal2014primal}.
We adopt a similar feature weighting embedding technique in the discrete GRU model to conduct feature selections on both the local feature $\vx^{T,K}_{t}$ and the sequential feature $\mathbf{Y}_{t}^{T,K}$.
Features are first weighted and then fed into the encoder and decoder as inputs. Feature weights are optimized during model training.

We  use a softmax function to generate a normalized  feature weighting vector. For the local feature $\vx^{T,K}_{t}  \in \Real^{d_l}$, the $j^\text{th}$ component of the normalized weight vector is given by
\[
\frac{exp(\omega^L_j)}{\sum_{i=1}^{d_l} exp(\omega^L_i)}
\]
The weighted  local feature vector is defined as
\[\widehat{\vx}^{T,K}_{t}=\frac{exp(\omega^L)}{\sum_{i=1}^{d_l} exp(\omega^L_i)}\odot \vx^{T,K}_{t} \] 
where $\odot$ denotes the element-wise multiplication.

Similarly,
for the sequential feature $\mathbf{y}_{\tau_i}^{T,K}$,  the $j^\text{th}$ component of the normalized weight vector is given by
\[
\frac{exp(\omega^S_j)}{\sum_{i=1}^{d_s} exp(\omega^S_i)}
\]
%The normalized weight vector for sequential feature is therefore \[\mathbf{\zeta}=[\zeta_1,\dots,\zeta_{d_s}]\]
% Recall that $\mathbf{y}_{\tau_i}^{T,K}$ is a vector of the ${d_s}$ features.
  The weighted feature vector at time  $\tau_i$ is defined as
\[
\widehat{\vy}_{\tau_i}^{T,K} =\frac{exp(\omega^S)}{\sum_{j=1}^{d_s} exp(\omega^S_j)} \odot \mathbf{y}_{\tau_i}^{T,K}
\]
{The weighting procedure acts as a feature selection.}
If a  particular feature is not relevant in predicting the hedging position, the associated weight after learning  is expected to be negligible.
%The unnormalized feature weight vectors $LW$ and $SW$ are parts of the parameters to be learned for the model. The model will be trained to adjust the weight dynamically.


\subsection{GRU Encoder}
The  proposed $\model$  in Figure \ref{fig:RNNModel} has an one-layer GRU, which encodes the sequential feature $\mathbf{Y}_{t}^{T,K}$ to  a fixed-sized vector
$\mathbf{\widehat{h}}_E$ .
At the  step $i$,  the encoder computes the value of the hidden state $\vh_{i}$ using a GRU cell. The input at the step $i$ of the encoder is $\widehat{\vy}^{T,K}_{\tau_{i}}$,  $i=1,\ldots,N+1$.
The internal structure of the GRU cell is shown in Figure \ref{fig:RNN}.

Let  $\vW_z,\vU_z,\vb_z,\vW_r,\vU_r, \vb_r,\vW_h,\vU_h,\vb_h$  denote
parameters shared by all GRU cells.


\begin{enumerate}
	\item The \textbf{update gate} decides how much the cell updates its activation:
	\[\mathbf{z}_i= sigmoid ( \vW_z \widehat{\vy}^{T,K}_{\tau_{i}} + \vU_z \vh_{i-1} +\vb_z)\]
	\item The \textbf{reset gate} decides how much  information to retain from the  previous hidden state $\vh_{i-1}$:
	\[\mathbf{r}_i= sigmoid ( \vW_r \widehat{\vy}^{T,K}_{\tau_{i}} + \vU_r \vh_{i-1} +\vb_r)\]
	\item The \textbf{candidate} hidden state value $\widehat{\vh}_i$ is computed from the current input $\widehat{\vy}^{T,K}_{\tau_{i}}$,  previous hidden state $\vh_{i-1}$, and the reset value $\mathbf{r_i}$ :
	\[
	\widehat{\vh}_i=tanh( \vW_h \widehat{\vy}^{T,K}_{\tau_{i}}  + \vU_h (\mathbf{r}_i \odot \vh_{i-1}) +\vb_h)
	\]
	\item  The \textbf{output} hidden state value $\vh_{i}$ is  computed based on a weighted combination of the previous activation $\vh_{i-1}$ and the candidate activation $\widehat{\vh}_i$:
	\[
	\vh_i=(1-\mathbf{z}_i) \odot \vh_{i-1} + \mathbf{z}_i \odot \widehat{\vh}_i
	\]
\end{enumerate}







\begin{figure}[htp!]
	\centering
			\begin{tikzpicture}[
			prod/.style={circle, draw, inner sep=0pt},
			ct/.style={circle, draw, inner sep=5pt, ultra thick, minimum width=10mm},
			ft/.style={circle, draw, minimum width=8mm, inner sep=1pt},
			filter/.style={circle, draw, minimum width=7mm, inner sep=1pt, path picture={\draw[thick, rounded corners] (path picture bounding box.center)--++(65:2mm)--++(0:1mm);
					\draw[thick, rounded corners] (path picture bounding box.center)--++(245:2mm)--++(180:1mm);}},
			mylabel/.style={font=\scriptsize\sffamily},
			>=LaTeX
			]
			\node[draw, rectangle,inner sep=1pt] (ct) {tanh};
			\node[inner sep=0pt]  (l1) at (6, 2) {$\odot$};
			\node[ right=10mm of ct,inner sep=0pt] (int1) {$\odot$};
			\node[ right=15mm of int1,inner sep=0pt] (x1) {$\odot$};
			\node[left=of ct] (x2) {};
			\node[ below=5mm of ct,inner sep=0pt] (x3) {$\odot$};
			\node[draw, rectangle,inner sep=1pt, below=15mm of x2, label={[mylabel]above:Reset}] (ft) {sigmoid};
			\node[draw, rectangle,inner sep=1pt, right=22.75mm of ft, label={[mylabel]left:Update}] (ot) {sigmoid};
			
			\node[above=of x1] (temp) {};
			\node[prod, left= 6mm of temp,inner sep=2pt] (final) {$+$};
			\draw[->] (ft) to  node[above]{$\mathbf{r_i}$} (x3);
			\draw[->] (ot) to  node[right]{$1-\mathbf{z}_i$} (x1);
			\draw[->] (ot) to  node[right]{$\mathbf{z}_i$} (int1);
			\draw[->] (ct) to  node[above]{$\widehat{\vh}_i$} (int1);
			
			\draw[->] (x3) to  (ct);
			\draw[->] (x1) to  (final);
			\draw[->] (int1) to  (final);
			\node[fit= (ot) (ft) (x1) (x3) (final), draw, inner sep=0pt] (fit) {};
			\draw[->] (fit.south-|ct) to (x3);
			
			
			
			
			
			\draw[<-] (fit.south-|ot) coordinate (aux)--++(-45:7mm) node[below]{$\widehat{\vy}^{T,K}_{\tau_{i}}$ };
			\draw[<-] (fit.south-|ot) coordinate (aux)--++(-135:7mm) node[below]{$\vh_{i-1}$};
			
			\draw[<-] (fit.south-|ft) coordinate (aux)--++(-135:7mm) node[below]{ $\vh_{i-1}$};
			\draw[<-] (fit.south-|ft) coordinate (aux)--++(-45:7mm) node[below]{$\widehat{\vy}^{T,K}_{\tau_{i}}$ };
			
			\draw[<-] (x3.south) coordinate (aux)--++(-90:15mm) node[below]{$\vh_{i-1}$ };
			\draw[<-] (ct.west) coordinate (aux)--++(180:18mm) node[left]{$\widehat{\vy}^{T,K}_{\tau_{i}}$ };
			
			\draw[<-] (x1.east) coordinate (aux)--++(0:5mm) node[right]{$\vh_{i-1}$ };
			\draw[->] (final.north) coordinate (aux)--++(90:5mm) node[above]{$\vh_{i}$ };
			%\node[filter, below=23mm of x3, label={[mylabel]right:tanh}] (ex1) {};
			\end{tikzpicture}
	\caption{Illustration of the GRU Cell}
	\label{fig:RNN}
\end{figure}
The hidden state at the last step $\vh_{N+1}$, corresponding to time ${\tau_{N+1}}={t}$, is supplied to
the decoder as the fixed size vector $\mathbf{\widehat{h}}_E$, which  extracts relevant information in $\mathbf{Y}_{t}^{T,K}$.
\subsection{Decoder}

The decoder combines the output of the encoder from the sequential feature $\mathbf{Y}_{t}^{T,K}$  at the hedging time $t$ with the current local feature $\mathbf{x}_{t}^{T,K}$.
It uses one neural network to first compute a candidate output  $\widehat{\delta}^M_{t,T,K}$,  based on both the weighted local input  $\widehat{\vx}_{t}^{T,K}$ and the fixed size vector $\mathbf{\widehat{h}}_E$ :
$$
\widehat{\delta}^M_{t,T,K}=sigmoid (\vv^T_{out} \ tanh( \vU_{out} \mathbf{\widehat{h}}_E + \vW_{out} \widehat{\vx}_{t}^{T,K}+ \vb_{out})).
$$

The the output gate value $W_{\delta}$ is given by another neural network, based on both the weighted local input  $\widehat{\vx}_{t}^{T,K}$ and the fixed size vector $\mathbf{\widehat{h}}_E$ :
\begin{equation}\label{WDel}
W_{\delta}=sigmoid (\vv^T_{Gate} \ tanh( \vU_{Gate} \mathbf{\widehat{h}}_E + \vW_{Gate} \widehat{\vx}_{t}^{T,K}+ \vb_{Gate})).
\end{equation}

The practitioner's BS delta encapsulates a significant portion of the sensitivity of the option price to the underlying and has been adopted in option hedging practice. Therefore we use BS implied volatility delta as a pre-trained model in the designed $\model$ to utilize both local and sequential features to minimize hedge risk.
Specifically, the output of the proposed  $\model$ is a linear combination of the candidate output $\widehat{\delta}^M_{t,T,K}$ and the BS delta $\delta^{BS}_{t,T,K}$ computed from the implied volatility at the hedging time $t$ for the option with expiry $T$ and strike $K$. In addition,  different combination formulae are used when training for different types of options.
For hedging a call option, the final output from $\model$ is :
\begin{equation}
\delta^M_{t,T,K}=\widehat{\delta}^M_{t,T,K} \times W_{\delta} +\delta^{BS}_{t,T,K} \times (1-W_{\delta})
\label{eq:call}
\end{equation}
For hedging a put option, the final output from the model is:
\begin{equation}
\delta^M_{t,T,K}=-\widehat{\delta}^M_{t,T,K} \times W_{\delta} +\delta^{BS}_{t,T,K} \times (1-W_{\delta})
\label{eq:put}
\end{equation}
where $\widehat{\delta}^M_{t,T,K}$ is the candidate output.
When $W_{\delta}=0$,  $\delta^{BS}_{t,T,K}$  is the output and, when $W_{\delta}=1$,  the output  is the candidate $\widehat{\delta}^M_{t,T,K}$. The combination weight $W_{\delta}$
%, for $\widehat{\delta}^M_{t,T,K}$ and $\delta^{BS}_{t,T,K}$ respectively,
is defined in \eqref{WDel}.
  The sigmoid function is used as the final activation function because, under the Black-Scholes-Merton framework \citep{black1973pricing}, the range of the hedging position for call options is $[0,1]$ and  the range of the hedging position for put options is $[-1,0]$. It can be easily shown that $\delta^M_{t,T,K}$, given by \eqref{eq:call}, is within $[0,1]$ and  $\delta^M_{t,T,K}$, given by \eqref{eq:put}, is within $[-1,0]$.

\subsection{Robust Loss Function}

At the hedging time $t_i$, for the data instance $i$ with  strike $K_i$, and expiry $T_i$ , the local discrete hedging loss is:
\[
loss_i=\Delta V^{mkt}_{t_i,T_i,K_i}-\DS_{t_i} \delta^M_{{t_i,T_i,K_i}}
\]
where  $\delta^M_{{t_i,T_i,K_i}}$ is the corresponding  final output from $\model$,  $\DS_{t_i}$ denotes the change in the market underlying price, and $\Delta V^{mkt}_{t_i,T_i,K_i}$ denotes the change in the market option price,  see \eqref{eq:DVDS}.
Let $M$ be the number of data instances.
The objective function used in the data-driven regularized kernel hedging model  in \citep{knian2017} is the mean squared loss,
\begin{equation}
MSE=\frac{1}{2M} \sum_{i=1}^{M} loss_i^2.
\label{l1}
\end{equation}
Since the quadratic error function is sensitive to outliers, in this work,
we additionally incorporate the Huber loss function  \citep{Huber1964robust}  below in the proposed $\model$:
\[
HE=\frac{1}{M} \sum_{i=1}^{M} Huber(loss_i),
\]
where $Huber(\cdot)$ is as defined below:
\[
Huber(loss,\mathcal{T})=\left\{ \begin{array}{ll }
\frac{1}{2} loss^2  , \;&  \text{if} \; |loss|\leq \mathcal{T}\\
\mathcal{T}(|loss|-\frac{1}{2}\mathcal{T}), \; &\text{otherwise}
\end{array} \right.
\]
The Huber loss \citep{Huber1964robust} has been shown to be more robust than the squared loss with respect to toutliers, when  the threshold parameter $\mathcal{T}$ is carefully chosen {\em a priori}.
In the context of machine learning, the parameter $\mathcal{T}$ can be tuned but this can be computationally expensive. In the proposed
$\model$, since delta from the BS model  with the implied volatility   is used as a pre-trained model, we adaptively set the thresholding parameter $\mathcal{T}$ to be absolute value of the hedging error from the Black–Scholes delta of the data instance $i$, i.e.,
\begin{equation}\label{eq:thresh}
\mathcal{T}_i=|\Delta V^{mkt}_{t_i,T_i,K_i}-\DS_{t_i} \delta^{BS}_{t_i,T_i,K_i}|
\end{equation}
where $\delta^{BS}_{t_i,T_i,K_i}$ is the Black–Scholes delta using implied volatility for data instance $i$.
The modified Huber loss  becomes
\[
MHuber(loss_i,\mathcal{T}_i)=\left\{ \begin{array}{ll}
\frac{1}{2} loss_i^2  , \; &\text{if} \; |loss_i|\leq \mathcal{T}_i\\
\mathcal{T}_i(|loss_i|-\frac{1}{2}\mathcal{T}_i), \; & \text{otherwise}
\end{array} \right.
\]
Using the Huber loss with the adaptive threshholding parameter in \eqref{eq:thresh}, the objective for the proposed $\model$  is:
\begin{equation}
MHE=\frac{1}{M} \sum_{i=1}^{M} MHuber(loss_i,\mathcal{T}_i)
\label{l2}
\end{equation}







\section{Training $\model$} \label{sec:train}
Next we discuss training  $\model$,  including initialization,  pre-training,  optimization, and regularization.
\subsection{Initialization}\label{sec:init}
When training a RNN model,  an initial weight matrix is typically chosen as a random orthogonal matrix. Since the orthogonal initialization  can often speed up training \citep{le2015simple}, all the weight matrices   are initialized as orthogonal random matrices in training $\model$. The random orthogonal matrices are   Householder transformations of random matrices \citep{le2015simple}.

\subsection{Optimization}
First-order optimization methods, such as stochastic gradient descent (SGD) and its extensions,  are the most widely used optimization methods in machine learning, due to  their low computational costs. Despite their wide usage,  well-known deficiencies when  training highly non-convex objective functions, including  relatively-slow convergence, sensitivity to hyper-parameter values (e.g., learning rate), stagnation at high training errors, and difficulty in escaping flat regions and saddle points \citep{yao2018inexact}.

More recently,  improved SGD methods such as ADAM \citep{kingma2014adam}, have been proposed.
These methods seem to achieve significantly better solutions compared with earlier SGD. However,  it has recently been empirically observed in \cite{reddi2019convergence}  that these algorithms sometimes fail to converge to a first-order critical point due to the exponential moving average used in the algorithms.
%These algorithms are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients and


Since the data size and the number of the parameters in $\model$ are relatively small (which is a distinguishing characteristics of many financial data mining  problems), it is computationally feasible to apply a second order optimization method.  Furthermore, the recently proposed trust-region sub-problem solver \citep{lenders2018trlib} computes the trust region sub-problem solution iteratively requiring only Hessian-vector products. % avoiding direct computation of the Hessian matrix and  Hessian matrix factorizations.
Consequently we use a trust region method, which is shown in Algorithm \ref{alg:trust} when training $\model$.  The meta-parameters for trust-region algorithm are shown in Table \ref{para2}. 
\begin{table}[htp!]
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|}
	\hline
	$\mathcal{R}_0$&
	$\epsilon_{\theta}$& 
	$\epsilon_{r}$&
	$\eta_{r_1}$&
	$\eta_{r_2}$&
	$\gamma_u$&
	$\gamma_d$  \\ \hline
	 1.0&$10^{-5}$&$10^{-8}$&0.25&0.75 &2 & 0.5  \\    \hline
	\end{tabular}
	\end{center}
	
	\caption{Meta-Parameters for the Trust-Region Algorithm \ref{alg:trust}}
	\label{para2}
	\end{table}



 \begin{algorithm}[htp!]
	%\DontPrintSemicolon
	\SetAlgoNoLine
	
	\KwIn{$\theta_0 \in \Real^n$: initial vector of parameters.\newline
		$Obj(\theta)$: the objective function (For $\model$, we use either \eqref{l1} or  \eqref{l2})\newline
		$\mathcal{R}_0$: initial trust region radius \newline
		$\epsilon_{\theta}$: tolerance for the norm of the gradient \newline
		$\epsilon_{r}$: tolerance for the trust region radius \newline
		$\eta_{r_1}$: first threshold  for update the trust region radius \newline
		$\eta_{r_2}$: second threshold  for update the trust region radius \newline
		$\gamma_u>1$: ratio to increase the trust-region radius \newline
		$\gamma_d<1$: ratio to decrease the trust-region radius}
	\KwOut{
		$\theta^*$: the vector of parameters that minimize the objective function $Obj(\theta)$
	}
	
	\Begin{
		$k=0$\;
		\While{$\|\nabla Obj(\theta_k)\|_2 \geq \epsilon_{\theta} $ \text{and} $\mathcal{R}_k \geq \epsilon_r$  }{
			solve the trust-region subproblem \cite{lenders2018trlib}:
			\[
			\begin{split}
			s^*_k=\argmin_s\;& m_k(s)=Obj(\theta_k)+s^T \nabla Obj(\theta_k)+ s^T \nabla^2 Obj(\theta_k) s \\
			\text{sub to}& \|s\|_2 \leq \mathcal{R}_k
			\end{split}
			\]
			define
			\[
			\mathcal{P}_k=\frac{Obj(\theta_k)-Obj(\theta_k+s^*_k)}{Obj(\theta_k)-m_k(s^*_k)}
			\]
			Update the radius:
			\eIf{$\eta_{r_2}>\mathcal{P}_k < \eta_{r_1}$}
			{$\theta_{k+1}=\theta{k}+s^*_k$}
			{$\theta_{k+1}=\theta{k}$}
			\[
				\mathcal{R}_{k+1}=\left\{ 
				\begin{array}{ll}
					\gamma_u  \|s^{*}_k\|_2
					  , \; &\text{if} \; \mathcal{P}_k < \eta_{r_1}\\


					   \mathcal{R}_k
					  , \; &\text{if} \; \eta_{r_1} \leq \mathcal{P}_k \leq \eta_{r_2}\\

					 \max(\gamma_u \|s^{*}_k\|_2,\mathcal{R}_k)
					  , \; &\text{if} \; \mathcal{P}_k >\eta_{r_2}
				\end{array} \right.
    		 \]
			k=k+1
		}
		\[
		\theta^*=\theta_k
		\]
		
		
		
		
		
	}
	\SetAlCapSkip{5em}
	\IncMargin{1em}
	\caption{Trust-region Algorithm}  \label{algorithm}
	\label{alg:trust}
	\DecMargin{1em}
\end{algorithm}




\subsection{Pre-training and  Regularization} \label{sec:preT}
We use  early stopping as the regularization \citep{raskutti2014early}.
 %We evaluate hedging performance using the gain ratio defined in \citep{hulloptimal}, which is the average quadratic hedging error of a hedging method over the average quadratic hedging error of the implied volatility BS delta hedging. The average quadratic hedging error is equal to the variance when the mean of the hedging error is zero.
At each training date, we reserve a fraction of the observed data to be the validation set. The rest of the observed data is used as the training set. The performance on the validation set is used to determine when over-fitting begins. Let $\theta$ denote the set of parameters to be learned for the proposed $\model$. After each training step $k$,  the model performance is evaluated on the validation set and the associated parameters $\theta_k$ is recorded.  Model training optimization is performed on the training set until the trust-region optimizer terminates. The parameters that achieve the best performance on the validation set  is used to predict the hedging position for the testing data instances.

For $\model$,  the training optimization problem  is nonconvex,   which can benefit from a good initial model.   We choose the initial model for the training  problem  by matching  the output of $\model$ to  the pre-trained BS model  $\delta^{BS}_{t,T,K}$.  Specifically,  we determine the initial model for training optimization by solving the nonlinear least square problem below,
\begin{equation}\label{NLS}
\min \frac{1}{2M} \sum_{i=1}^{M} ( \delta^M_{t_i,T_i,K_i} -\delta^{BS}_{t_i,T_i,K_i})^2,
\end{equation}
where the starting point for \eqref{NLS} is a set of randomly initialized  weight matrices, as described in section \ref{sec:init}.

Using a solution to \eqref{NLS} as the initial model, we train $\model$ with the robust objective function \eqref{l2}. Recall that the  output of  $\model$, either \eqref{eq:call} or \eqref{eq:put}, is a linear combination of the candidate output $\widehat{\delta}^M_{t,T,K}$ and the BS delta $\delta^{BS}_{t,T,K}$ computed from the implied volatility. When $W_{\delta}$ approaches 0, the  output of  $\model$ converges to $\delta^{BS}_{t,T,K}$ as the output. When  $W_{\delta}$ approaches $1$,   $\model$ outputs  candidate $\widehat{\delta}^M_{t,T,K}$.
This simple combination structure allows the pre-training phase to be finished in just a few training steps.\footnote{Note that we will also stop the pre-training when $W_{\delta}>0.97.5$ or $W_{\delta}<0.025$. This is because, when we have $W_{\delta}>0.97.5$ or $W_{\delta}<0.025$, due to saturating gradient problem of the sigmoid function, the initial gradient, when training with actual local hedging objective,  will be very small. This will slow down the first few  learning steps when training with actual local hedging objective.}
The pre-training stage guarantees that the initial performance of the model on the validation set is close to $\delta^{BS}_{t,T,K}$. If the output from the model after training performs worse than $\delta^{BS}_{t,T,K}$ on the validation set, we use the initial model after pre-training.


Additionally  the validation set is also used to select  whether to use the mean square objective function \eqref{l1} or  the modified Huber loss objective \eqref{l2} . The model trained with the objective function that performs better on the validation set is used when predicting the hedging position of the testing data instances, which are unobserved at the time of training.


\subsection{Model Re-use and  Re-initialization}
In contrast to the regularized kernel method in \citep{knian2017},  one advantage of the proposed $\model$ is
that the parameters from one trained model can be readily reused as the starting point for the model training on a subsequent rebalancing day.
This allows a model to be updated more frequently, adapting to market changes without completely rebuilding a model.
The kernel method in \citep{knian2017}, on the other hand,  recomputes the kernel matrix at each rebalancing time, which requires $O(m^3)$ computation assuming a completely new kernel matrix. Thus, updating the model frequently using the kernel method in \citep{knian2017} is computationally prohibitive.



An artificial neural network is known to potentially suffer catastrophic interference or catastrophic forgetting \citep{mccloskey1989catastrophic}, i.e., a model  forgets completely and abruptly previously learned information upon observing new information.
When this happens, the resulting broken model usually has poor generalization ability. If a broken model with poor generalization ability is allowed to be continually updated,  the performance of all the future models may be negatively affected. Thus, whenever a worse performance, in comparison to   $\delta^{BS}_{t,T,K}$ , is observed on a validation set,  we re-initialize the parameters of  $\model$ and pre-train the model again. This avoids continually updating the broken model.  %Model re-initialization can happen frequently when a market change frequently.
If after model re-initialization and training, the performance of the proposed $\model$ remains worse than that of  $\delta^{BS}_{t,T,K}$ on the validation set, we simply output $\delta^{BS}_{t,T,K}$.
%to avoid a potential poor performance on the testing data. % due to adoption of the early stopping regularization.

In summary, we initialize the model as described in section \ref{sec:init} and \ref{sec:preT} on the first testing date. When we train the model,  %on other testing dates, 
we reuse the parameters from the previously trained model as the starting point unless the performance of the proposed $\model$ is worse than that of $\delta^{BS}_{t,T,K}$,
%When the performance of the proposed $\model$ is worse than that of  $\delta^{BS}_{t,T,K}$, 
in which case we re-initialize the parameters of the proposed $\model$ and train the model again.
\section{Alternative Data-Driven Hedging Models Under Neural Network Framework}
\label{sec:alternativeModel}
Lastly, before jumping into the real data experimental results for local discrete hedging in Chapter \ref{sec:LocalComparison}, we introduce the following supplementary models:
\begin{enumerate}
	\item To gain insights on the roles of the decoder and encoder in the proposed $\model$,  we  also consider a decoder only model $\modelN$, which corresponds to removing the encoder from $\model$ in Figure \ref{fig:DNN}.  We train  $\modelN$ in the same way  as described in section \ref{sec:train}. 
	\item To gain insights on the role of output gate and robust Huber loss, we remove the output gate part of the decoder model. The resulting model  is shown in Figure \ref{fig:DRNNC}. In other words, the output $\delta^M$ is purely based $\mathbf{\widehat{h}}_E$ and $ \widehat{\vx}_{t}^{T,K}$:
	\[
		{\delta}^M_{t,T,K}=sigmoid (\vv^T_{out} \ tanh( \vU_{out} \mathbf{\widehat{h}}_E + \vW_{out} \widehat{\vx}_{t}^{T,K}+ \vb_{out})).
	\]
	We denote the model shown in Figure \ref{fig:DRNNC} as $\textsc{GRU}_{c}$. The training procedure of $\textsc{GRU}_{c}$ is slightly different from $\model$:
	\begin{itemize}
		\item Early stopping, pre-training, and model re-initialization mentioned in section \ref{sec:train} are not used.
		\item The validation set is not needed since early stopping and model re-initialization are not used.
		\item The objective function is fixed to be the mean squared loss.
	\end{itemize}
\end{enumerate}

\begin{figure}
	\centering
	\resizebox{0.40\textwidth}{!}{
		\begin{tikzpicture}[
		prod/.style={circle, draw, inner sep=0pt},
		ct/.style={circle, draw, inner sep=5pt, ultra thick, minimum width=10mm},
		ft/.style={circle, draw, minimum width=8mm, inner sep=1pt},
		filter/.style={circle, draw, minimum width=7mm, inner sep=1pt, path picture={\draw[thick, rounded corners] (path picture bounding box.center)--++(65:2mm)--++(0:1mm);
				\draw[thick, rounded corners] (path picture bounding box.center)--++(245:2mm)--++(180:1mm);}},
		mylabel/.style={font=\scriptsize\sffamily},
		>=LaTeX
		]
		\node[filter] (oe2) at (9, 5) {};
		\node[filter] (oe3) at (6, 5) {};
		\node [inner sep=0pt] (oe4) at (6, 7) {$\odot$};
		\node [inner sep=0pt] (oe5) at (9, 7) {$\odot$};
		\node [draw,circle,inner sep=0pt] (oe6) at (7.5, 8) {$+$};
		\node (oe7) at (7.5, 9) {$\delta^M_{t,T,K}$};
		\node  (bs) at (4.5, 7) {$\delta^{BS}_{t,T,K}$};
		\draw[->] (oe3.north) to node[left]{$1-W_{\delta}$} (oe4.south);
		\draw[->] (oe3.north) to node[above]{$W_{\delta}$} (oe5.west);
		\draw[->] (oe2.north) to node[left]{$\widehat{\delta}^M_{t,T,K}$} (oe5.south);
		\draw[->] (oe4.north) to node[left]{} (oe6.west);
		\draw[->] (oe5.north) to node[left]{} (oe6.east);
		\draw[->] (oe6.north) to node[left]{} (oe7.south);
		\draw[->] (bs.east) to node[left]{} (oe4.west);
		\node (he2) at (7.5, 4) {$\widehat{\vx}_{t}^{T,K}$};
		\draw[->] (he2.north) to (oe2.south);
		\draw[->] (he2.north) to (oe3.south);
		
		\node[inner sep=0pt]  (l1) at (7.5, 3) {$\odot$};
		\node[draw,rectangle]  (l3) at (7.5, 2) {softmax};
		\node  (l2) at (6, 3) {$\vx_{t}^{T,K}$};
		\node  (l4) at (6, 2) {$\omega^L$};
		\draw[->] (l4.east) to (l3.west);
		\draw[->] (l2.east) to (l1.west);
		\draw[->] (l3.north) to  (l1.south);
		\draw[->] (l1.north) to (he2.south);
		
		
		\end{tikzpicture}
	}\caption{$\modelN$: decoder only model}
	\label{fig:DNN}
\end{figure}



\begin{figure}
	\centering
	\resizebox{0.80\textwidth}{!}{
			\begin{tikzpicture}[
		prod/.style={circle, draw, inner sep=0pt},
		ct/.style={circle, draw, inner sep=5pt, ultra thick, minimum width=10mm},
		ft/.style={circle, draw, minimum width=8mm, inner sep=1pt},
		filter/.style={circle, draw, minimum width=7mm, inner sep=1pt, path picture={\draw[thick, rounded corners] (path picture bounding box.center)--++(65:2mm)--++(0:1mm);
		\draw[thick, rounded corners] (path picture bounding box.center)--++(245:2mm)--++(180:1mm);}},
		mylabel/.style={font=\scriptsize\sffamily},
		>=LaTeX
		]
		
			
			\node[draw,rectangle]  (s1) at (4.5, -1) {softmax};
			\node[draw,rectangle]  (s3) at (7.5, -1) {softmax};
			\node [draw,circle,inner sep=0pt] (rp1) at (3*1, -1) {$\cdot$};
			\node  (rp2) at (3*2, -1.5) {$\dots$};
			\node [draw,circle,inner sep=0pt] (rp3) at (3*3, -1) {$\cdot$};
	
			\foreach \i [count=\step from 1] in {$\vy_{\tau_1}^{T,K}$,$\dots$,$\vy_{\tau_{N+1}}^{T,K}$}
			\node (ri\step) at (3*\step, -2) {\i};
			\node  (sw1) at (4.5, -2) {$\omega^{S}$};
			\node  (sw3) at (7.5, -2) {$\omega^{S}$};
			\draw[->] (s1.west) to  (rp1.east);
			\draw[->] (s3.east) to  (rp3.west);
			\draw[->] (ri1.north) to (rp1.south);
			\draw[->] (ri3.north) to (rp3.south);
			\draw[->] (sw1.north) to (s1.south);
			\draw[->] (sw3.north) to (s3.south);
			\node (h2) at (3*2, 0.0) {$\dots$};
			\foreach \step in {1,3} {
				\node[draw,rectangle] (h\step) at (3*\step, 0.0) {GRU};
			}
			\draw[->] (rp1.north) to node[left]{$\widehat{\vy}_{\tau_1}^{T,K}$} (h1.south);
			\draw[->] (rp3.north) to node[right]{$\widehat{\vy}_{\tau_{N+1}}^{T,K}$} (h3.south);
			
			%\draw[->] (i4) -> (h4.south);
			
			\foreach \step in {1,...,2} {
				\pgfmathtruncatemacro{\next}{add(\step,1)}
				\draw[->] (h\step.east) -> (h\next.west);
			}
		
			\node[draw,circle,inner sep=0pt]  (l1) at (12.5, 0) {$\cdot$};
			\node[draw,rectangle]  (l3) at (12.5, -1) {softmax};
			\node  (l2) at (14, 0) {$\vx_{t}^{T,K}$};
			\node  (l4) at (14, -1) {$\omega^{L}$};
			\draw[->] (l4.west) to (l3.east);
			\draw[->] (l2.west) to (l1.east);
			\draw[->] (l3.north) to  (l1.south);
	
			\node[filter] (he2) at (10.75, 1) {};
			\node (he3) at (10.75, 2) {$\delta^{M}_{t,T,K}$};
			\draw[->] (h3.north) to node[above]{$\widehat{\mathbf{h}}_E$} (he2.west);
			\draw[->] (l1.north) to node[above]{$\widehat{\vx}_{t}^{T,K}$} (he2.east);
			\draw[->] (he2.north) to (he3.south);
	
			\node at (10.75, -3) {};
			\end{tikzpicture}
	}\caption{$\textsc{GRU}_{c}$: Removing the Output Gate}
	\label{fig:DRNNC}
	\end{figure}



\chapter{Local Discrete Hedging Performance Comparison Using  S\&P 500 index Options}
\label{sec:LocalComparison}
Using the S\&P 500  ({European})  index option market data from January 2, 2001 to August 31, 2015,
we  compare hedging performance of different hedging strategies.

Following \cite{hulloptimal},  we use the Gain ratio below as the evaluating measure for the hedging performance:
\[
\textsc{Gain}=1-\frac{\sum_{i=1}^m \bigg((\Delta V^{mkt}_{t_i,T_i,K_i}-\delta_{t_i,T_i,K_i} ~ \Delta S_{t_i} \bigg)^2}{\sum_{i=1}^m \bigg((\Delta V^{mkt}_{t_i,T_i,K_i}-\delta^{BS}_{t_i,T_i,K_i} ~ \Delta S_{t_i} \bigg)^2},
\]
where $m$ is the total number of data instances to be evaluated. In addition,  for the data instance $i$, corresponding to hedging option with expiry $T_i$ and strike $K_i$ at hedging time $t_i$, we recall that $\delta^{BS}_{t_i,T_i,K_i}$ is the implied Black–Scholes delta, $\delta_{t_i,T_i,K_i}$  is the hedging position from the method under consideration,  e.g., SABR delta,  MV delta,  and output from $\DKLs$ or $\model$.
  In addition $\Delta V^{mkt}_{t_i,T_i,K_i}$ is the change in option price, and $\Delta S_{t_i}$ is  the change in the underlying price which are given by \eqref{eq:DVDS}.




\section{Data and Experimental Setting}
The option data used in this study comes from the OptionMetric database  and the data is processed following the same procedure described in \cite{hulloptimal}.
On each day, the mid-price from the bid and ask is used as the price of the option on that day. The closing price for the underlying is regarded as the daily underlying price.
Options with time-to-expiry less than 14 days are removed from the data set.  The call options  with the  BS delta $\delta^{BS}_{t,T,K}$ from the implied volatility outside of  [0.05, 0.95]  and the put options with the  BS delta $\delta^{BS}_{t,T,K}$ from implied volatility outside of [-0.95, -0.05] are also removed from the data set. These removed option prices are noisy and unreliable since they correspond to either deeply out-of-money options, deeply in-money option, or very short term options.
As in \cite{hulloptimal},  the option data instances are divided into nine buckets according to the Black–Scholes delta $\delta^{BS}$ from the implied volatility (rounded to the nearest tenth) for  detailed hedging performance comparisons.


Inputs to $\model$, $\mathbf{Y}_{t}^{T,K} \in \Real^{d_s \times (N+1)}$, are  6  sequential features and the length of each sequence is 6, i.e., $d_s=6$ and $N=5$.  At each hedging time $t$, the six features are:
\begin{enumerate}
	\item  option middle price  for the previous 5 business days and the hedging day $t$,

	\item  option implied volatility  for the previous 5 business days and the hedging day $t$,
	\item  option  Black–Scholes delta  for the previous 5 business days and the hedging day $t$,
	\item  option Black–Scholes gamma  for the previous 5 business days and the hedging day $t$,
	\item  option Black–Scholes vega  for the previous 5 business days and the hedging day $t$,
	\item  moneyness  for the previous 5 business days and the hedging day $t$.
\end{enumerate}
 At each hedging time $t$, there are 10 local features  $\vx_{t}^{T,K}$:
\begin{enumerate}
	\item moneyness,
	
	\item time to expiry,
	\item index close price,
	\item option bid price,
	\item option offer price;
\item option Black–Scholes delta,
\item minimum variance delta $\delta_{MV}$  \eqref{eq:HullWhite} estimated using
the training data set,
	\item option implied volatility;
	\item option Black–Scholes gamma;
	\item option Black–Scholes vega.
\end{enumerate}

The number of hidden states, for the single-layer GRU encoder, the neural network outputting $\widehat{\delta}^M_{t,T,K}$, and the neural network outputting $W_{\delta}$  in Figure \ref{fig:RNNModel}, are all set to be 5.   In comparison with most RNN applications, we note we train a relatively small model and the number of data instances available for the hedging problem is also relatively small.
We note that our training and testing data instances are generated from daily observations with a moving window.  On each day, data instances observed in the previous 36 months are reserved as the training set and the validation set. For daily and weekly hedging, the validation consists of the data instances observed during the past 30 days. For monthly hedging, the validation set is the data instances observed during the previous 60 days. This is because, for monthly hedging, the  $\DS_t$ and $\Delta V^{mkt}_{t,T,K}$  of many data instances during the last 30 days have not been observed.
The data instances, which are observed in the previous 36 months but are not used in the validation set, are used as the training set.
The models are updated daily.





We note that the length of our backtest time is longer than a decade.
 Since
learning a single $\DKLs$ model for all options will be computationally expensive,   a separate model is built for each delta bucket when training $\DKLs$, which has the feature set \# 2
 $$\{\textsc{moneyness,~time-to-expiry,~ Black–Scholes Delta}\}.$$
 In addition,  we use the efficient leave-one-out cross-validation (LOOCV) as in section \ref{sec:cross} to choose regularization parameter $\lambda_P$ for $\DKLs$. For each month, LOOCV is used to choose a  value of the regularization parameter from a candidate set:
$$\lambda_P \in \{10^7,10^6,10^5,10^4,10^3,10^2,10^0,10^{-1},10^{-2}, 10^{-3},10^{-4},10^{-5},10^{-6},10^{-7} \}.$$
For the call option, the  $DKLs$ for each bucket is estimated  using all options \textbf{traded} in a 36 months window. For the put option, there is much larger variation in the number of data instances in each bucket.
 To ensure a reasonable training data size, we have used a time window of 24 months for delta buckets -0.1 and -0.2,   window of 36 months for delta buckets -0.3 and -0.4,  window of 48 months  for delta buckets -0.5 and -0.6, and window of 72 months  for delta buckets -0.7, -0.8 and -0.9. Hence, the training data  for puts ranges from  Jan 2001 to August 31 2015.
We train the direct hedging function model $\DKLs$ using all options \textbf{traded} in a fixed time window, as described above, and determine the hedge each day in the following month using the trained model, following the same sliding training-testing window procedure in \citep{hulloptimal}.Furthermore, the  $\DKLs$  model is updated monthly due to the limitation of computationally cost.



 The training set, the validation set, and the testing set are the same for $\model$ and $\modelN$. The training procedure is the same for $\model$ and $\modelN$ which is described in section \ref{sec:train}. In addition, $\textsc{GRU}_{c}$ is trained using the data instances, observed in the previous 36 months and procedure described as in section \ref{sec:alternativeModel}. 
 
 For additional reference, we also include the performance of the MV model \eqref{eq:HullWhite}  and Bartlett delta on weekly and monthly hedging. For $\MV$ in Table \ref{SP500CallC} \& \ref{SP500PutC}, on each day,  the model parameter, $a,b \text{ and } c$ are estimated using all {traded} options in a 36-month-window. \footnote{Note that, in the OptionMetric Database, not all the option data instances are actually traded. For many data instances, the trading volumes are zero. It is not clear to us whether the reported test results in \citep{hulloptimal} are computed using all data or traded data only.} 






 For $\DKLs$ and $\model$, we present out-of-sample daily hedging performance test result using either {traded} data or {all} data in the database.  


\subsection{Weekly and Monthly Hedging Comparisons}\label{sec:weekly}



We  first present hedging comparisons for weekly and monthly hedging. We assume a period of 5-business-days  for weekly hedging and a  period of  20-business-days  for monthly hedging.
We note that the MV hedging method is only considered for daily hedging in \cite{hulloptimal}.

Specifically, we compare $\model$ with the following methods,
\begin{itemize}
	\item $\model$: The sequential learning framework trained as in section \ref{sec:train}.
	\item $\MV$: MV hedging   based on formulation \eqref{eq:HullWhite},
    \item Bartlett: Barlett corrective  delta based on \eqref{eq:bartlett},
    \item $\DKLs$: Direct spline kernel hedging method as in \ref{sec:KernelDirect},
    \item $\modelN$: NN with the decoder only described in Figure \ref{fig:DNN}.
    \item $\textsc{GRU}_{c}$: The sequential learning model excluding the output gate and  trained as in section \ref{sec:alternativeModel}.
\end{itemize}


Table \ref{SP500CallC} present comparisons for calls while Table \ref{SP500PutC} demonstrate comparisons for puts.
We further note that  the parameters of $\MV$ are updated daily  in this section while the parameters of $\MV$ in \cite{hulloptimal} are updated monthly.
We also note that the out-of-sample results for the $\MV$ model in \cite{hulloptimal}  are obtained with daily changes of the index price and option price in  \cite{hulloptimal}  while, in Table \ref{SP500CallC} and \ref{SP500PutC}, the out-of-sample results are obtained with weekly and monthly changes of the index price and option price. The SABR model used to compute the Bartlett delta is also calibrated  daily.



\begin{table}[htp!]
	\centering
	\small
	\begin{threeparttable}
		\begin{tabular}{|c|cccccc| cccccc|}
			\hline
			\multirow{4}{*}{Delta}&\multicolumn{12}{c|}{Comparing Model(\%)}\\
			&\multicolumn{6}{c}{Weekly }&\multicolumn{6}{c|}{Monthly}\\ %\cline{2-5}
			&{\tiny MV}& {\tiny Bartlett}&\multicolumn{1}{c}{\tiny $\DKLs$}
			&\multicolumn{1}{c}{\tiny $\modelN$ } &\multicolumn{1}{c}{\tiny $\textsc{GRU}_{c}$} &\multicolumn{1}{c}{\tiny $\model$}
            &{\tiny MV}& {\tiny Bartlett} &\multicolumn{1}{c}{\tiny $\DKLs$}
            &\multicolumn{1}{c}{\tiny $\modelN$} &\multicolumn{1}{c}{\tiny $\textsc{GRU}_{c}$} & \multicolumn{1}{c|}{\tiny $\model$ } \\ \hline
			0.1 &26.3 &-16.9   &38.9    &35.6  & 36.6 &\textbf{47.8}   &13.5  & -8. 2   &22.7 &29.7         &34.8     & \textbf{53.9}  \\
			
			0.2 &21.6 &-5.6   &29.0     &36.4    &39.6  &\textbf{48.5}    &16.4  & 0.4   &23.5 &38.4      &38.9   & \textbf{51.7}  \\
			
			0.3 &20.1 &11.9   &23.5     &38.6   &39.7  &\textbf{48.5}    &17.9  & 2.1   &24.0 &40.2         &41.7 & \textbf{50.2}  \\
			
			0.4 &18.1 &17.3   &20.8     &38.7 &38.9  &\textbf{45.9}    &16.9  & 2.7   &21.0 &38.6           & 42.6& \textbf{47.8}  \\
			
			0.5 &16.0 &21.7   &19.9     &42.3   &37.5   &\textbf{46.6}    &15.2  & 5.7   &13.5 &36.3    &42.3       & \textbf{44.5}  \\
			
			0.6 &12.1 &24.1   &17.3     &43.4  &33.5 &\textbf{44.8}    &12.7  & 8.4  &14.3 &36.0       &40.7  & \textbf{44.6}  \\
			
			0.7 &8.1  &26.3   &16.8         &45.6  & 31.1 &43.9    &5.9   & 7.5   &6.1  &30.2        &26.3 & \textbf{35.3}  \\
			
			0.8 &3.7  &25.5   &12.5     &39.6  &31.7 &\textbf{37.7}   &-1.2  & 4.2   &5.3      &22.3         &26.3 & \textbf{24.8}  \\
			
			0.9 &2.4  &21.7   &6.2      &26.3 &28.7&16.4     &-1.8  &9.8    &4.1  &21.1       & 17.3& 10.5  \\
			
			Overall&15.1&18.6 &20.2     &39.9 &33.5  &\textbf{43.7}    &13.4  & 4.5   &16.3 &35.4     &38.0    & \textbf{44.5}  \\
			\hline
		\end{tabular}
		\caption{S\&P 500 Call Options Hedging Comparison on Traded Data, bold entries indicating best Gain from $\model$}
\label{SP500CallC}
\end{threeparttable}
\end{table}


 From  Table \ref{SP500CallC} and Table \ref{SP500PutC},  we observe that, overall, $\model$ outperforms $\textsc{GRU}_{c}$, $\MV$, Bartlett, $\DKLs$, and $\modelN$. In addition,
 the hedging performance of the four data-driven models, $\DKLs$, $\modelN$, $\textsc{GRU}_{c}$, and $\model$, is   significantly better than that of the MV model and Bartlett except for put option weekly hedging where Bartlett correction performs better than $\DKLs$. This is not surprising since the MV formula \eqref{eq:HullWhite} and Bartlett formula \eqref{eq:bartlett} are based on instantaneous hedging analysis.  Furthermore, by comparing $\DKLs$ with $\modelN$, we  see that the decoder only $\modelN$ still achieves significant improvement over $\DKLs$. The improvement possibly comes from inclusions of more local features in $\modelN$ and more frequent updates of $\modelN$, timely addressing the market changes. The overall improvement of $\model$ over $\modelN$ illustrates the important role of the GRU encoder, which incorporates sequential feature information, in weekly and monthly hedging. Lastly,  the overall improvement of $\model$ over $\textsc{GRU}_{c}$ illustrates the important role of the output gate, the robust huber loss and the robust training procedure in section \ref{sec:train}.

\begin{table}[htp!]
	\centering
	\small
	\begin{threeparttable}
		\begin{tabular}{|c|cccccc| cccccc|}
			\hline
			\multirow{4}{*}{Delta}&\multicolumn{12}{c|}{Comparing Model(\%)}\\
			&\multicolumn{6}{c}{Weekly }&\multicolumn{6}{c|}{Monthly}\\ %\cline{2-5}
			&{\tiny MV}& {\tiny Bartlett}&\multicolumn{1}{c}{\tiny $\DKLs$}
			&\multicolumn{1}{c}{\tiny $\modelN$ } &\multicolumn{1}{c}{\tiny $\textsc{GRU}_{c}$} &\multicolumn{1}{c}{\tiny $\model$}
            &{\tiny MV}& {\tiny Bartlett} &\multicolumn{1}{c}{\tiny $\DKLs$}
            &\multicolumn{1}{c}{\tiny $\modelN$} &\multicolumn{1}{c}{\tiny $\textsc{GRU}_{c}$} & \multicolumn{1}{c|}{\tiny $\model$ } \\ \hline
			-0.9   &23.9  &9.1  &10.1   &29.5 &32.1 &\textbf{34.7}  &16.9 &1.2 &6.5   &28.3         & 27.4& \textbf{32.6}  \\

			-0.8   &21.5  &-0.1 &18.3   &39.6 &40.1 &\textbf{44.2}  &11.5 &5.6 &6.1   &41.7        &35.6 & \textbf{49.5}  \\

			-0.7   &19.1  &0.4  &20.2   &44.0 &40.6 &\textbf{49.6}  &9.6  &6.7 &7.3   &43.4        &42.1 & \textbf{52.4}  \\
			-0.6   &16.1  &9.1  &20.8   &43.0 &43.6 &\textbf{51.3}  &8.1  &8.6 &10.3  &42.1        &42.5 & \textbf{51.6}  \\
			-0.5   &15.3  &20.9 &22.4   &43.4&41.3 &\textbf{53.5}  &7.7  &13.2 &13.9  &41.2        &42.7 & \textbf{51.4}  \\
			-0.4   &12.3  &25.7 &21.0   &41.4&45.6 &\textbf{53.2}  &6.8  &14.4 &15.6  &40.7        &42.9 & \textbf{53.4}  \\
			-0.3   &9.7   &29.4 &22.2   &38.2&44.6 &\textbf{51.1}  &4.7  &13.6 &19.5  &34.1       &47.5  & \textbf{48.4}  \\
			-0.2   &7.8   &33.1 &20.8   &29.8 &40.4&\textbf{46.3}  &2.9  &10.7 &20.6  &21.7         & 36.7 &\textbf{44.7}  \\
			-0.1   &4.9   &30.5 &19.2   &15.5&36.4 &\textbf{37.2}  &-1.8 &10.8 &13.0  &12.3         &24.1& \textbf{26.8}  \\
			Overall&14.4  &26.4 &20.4   &38.7 &42.4&\textbf{49.1}  &8.6  &12.1 &13.5  &38.6        &43.5 & \textbf{49.5}  \\
			\hline
		\end{tabular}
		\caption{S\&P 500 Put Options Hedging Comparison on Traded Data, bold entries indicating best Gain from $\model$}
\label{SP500PutC}
\end{threeparttable}
\end{table}


\subsection{Feature Importance}\label{sec:featureWeek}
Next, we discuss feature importance in the trained model $\model$. Specifically, we show how the normalized feature weight, for both the local features $\vx_{t}^{T,K}$ and for the sequential features $\mathbf{X}$,  changes from Jan 2007 to Aug 2015. 
The normalized feature weight vector are:
\[
\frac{exp(\omega^L)}{\sum_{i=1}^{d_l} exp(\omega^L_i)}
\]
and
\[
\frac{exp(\omega^S)}{\sum_{i=1}^{d_s} exp(\omega^S_i)}
\]
respectively.
The feature score, shown in Figure \ref{fig:call1}\& \ref{fig:put1}, represents the normalized feature weights averaged over a calendar month for the model trained using {traded} data instance. Note that
the model is updated daily and  the normalized feature weights vary with the training date.
\begin{figure}[htp]
\centering
% \subfigure[Local Features Daily Hedging]{
%   \includegraphics[width=0.48\textwidth]{CLocal0}}
% \subfigure[Sequential Features Daily Hedging]{
%   \includegraphics[width=0.48\textwidth]{C0}}
\subfigure[Local Features Weekly Hedging]{
	\includegraphics[width=0.48\textwidth]{./figures/CLocal1}}
\subfigure[Sequential Features Weekly Hedging]{
	\includegraphics[width=0.48\textwidth]{./figures/C1}}
\subfigure[Local Features Monthly Hedging]{
	\includegraphics[width=0.48\textwidth]{./figures/CLocal2}}
\subfigure[Sequential Features Monthly Hedging]{
	\includegraphics[width=0.48\textwidth]{./figures/C2}}
\caption{Feature Scores for Weekly and Monthly Hedging Models on  Traded S\&P500 Call Option  Data } \label{fig:call1}
\end{figure}





From subplots (a) and (b) in Figure \ref{fig:call1}, it can be observed that, for weekly hedging  call options, the index price $S$ is ranked as the most important local feature after Jan 2010.
Moreover, the average weight for the index price  $S$ exhibits an increasing trend after Jan 2009.
The past implied volatility sequence is ranked as the most important sequential feature after March 2009. The average weight for the past implied volatility sequence also exhibits an increasing trend after Jan 2009. From subplots (c) and (d) in Figure \ref{fig:call1}, it can be observed that, for monthly hedging  the call option, the index price $S$ is always ranked as the most important local feature. In addition, the average weight of the index price  $S$ is always higher than 0.5 after Jan 2008.
The past implied volatility sequence is always ranked as the most important sequential feature after Jan 2009. The average weight for the past implied volatility sequence also is always greater than 0.4 after Jan 2008.

For the put option, the situation is slightly different. From subplots (a) and (b) in Figure \ref{fig:put1}, we can see that, for weekly hedging the put option, the index price $S$ is often ranked as the most important local feature. The past implied volatility sequence and the past BS delta sequence are identified as the two most important sequential features. From subplots (a) and (b) in Figure \ref{fig:put1}, we can see that, for monthly hedging put options, the index price $S$ is ranked as the most important local feature after Jan 2008. The past implied volatility sequence is ranked as the most important sequential feature after Jan 2013.

Overall, the index price $S$ has often been identified as an important local feature for hedging  both call and put options. Since the BS delta of the implied volatility is one of the local features, this confirms the fact that the BS delta does not capture all dependence on the underlying
In addition, the past implied volatility has often been identified as an important sequential feature for hedging both calls and puts.

\begin{figure}[htp]
\centering
%\subfigure[Local Features Daily Hedging]{
%    \includegraphics[width=0.48\textwidth]{PLocal0}}
%\subfigure[Sequential Features Daily Hedging]{
%    \includegraphics[width=0.48\textwidth]{P0}}
\subfigure[Local Features Weekly Hedging]{
	\includegraphics[width=0.48\textwidth]{./figures/PLocal1}}
\subfigure[Sequential Features Weekly Hedging]{
	\includegraphics[width=0.48\textwidth]{./figures/P1}}
\subfigure[Local Features Monthly Hedging]{
	\includegraphics[width=0.48\textwidth]{./figures/PLocal2}}
\subfigure[Sequential Features Monthly Hedging]{
	\includegraphics[width=0.48\textwidth]{./figures/P2}}
\caption{Feature Score for Weekly and Monthly Hedging Models on Traded S\&P500 Put Option  Data} \label{fig:put1}
\end{figure}

\subsection{Daily Hedging Comparison}\label{sec:daily}
In section \ref{sec:weekly} - \ref{sec:featureWeek}, we have have presented hedging comparisons for weekly and monthly hedging. Now we compare the proposed $\model$  with the following methods for daily hedging,
\iffalse
Also, the decoder only model significantly improves over the regularized kernel model $\DKLs$. This improvement comes partly from the daily model updating (which is computationally feasible for the proposed $\model$)  while $\DKLs$ is updated monthly only (due to computational cost and the length of the study period).
\fi


\begin{itemize}
	\item $\MV$: MV hedging   based on formulation \eqref{eq:HullWhite},
\item ${\LVF}$: MV hedging in \cite{hulloptimal} from local volatility function based on formulation \eqref{eq:HullWhiteLVF},
\item ${\SABR}$ : MV hedging in  \cite{hulloptimal} from {{SABR}} model based on formulation \eqref{eq:HullWhiteSabr},
\item ${\delta^{BS}}$:  implied  volatility Black–Scholes delta,
\item Bartlett: Bartlett formula based on \eqref{eq:bartlett},
  \item $\DKLs$: Direct spline kernel hedging method as in \ref{sec:KernelDirect}.
\end{itemize}

We have omitted  $\modelN$ and $\textsc{GRU}_{c}$ in the daily hedging comparison since it has been shown to underperform $\model$. But we  have added ${\LVF}$ and ${\SABR}$ since they are based on instantaneous hedging analysis and are more suitable for daily hedging.

%In the following discussion, we use $\model$  to denote the proposed $\model$ in Figure \ref{fig:RNNModel}.
Since the  same S\&P 500 index option data from the OptionMetric Database is used here,  we simply quote the results  presented in \cite{hulloptimal} for  $\MV$,  ${\LVF}$,  and ${\SABR}$, and the results  presented in \cite{knian2017} for  $\DKLs$.

As stated in \cite{hulloptimal} and section \ref{sec:Classical},
the ${\SABR}$ model is calibrated daily, from which the hedge position is determined and applied the next day.
For  ${\LVF}$, the partial derivative of the expected implied volatility to the underlying is calculated from the slope of the observed implied volatility smile daily.  For $\MV$, the model parameter, $a,b \text{ and } c$ are estimated using all options in a 36-month-window and then applied to determine the hedging position daily in the subsequent month.
%We refer interested readers to \citep{hulloptimal} for more details of how  ${\SABR}$,${\LVF}$ and $\MV$ are calibrated and implemented.


Table \ref{SP500Call} presents daily hedging comparisons for call options.
From  Table \ref{SP500Call}, using either {traded} data or {all} data for testing, $\model$  outperforms the minimum variance hedging methods reported in \cite{hulloptimal} in the overall performance.   For most delta buckets, delta greater than or equal to 0.3, the performance of  $\model$  is better than those of the minimum variance hedging methods. However, for the bucket of the delta value 0.1 or 0.2, the performance is slightly worse.


%Secondly, we compare the hedging performance from the direct kernel hedging $\DKLs$ method and the proposed $\model$ method.
In addition, from Table \ref{SP500Call}, the performance of the $\model$ model is slightly better than that of the kernel hedging $\DKLs$ in overall performance. For the the delta buckets 0.3-0.9,  $\model$ outperforms the direct kernel hedging $\DKLs$.  However,  $\model$ performs slightly worse for the delta bucket 0.1 and 0.2.

Table \ref{SP500Put}  presents daily hedging comparisons for put options.
Table \ref{SP500Put} shows that both overall performance and the performances for different delta buckets of the $\model$ model are better than those from minimum variance hedging methods in \cite{hulloptimal} and the direct kernel hedging $\DKLs$, for testing result using all data. Similarly to the call, $\model$ has model bold entries in most of delta buckets, indicating outperforming other methods.
\iffalse
For the testing result using only traded data, the overall results from $\model$ are better than results reported from the minimum variance hedging methods in \cite{hulloptimal} and from the direct kernel hedging $\DKLs$.  As for the testing results with traded data, in different buckets, the performances of $\model$ for delta buckets -0.9, -0.8 and -0.1 are slightly worse than those from the minimum variance hedging methods in \cite{hulloptimal} and the direct kernel hedging $\DKLs$,  while  the performance of the $\model$ method for the other delta bucket is  better or similar.
\fi

 Both $\model$ and $\DKLs$ are non-parametric data-driven hedging  methods, with a main difference in features included. The hedging performance comparison between the two suggests that including the sequential features and more local features improves the overall hedging performance slightly for daily hedging.



\begin{table}[htp!]
\centering
\begin{threeparttable}
\begin{tabular}{|c |r r r r r r r r r|}
\hline
\multirow{3}{*}{Delta}&\multirow{3}{*}{MV (\%)}&\multirow{3}{*}{\;$\SABR$(\%)}&\multirow{3}{*}{\LVF (\%)}&\multicolumn{2}{c|}{Bartlett}& \multicolumn{4}{c|}{Data-Driven Model}\\
&&&&\multirow{2}{*}{Traded}&\multirow{2}{*}{All}&\multicolumn{2}{|c}{$\DKLs$ (\%)} &\multicolumn{2}{c|}{$\model$ (\%)}\\
&&&&&&\multicolumn{1}{|c}{\small Traded}&\multicolumn{1}{c}{\small All}&\multicolumn{1}{c}{\small Traded}&\multicolumn{1}{c|}{\small All}\\ \hline
  0.1 & 42.1 & 39.4 & 42.6 &29.0 &35.1    & 47.1  & 48.6        &32.3          &   33.8        \\
  0.2 & 35.8 & 33.4 & 36.2 &28.2 &32.3    & 37.8  & 40.0        &33.7          &   36.4        \\
  0.3 & 31.1 & 29.4 & 30.3 &27.7 &28.9   & 34.1  & 35.1        &34.1          &\textbf{35.5}        \\
  0.4 & 28.5 & 26.3 & 26.7 &28.7 &27.3   & 32.3  & 32.0        &\textbf{33.7} &\textbf{34.2}    \\
  0.5 & 27.1 & 24.9 & 25.5 &26.9 &26.7   & 29.3  & 29.4        &\textbf{35.1} &\textbf{33.0}   \\
  0.6 & 25.7 & 25.2 & 25.2 &28.3 &26.6   & 29.9  & 28.4        &\textbf{35.6} &\textbf{32.1}    \\
  0.7 & 25.4 & 24.7 & 25.8 &28.5 &26.4   & 29.0  & 26.8        &\textbf{31.8} &\textbf{29.7}   \\
  0.8 & 24.1 & 23.5 & 25.4 &23.1 &24.9   & 25.9  & 24.7        &\textbf{28.6} &\textbf{26.5}   \\
  0.9 & 16.6 & 17.0 & 16.9 &14.0 &15.6   & 17.7  & 13.9        &\textbf{19.3} &\textbf{18.9}    \\
  Overall & 25.7 & 24.6 & 25.5 &27.1 &24.8 & 31.3  & 26.0        &\textbf{32.9} & \textbf{28.7} \\
  \hline
\end{tabular}
\caption{S\&P 500 Call Option Hedging For 1-Business Day: bold entries indicating best Gain from $\model$}
\label{SP500Call}

\end{threeparttable}
\end{table}


\begin{table}[htp!]
\centering
\begin{threeparttable}
\begin{tabular}{|c |r r r r r r r r r|}
\hline
\multirow{3}{*}{Delta}&\multirow{3}{*}{MV (\%)}&\multirow{3}{*}{\;$\SABR$(\%)}&\multirow{3}{*}{\LVF (\%)}&\multicolumn{2}{c|}{Bartlett}& \multicolumn{4}{c|}{Data-Driven Model}\\
&&&&\multirow{2}{*}{Traded}&\multirow{2}{*}{All}&\multicolumn{2}{|c}{$\DKLs$ (\%)} &\multicolumn{2}{c|}{$\model$ (\%)}\\
&&&&&&\multicolumn{1}{|c}{\small Traded}&\multicolumn{1}{c}{\small All}&\multicolumn{1}{c}{\small Traded}&\multicolumn{1}{c|}{\small All}\\ \hline
  			-0.9 & 15.1    &11.2  &-7.4 &9.1  &23.4   &8.6    &13.6  &\textbf{15.1}    &\textbf{17.2} \\
			-0.8 & 18.7    &19.6  &6.8  &3.2  &21.9   &6.5    &16.7  &\textbf{23.2}    &\textbf{28.5} \\
			-0.7 & 20.3    &17.7  &9.1  &1.5  &20.1   &10.6   &19.8  &\textbf{28.5}    &\textbf{32.8} \\
			-0.6 & 20.4    &16.7  &9.2  &6.1  &19.2   &14.9   &21.0  &\textbf{28.3}    &\textbf{33.9} \\
			-0.5 & 22.1    &16.7  &10.8 &15.5 &21.3   &22.5   &23.1  &\textbf{29.2}    &\textbf{34.5} \\
			-0.4 & 23.8    &17.7  &12.0 &21.0 &24.4   &24.2   &25.2  &\textbf{29.9}    &\textbf{34.7} \\
			-0.4 & 27.1    &21.7  &16.8 &26.7 &29.0   &27.7   &28.3  &\textbf{30.6}    &\textbf{33.6} \\
			-0.2 & 29.6    &25.8  &20.6 &29.3 &31.6   &30.1   &30.8  &25.4    &29.9 \\
			-0.1 & 27.5    &26.9  &17.7 &31.4 &32.5   &29.1   &31.2  &18.7    &21.4 \\
			Overall &22.5  &19.0  &10.2 &20.0 &24.8   &23.4   &23.2  &\textbf{26.2}    &\textbf{29.7} \\
  \hline
\end{tabular}
\caption{S\&P 500 Put Option Hedging For 1-Business Day: bold entries indicating best Gain from $\model$  }
\label{SP500Put}

\end{threeparttable}
\end{table}

We similarly demonstrate feature importance in Figure \ref{fig:call1daily}\&\ref{fig:put1daily} for daily hedging.  From Figure \ref{fig:call1daily}, it can be observed that, for daily hedging call options, MV delta $\delta^{MV}$ and index price $S$ are the local features that are often ranked as the most important under $\model$. The history of BS delta $\delta^{BS}$ is ranked as the most important sequential feature during most of the months. The past implied volatility sequence is often ranked as the second most important sequential feature.
\begin{figure}[htp]
\centering
\subfigure[Local Features Daily Hedging]{
	\includegraphics[width=0.48\textwidth]{./figures/CLocal0}}
\subfigure[Sequential Features Daily Hedging]{
	\includegraphics[width=0.48\textwidth]{./figures/C0}}
\caption{Feature Score for Daily Hedging Model: S\&P500 Call Option (Traded Data)} \label{fig:call1daily}
\end{figure}

From subplots (a) and (b) in Figure \ref{fig:put1daily}, it can be observed that the index price $S$ is often ranked as the most important local feature for daily hedging put options. The past implied volatility sequence, the past  BS delta sequence, and the past option price $V_{mid}$  are often identified as the three important sequential features.
\begin{figure}[htp]
\centering
\subfigure[Local Features Daily Hedging]{
	\includegraphics[width=0.48\textwidth]{./figures/PLocal0}}
\subfigure[Sequential Features Daily Hedging]{
	\includegraphics[width=0.48\textwidth]{./figures/P0}}
\caption{Feature Score for Daily Hedging Model:  S\&P500 Put Option (Traded Data)} \label{fig:put1daily}
\end{figure}
\section{Concluding Remarks for Data-Driven Local Hedging Models}\label{sec:Localconclusion}

In this thesis, we have proposed a direct kernel hedging model $\DKLs$ and a novel encoder-decoder  model, $\model$ , for discrete local risk option hedging. The $\DKLs$ is our preliminary exploration on computing a data-driven local hedging model without estimating a option pricing model. The $\model$ is our enhancement to data-driven local hedging model. $\model$ consists of an encoder, which generates a concise representation of the past market  information. The decoder uses the Black-Scholes delta as a pre-trained model and utilizes a gate to generate a predicative hedging model,  combining the pre-trained delta model and the outputs from the encoder. Feature selections are implemented through normalized weights embedded in the model training. In addition, a data instance adaptive Huber loss function is incorporated  for robustness, with the error from the pre-trained Black-Scholes delta model for each instance as the thresholding parameter.


Using the S\&P 500 index and the index option data,  from January 1, 2004, to  August 31st, 2015, we assess and compare
hedging performance of the  $\model$ and $\DKLs$ with other hedging strategies.
 For weekly and monthly hedging, computational results demonstrate that performance of the proposed $\model$  significantly surpasses that of the MV model , SABR-Bartlett, regularized spline kernel model $\DKLs$ , all of which perform significantly better than the Black–Scholes model with implied volatility.
In addition, the $\DKLs$ also outperforms MV model in terms of weekly and monthly hedging results.
We further demonstrate that the encoder for the sequential features plays a significant role in $\model$, since removing the encoder deteriorates hedging performance.
Lastly, by comparing the weekly and monthly hedging performance from $\textsc{GRU}_{c}$ and $\model$, we demonstrate that the output gate,  robust loss and training procedure, also play  a significant role in $\model$.

We further demonstrate that the daily hedging performance of the proposed $\model$  also surpasses that of the MV hedging method , LVF and SABR corrective methods  (implemented in \cite{hulloptimal}), data-driven regularized spline kernel network model $\DKLs$ , and SABR-Bartlett. In addition, $\DKLs$ also outperforms MV hedging method , LVF and SABR corrective methods  (implemented in \cite{hulloptimal}).

Besides, from the testing hedging performance, we assess feature importance in  $\model$  for the S\&P 500 index  option hedging. The monthly average feature weights identify the underlying as the most important local feature and the past implied volatility sequence as the most important sequential feature during normal market periods.

Overall, our research demonstrates the potential role of a market data-driven approach for financial derivative risk management.

A natural extension  is to extend the sequential local hedging model to be a total hedging model where we rebalance multiple times until the expiries of the options. In the following chapter \ref{sec:RNNTotal}, we discuss the challenges associated with building such models and present our exploration on the data-driven total hedging model.

\chapter{Data-Driven Sequential Learning Framework for Total Hedging Risk}
As we have discussed in section \ref{sec:DiscreteHedgingCriteria}, in reality, one often want to hedge until the expiry of the option. Enhancing the local hedging model $\model$ to output the multiple hedging positions by minimizing a total hedging objective is quite attractive. From the model perspective, such enhancement is achievable. Indeed, recently a deep hedging model was proposed to minimize the total option hedging risk evaluated at the option expiry  \citep{buehler2019deep}. It is shown  that using a RNN to represent the hedging position model can be a computationally efficient framework to determine the optimal hedging function when the market is incomplete, e.g., under the transaction cost \citep{buehler2019deep}.

Although minimizing the total hedging risk, which is hedging portfolio value at the expiry $T$, is more desirable. There are several major obstacles in obtaining enough market data to build a data-driven total hedging model. The deep hedging model \citep{buehler2019deep} is built on synthetic data. It is hard to directly apply their methodology in real world due to the lack of market data needed to build a sufficient model. 

In this chapter, we  provide several achievable remedies  to deal with issue of lack of market data.  A enhanced  sequential total hedging model $\modelT$ is introduced to minimize a discrete total hedging objective. We then build the total hedging model $\modelT$ based on the remedies we proposed and demonstrate the effectiveness of each of the remedies using real market data experiments.  




\section{Data Augmentation for Market Option Data}
\label{sec:DataAugmentation}
Recall that the discrete total hedging risk \eqref{eq:TotalObjV3} is defined as the the hedging portfolio value at expiry:
\[
    P_H(T,T,K)=\sum_{j=0}^{N-1}\left\{ \left[e^{r (N-j-1) \Delta t} \Smkt_{t_{j+1}}-e^{r (N-j) \Delta t}\Smkt_{t_{j}}\right] \delta_{t_j,T,K} \right\}+e^{r N \Delta t} \Vmkt_{t_0,T,K}-\Vmkt_{T,T,K}
\]

Suppose we have $M$ hedging scenarios, a data driven model can be built by minimizing total hedging loss function such as MSE:
\[
    MSE_{total}=\sum_{i=1}^M P_H(T_i,T_i,K_i)^2
\]

Clearly, in order to build a efficient data-driven total hedging model, one needs to gather a sufficiently large number of sample  $M$.  In addition, ideally, one can also use non-overlapping underlying asset paths to generate training and testing hedging scenarios.
However, in reality,  we have the following challenges:
\begin{enumerate}
	\item Using non-overlapping underlying asset path to  generate training and testing scenarios is not practical. For instance, assuming we are building a data-driven model for hedging 3 months until expiry, with non-overlapping underlying asset paths, 20 years of market data can provide only 80 different non-overlapping underlying asset paths which is clearly not enough to build a data-driven model.
	\item Option data  in real market is limited. 
	\begin{enumerate}
		\item Options listed in exchange in real market only have fixed expiry dates. For example, the expiration date for  S\&P 500 index option is fixed at the third Friday of each month. Therefore, if we only use market available expiration dates, the number of training scenarios will also be severely limited. 
		\item In addition, the option with specific strike $K$ and expiry $T$ is not traded on every data. Even we use market available expiration dates, on the initial date when we want to set up the hedging portfolio, we may not have the initial market option price $\Vmkt_{t_0,T,K}$.
	\end{enumerate}
\end{enumerate}
In order to overcome the above challenges, we propose the following solution.  
\begin{enumerate}
	\item Overlapping underlying asset path is allowed to be used.
	\item Instead of using only market available expiration dates, we assume every business day can be the expiration dates of the options.
	\item A no-arbitrage price surface is calibrated every day $t$ to obtain option price for any arbitrary combination of  $K$ and $T$: $V^{Aug}_{t,T,K}$.
\end{enumerate}

Therefore, we can greatly increase the number of training scenarios, since now the hedging scenarios can have arbitrary combination of $T$ and $K$ even if the combination of $T$ and $K$ is not observed in the market. 

\subsection{No-Arbitrage Price Surface}
In this section, we describe how to calibrate an arbitrage-free price surface on each business day. We want to create a price surface that is \textbf{arbitrage-free}. In addition, we want to able to \textbf{extrapolate} for deep in-money options and deep out-of-money options. 
Let  $Surf_{t}(T,K)$ be the option price surface function at time $t$. We want to make sure that the price surface satisfies the following no-arbitrage conditions:
\begin{enumerate}
		\item No call spread arbitrage or put spread arbitrage:
		\[
		-1 \leq \frac{\partial Surf_{t}(T,K)}{ \partial K} \leq 0
		\; \text{ (Call) }
		\]
		or
		\[
			0 \leq \frac{\partial Surf_{t}(T,K)}{ \partial K} \leq 1
			\; \text{ (Put) }
		\]
		\item  No butterfly spread arbitrage:
		\[
		\frac{\partial^2 Surf_{t}(T,K)}{ \partial K^2} \geq 0
		\]
		\item  No calendar spread arbitrage:
		\[
		\frac{\partial Surf_{t}(T,K)}{ \partial T} \geq 0
		\]
\end{enumerate}
In the following subsection, we will discuss the steps of calibration of the arbitrage-free surface.
\subsubsection{SABR Interpolation and Extrapolation}
In this thesis, we will use SABR model to obtain the option price for arbitrary strike $K$ unobserved in the market.
On each business day $t$, we have observed option quotes for a set of market observed expires. We denote the market available expires as $T_1 < \dots< T_{max}$. We thus have a grid of expires $t=T_0 < T_1 < \dots< T_{max}$ where we have market quotes for option (Note for for $T_0$, the market option price is just the option payoff at $t$).

We firstly use SABR model to obtain option price for a predetermined grid of strike $0=K_0<K_1<\dots<K_{max}$.  More specifically, for a fixed expiry $T_i$, let us define 
\[K^{mkt}=
\{
	\mathbb{K}^{mkt}_{1},\dots,K^{mkt}_{N_K}
\}
\] 
to be the strikes for which market option price is available for $T_i$.
We solve
\[
\min_{\alpha,\beta,\nu,\rho} \sum_{j=1}^{N_K} \left(V_{SABR}(S,t,T_i,K^{mkt}_j,r,q;\alpha,\beta,\nu,\rho)-\Vmkt(t,T_i,K^{mkt}_j)\right)^2
\]
where $V_{SABR}(S,t,T,K,r,q;\alpha,\beta,\nu,\rho)$ is option pricing function described in section \ref{sec:SABR}.

For each $T_i$, we calibrate a separate set of SABR parameters and we then use the calibrated SABR parameters to compute SABR price of the predetermined grid of strikes for each $T_i$.  The SABR prices of the predetermined grid of strikes are used as the calibration targets for the volatility interpolation procedure described in the following subsection.

Although SABR model is computationally efficient and can match the market volatility smile well, it is not arbitrage-free. Butterfly and call spread arbitrage can exist.
In addition,  for each expiry, a separate set SABR parameters are calibrated so calender spread arbitrage can also exist.
Therefore, before using the SABR price as the target for the LVF calibration, we need to fix the potential arbitrage.

In this thesis, we use the sufficient conditions for no arbitrage \cite{carr2005note} to check whether arbitrage exists for
 the grid of strikes: $0=K_0<K_1<\dots<K_{max}$, and the grid of expiries $t=T_0 < T_1 < \dots< T_{max} $. If arbitrage is detected, we will manually adjust the option prices from SABR model to satisfy the no-arbitrage condition.

\subsubsection{Volatility Interpolation}
\citet{andreasen2010volatility} have introduced an efficient and arbitrage free volatility interpolation method based on a one step finite difference implicit Euler scheme applied to a local volatility parametrization. In this thesis, we use the volatility interpolation approach to compute option price for arbitrary expiry $T$ unobserved in market.

The volatility interpolation method is based on the Dupire formula.  The Dupire enables us to deduce the volatility function in a local volatility
model from quoted put and call options in the market.
Under a risk-neutral measure, we assume:
	\[
	\frac{d S_t}{ S_t}= \left(r-q\right)dt +\sigma(t,S_t) dZ_t
	\]
 Let C be the call option pricing function \footnote{In the following discussion, we use call option as the example but the analysis also holds for put options.}. Dupire's equation states:
	\[
	\frac{\partial C}{\partial T}=\frac{1}{2} {\sigma}^2(T,K)K^2  \frac{\partial^2 C}{ \partial K^2}-(r-q) K\frac{\partial C}{\partial K}-qC
	\]
Define the normalized call price in terms of discount factor $D(t,T)$ and forward price $F(t,T)$ and the normalized strike $\widehat{K}$ as:
\[
\begin{split}
D(t,T)&=e^{-r(T-t)}\\
F(t,T)&=S_te^{(r-q)(T-t)}\\
\widehat{K}&=\frac{K}{F(t,T)}\\
\widehat{C}(T,\widehat{K})&=\frac{C(T,\widehat{K} F(t,T))}{D(t,T)F(t,T)}=\frac{C(T,K)}{D(t,T)F(t,T)}
\end{split}
\]
Dupire's equation can be simplified as:
\[
\frac{\partial \widehat{C}}{\partial T}=\frac{1}{2} \widehat{\sigma}^2(T,\widehat{K}) \widehat{K}^2  \frac{\partial^2 \widehat{C}}{ \partial \widehat{K}^2},\;\; \widehat{\sigma}(T,\widehat{K})={\sigma}(T,K)
\]


 We recursively solve the finite difference  discretization of the Dupure forward equation using fully implicit method:
\[
\frac{\partial \widehat{C}}{\partial T}=\frac{1}{2} \widehat{\sigma}^2(T,\widehat{K}) \widehat{K}^2  \frac{\partial^2 \widehat{C}}{ \partial \widehat{K}^2}
\]
with initial condition $\widehat{C}(T_0,\widehat{K})=max(1-\widehat{K},0)$, lower boundary condition $\widehat{C}(T,0)=1$, upper boundary condition $\widehat{C}(T,\widehat{K}_{max})=0$.
Given a grid of expiries $0=T_0 < T_1 < \dots< T_{max} $ and  the grid of normalized strike: $0=\widehat{K}_0<\widehat{K}_1<\dots<\widehat{K}_{max}$, we assume $\sigma(T_i,K)$  to be a piecewise constant functions for a given $T_i$.
\[
	\sigma(T_i,K)=\left\{ \begin{array}{ll}
		\sigma_{T_i,K_0}  , \; &\text{if} \; K \leq K_0\\
		\vdots & \vdots\\
		\sigma_{T_i,K_j}  , \; &\text{if} \; K_{j-1}<K \leq K_j\\
		\vdots & \vdots\\
		\sigma_{T_i,K_{max}} , \; &\text{if} \; K_{max} \geq K\\
		\end{array} \right.
\]
We further assume:
\[
 \frac{\partial^2 \widehat{C}(T,0)}{ \partial \widehat{K}^2}=\frac{\partial^2 \widehat{C}(T,\widehat{K}_{max})}{ \partial \widehat{K}^2}=0
\]
The fully implicit finite difference method in matrix form is:
\[\begin{bmatrix}
\widehat{C}(T_{i},\widehat{K}_0)\\
\widehat{C}(T_{i},\widehat{K}_1)\\
\widehat{C}(T_{i},\widehat{K}_2)\\
\vdots\\
\widehat{C}(T_{i},\widehat{K}_{max})
\end{bmatrix}=\mathcal{\mathbb{M}}
\begin{bmatrix}
\widehat{C}(T_{i+1},\widehat{K}_0)\\
\widehat{C}(T_{i+1},\widehat{K}_1)\\
\widehat{C}(T_{i+1},\widehat{K}_2)\\
\vdots\\
\widehat{C}(T_{i+1},\widehat{K}_{max})
\end{bmatrix}
\]
where $\mathcal{\mathbb{M}}$ is a tri-diagonal matrix parametrized by  $\sigma(T_i,.)$
\[\mathcal{\mathbb{M}}^{-1} \begin{bmatrix}
	\widehat{C}(T_{i},\widehat{K}_0)\\
	\widehat{C}(T_{i},\widehat{K}_1)\\
	\widehat{C}(T_{i},\widehat{K}_2)\\
	\vdots\\
	\widehat{C}(T_{i},\widehat{K}_{max})
	\end{bmatrix}=
	\begin{bmatrix}
	\widehat{C}(T_{i+1},\widehat{K}_0)\\
	\widehat{C}(T_{i+1},\widehat{K}_1)\\
	\widehat{C}(T_{i+1},\widehat{K}_2)\\
	\vdots\\
	\widehat{C}(T_{i+1},\widehat{K}_{max})
	\end{bmatrix}
	\]
	We try to find $\mathcal{\mathbb{M}}$ that so that $\widehat{C}(T_{i+1},\widehat{K}_j)=\widehat{C}_{SABR}(T_{i+1},\widehat{K}_j)$.
	Where
	\[
	\widehat{C}_{SABR}(T_{i+1},\widehat{K}_j)=\frac{C_{SABR}(T_{i+1},K_j)}{D(t,T_{i+1})F(t,T_{i+1})}
	\]
	\[
	K_j=\widehat{K}_j F(t,T_{i+1})
	\]
	and $C_{SABR}(T_{i+1},K_j)$ is the SABR price for the grid point $(T_{i+1},K_j)$.

	Given $\widehat{C}(T_{i},\widehat{K}_j)$, this can be done by:
	\begin{equation}
	\inf_{\sigma(T_i,.)} \sum_{j}(\frac{\widehat{C}(T_{i+1},\widehat{K}_j)-\widehat{C}_{SABR}(T_{i+1},k_j)}{Vega_{B}(T_{i+1},\widehat{K}_j)})^2
	\label{eq:LVFCal}
 	\end{equation}
	Note that $\widehat{C}(T_0,\widehat{K})=max(1-\widehat{K},0)$ is given, we can therefore recursively solve the optimization problem for $T_1, \dots, T_{max}$.

	After optimization,  the local volatility functions are translated into arbitrage-consistent prices for a discrete set of expiries but it does not directly specify the option prices between the expiries.  For $T \in [T_i,T_{i+1})$, we can fill in the gaps by:
	\[
	\begin{bmatrix}
	\widehat{C}(T,\widehat{K}_0)\\
	\widehat{C}(T,\widehat{K}_1)\\
	\widehat{C}(T,\widehat{K}_2)\\
	\vdots\\
	\widehat{C}(T,\widehat{K}_{max})
	\end{bmatrix}=\mathcal{\mathbb{M}}^{-1} \begin{bmatrix}
		\widehat{C}(T_{i},\widehat{K}_0)\\
		\widehat{C}(T_{i},\widehat{K}_1)\\
		\widehat{C}(T_{i},\widehat{K}_2)\\
		\vdots\\
		\widehat{C}(T_{i},\widehat{K}_{max})
		\end{bmatrix}
		\]
where $\mathcal{\mathbb{M}}$ is a tri-diagonal matrix parametrized by  $\sigma(T_i,.)$. Note that $\mathcal{\mathbb{M}}$ is known after the calibration \eqref{eq:LVFCal}.
We then recover the call price by:
\[
C(T,K)=\widehat{C}(T,\widehat{K}){D(t,T)F(t,T)}
\]
Interpolation based on the above procedure can guarantee the option prices computed is arbitrage-free. Detailed proof can be found in \cite{andreasen2010volatility}. To sum up, with the help of SABR model and the volatility interpolation procedure, we can compute the arbitrage-free option price for any arbitrary $T$ for the predetermined grid of strikes $0=K_0<K_1<\dots<K_{max}$. 
\subsubsection{Augemented Training Scenarios}

\begin{figure}
	\centering
	\subfigure[Implied  Volatility Surface]{
		\includegraphics[width=0.8\textwidth]{./figures/ImpVol}
	}
	\subfigure[Price Surface]{
		\includegraphics[width=0.8\textwidth]{./figures/Price}
	}
	\caption{The Price Surface and Implied Volatility Surface for 2012-01-04 SP500 index option}
\end{figure}


\begin{figure}[htp!]
	\centering
	\resizebox{0.65\textwidth}{!}{
		\begin{tikzpicture}[
		prod/.style={circle, draw, inner sep=0pt},
		ct/.style={circle, draw, inner sep=5pt, ultra thick, minimum width=10mm},
		ft/.style={circle, draw, minimum width=8mm, inner sep=1pt},
		filter/.style={circle, draw, minimum width=7mm, inner sep=1pt, path picture={\draw[thick, rounded corners] (path picture bounding box.center)--++(65:2mm)--++(0:1mm);
				\draw[thick, rounded corners] (path picture bounding box.center)--++(245:2mm)--++(180:1mm);}},
		mylabel/.style={font=\scriptsize\sffamily},
		>=LaTeX
		]
		\node[draw,rectangle]  (s1) at (4.5, -1) {softmax};
		\node[draw,rectangle]  (s3) at (7.5, -1) {softmax};
		\node [draw,circle,inner sep=0pt] (rp1) at (3*1, -1) {$\times$};
		\node  (rp2) at (3*2, -1.5) {$\dots$};
		\node [draw,circle,inner sep=0pt] (rp3) at (3*3, -1) {$\times$};
		
		\foreach \i [count=\step from 1] in {$\vx_1^{t_i}$,$\dots$,$\vx_K^{t_i}$}
		\node (ri\step) at (3*\step, -2) {\i};
		\node  (sw1) at (4.5, -2) {SW};
		\node  (sw3) at (7.5, -2) {SW};
		\draw[->] (s1.west) to node[below]{$\mathbf{\alpha}$} (rp1.east);
		\draw[->] (s3.east) to node[below]{$\mathbf{\alpha}$} (rp3.west);
		\draw[->] (ri1.north) to (rp1.south);
		\draw[->] (ri3.north) to (rp3.south);
		\draw[->] (sw1.north) to (s1.south);
		\draw[->] (sw3.north) to (s3.south);
		\node (h2) at (3*2, 0.0) {$\dots$};
		\foreach \step in {1,3} {
			\node[draw,rectangle] (h\step) at (3*\step, 0.0) {GRU};
		}
		\draw[->] (rp1.north) to node[left]{$\widehat{\vx}_1^{t_i}$} (h1.south);
		\draw[->] (rp3.north) to node[left]{$\widehat{\vx}_K^{t_i}$} (h3.south);
		
		\foreach \step in {1,...,2} {
			\pgfmathtruncatemacro{\next}{add(\step,1)}
			\draw[->] (h\step.east) -> (h\next.west);
		}
		\node[fit=(ri1) (ri3) (s1) (s3) (h1) (h3), draw, inner sep=0pt] (fit4) { };
		\node[align=center, outer sep=0] (encoder) [right=of fit4] {Hedging \\Encoder};
		\node[draw,circle,inner sep=0pt]  (l1) at (6, 2) {$\times$};
		\node[draw,rectangle]  (l3) at (6, 1) {softmax};
		\node  (l2) at (4.5, 2) {$\vy_{t_i}$};
		\node  (l4) at (4.5, 1) {$LW$};
		\draw[->] (l4.east) to (l3.west);
		\draw[->] (l2.east) to (l1.west);
		\draw[->] (l3.north) to node[left]{$\omega$} (l1.south);
		\node (he2) at (6, 3) {$\widehat{\vy}_{t_i}$};
		\node (he3) at (9, 3) {$\widehat{\mathbf{h}}_E^{t_i}$};
		
		\node[draw,rectangle]  (hedge) at (7.5, 4) {NN};
		\node[filter](output) at (7.5, 5) {};
		\node[draw,rectangle] (loss) at (7.5, 7) {Loss};
		\node (V0) at (7.5, 8) {$V_{t_0}$};
		\node  (outputPre) at (6, 5) {...};
		\node  (outputNext) at (9, 5) {...};
		\node  (hedgePre) at (6, 4) {...};
		\node (hedgeNext) at (9, 4) {...};
		
	
		\node[draw,rectangle]  (hedge0) at (4.5, 4) {NN};
		\node[filter](output0) at (4.5, 5) {};
		\node[draw,rectangle]  (hedgeF) at (10.5, 4) {NN};
		\node[filter](outputF) at (10.5, 5) {};
		\draw[->] (l1.north) -> (he2.south);
		\draw[->] (h3.north) -> (he3.south);
		\draw[->] (he2.north) -> (hedge.south);
		\draw[->] (he3.north) -> (hedge.south);
		\draw[->] (V0.south) -> (loss.north);
	
		\draw[->] (hedge.north) to node[left]{${\mathbf{c}}_{t_i}$} (output.south);
		\draw[->] (hedge.north) to node[left]{${\mathbf{c}}_{t_i}$} (output.south);
		
		\draw[->] (hedge0.north) to node[left]{${\mathbf{c}}_{t_0}$} (output0.south);
		\draw[->] (hedgeF.north) to node[left]{${\mathbf{c}}_{t_{N-1}}$} (outputF.south);
	
		\node[fit=(hedge) (hedge0) (hedgeF) (output0) (output) (outputF), draw, inner sep=0pt] (fit5) { };
		\node[align=center, outer sep=0] (encoder) [right=of fit5] {Hedging\\ Decoder};
		\node[rectangle]  (delta0) at (4.5, 6) {$\delta_{t_0}$};
		\node[rectangle]  (deltaP) at (6, 6) {...};
		\node[rectangle]  (delta) at (7.5, 6) {$\delta_{t_i}$};
		\node[rectangle]  (deltaN) at (9, 6) {...};
		\node[rectangle]  (deltaF) at (10.5, 6) {$\delta_{t_{N-1}}$};
		\draw[->] (delta0.north) -> (loss.south);
		\draw[->] (deltaP.north) -> (loss.south);
		\draw[->] (delta.north) -> (loss.south);
		\draw[->] (deltaN.north) -> (loss.south);
		\draw[->] (deltaF.north) -> (loss.south);
		
		\draw[->] (output0.north) -> (delta0.south);
		\draw[->] (output.north) -> (delta.south);
		\draw[->] (outputF.north) -> (deltaF.south);
		
		\draw [->]  (deltaP.east) to[out=0,in=180] (hedge.west);
		\draw [->]  (delta0.east) to[out=0,in=180] (hedgePre.west);
		\draw [->]  (delta.east) to[out=0,in=180] (hedgeNext.west);	
		\draw [->]  (deltaN.east) to[out=0,in=180] (hedgeF.west);			
		\end{tikzpicture}
	}
	\caption{Planned Model}
	\label{fig:RNNModel}
\end{figure}


\section{Total Discrete Hedging Performance Comparison Using  S\&P 500 index Options}
\subsection{Call Option}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[htp!]
	\centering
    \begin{tabular}{ll|l|l|l|}
    \cline{3-5}
                                                          &          & Near-The-Money   & In-The-Money     & Out-of-The-Money \\ \hline
    \multicolumn{1}{|l|}{\multirow{4}{*}{Mean abs Error}} & Total    & \textbf{0.1927}  & \textbf{0.0571}  & \textbf{0.7344}  \\  
    \multicolumn{1}{|l|}{}                                & Local    & 0.2250           & 0.0866           & 0.8285           \\  
    \multicolumn{1}{|l|}{}                                & Bartlett & 0.2347           & 0.0641           & 0.7383           \\  
    \multicolumn{1}{|l|}{}                                & BS       & 0.2198           & 0.0531           & 0.9706           \\ \hline
    \multicolumn{1}{|l|}{\multirow{4}{*}{VaR (95\%)}}     & Total    & \textbf{-0.2827} & \textbf{-0.1121} & \textbf{-0.5298} \\  
    \multicolumn{1}{|l|}{}                                & Local    & -0.3622          & -0.1806          & -0.5753          \\  
    \multicolumn{1}{|l|}{}                                & Bartlett & -0.4836          & -0.1656          & -0.9841          \\  
    \multicolumn{1}{|l|}{}                                & BS       & -0.4823          & -0.1523          & -0.8603          \\ \hline
    \multicolumn{1}{|l|}{\multirow{4}{*}{CVaR (95\%)}}    & Total    & \textbf{-0.4721} & \textbf{-0.1865} & \textbf{-1.0003} \\  
    \multicolumn{1}{|l|}{}                                & Local    & -0.5643          & -0.2081          & -1.1673          \\  
    \multicolumn{1}{|l|}{}                                & Bartlett & -0.7103          & -0.2192          & -1.4232          \\  
    \multicolumn{1}{|l|}{}                                & BS       & -0.7009          & -0.2724          & -1.2299          \\ \hline
    \multicolumn{1}{|l|}{\multirow{4}{*}{VaR (99\%)}}     & Total    & \textbf{-0.5301} & \textbf{-0.1976} & -1.5077          \\  
    \multicolumn{1}{|l|}{}                                & Local    & -0.6361          & -0.2168          & -1.3583          \\  
    \multicolumn{1}{|l|}{}                                & Bartlett & -0.778           & -0.2654          & -1.6152          \\  
    \multicolumn{1}{|l|}{}                                & BS       & -0.7171          & -0.3653          & \textbf{-1.3363} \\ \hline
    \multicolumn{1}{|l|}{\multirow{4}{*}{CVaR (99\%)}}    & Total    & -0.8205          & \textbf{-0.3261} & -1.6090          \\  
    \multicolumn{1}{|l|}{}                                & Local    & \textbf{-0.7942} & -0.2301          & -2.1206          \\  
    \multicolumn{1}{|l|}{}                                & Bartlett & -1.0827          & -0.2883          & -2.1225          \\  
    \multicolumn{1}{|l|}{}                                & BS       & -1.0040          & -0.4712          & \textbf{-1.6074} \\ \hline
    \end{tabular}
	\caption{Summary of Weekly Hedging Call Options} \label{table:CallTotalW}
    \end{table}


\begin{table}[htp!]
	\centering
    \begin{tabular}{ll|l|l|l|}
    \cline{3-5}
                                                          &          & Near-The-Money   & In-The-Money     & Out-of-The-Money \\ \hline
    \multicolumn{1}{|l|}{\multirow{4}{*}{Mean abs Error}} & Total    & \textbf{0.2643}  & \textbf{0.0633}  & 1.0479           \\  
    \multicolumn{1}{|l|}{}                                & Local    & 0.2740           & 0.0642           & 1.2255           \\  
    \multicolumn{1}{|l|}{}                                & Bartlett & 0.282            & 0.073            & \textbf{0.9674}  \\  
    \multicolumn{1}{|l|}{}                                & BS       & 0.2865           & 0.0655           & 1.3248           \\ \hline
    \multicolumn{1}{|l|}{\multirow{4}{*}{VaR (95\%)}}     & Total    & -0.4102          & -0.1472          & -1.0842          \\  
    \multicolumn{1}{|l|}{}                                & Local    & \textbf{-0.3998} & \textbf{-0.1394} & \textbf{-0.9531} \\  
    \multicolumn{1}{|l|}{}                                & Bartlett & -0.4775          & -0.1611          & -1.1936          \\  
    \multicolumn{1}{|l|}{}                                & BS       & -0.5115          & -0.1554          & -1.3442          \\ \hline
    \multicolumn{1}{|l|}{\multirow{4}{*}{CVaR (95\%)}}    & Total    & \textbf{-0.6073} & \textbf{-0.3125} & \textbf{-1.6658} \\  
    \multicolumn{1}{|l|}{}                                & Local    & -0.6671          & -0.3144          & -1.6962          \\  
    \multicolumn{1}{|l|}{}                                & Bartlett & -0.6680          & -0.3372          & -2.0221          \\  
    \multicolumn{1}{|l|}{}                                & BS       & -0.9735          & -0.4380          & -2.0016          \\ \hline
    \multicolumn{1}{|l|}{\multirow{4}{*}{VaR (99\%)}}     & Total    & -0.7752          & \textbf{-0.4300} & \textbf{-1.7567} \\  
    \multicolumn{1}{|l|}{}                                & Local    & -0.8703          & -0.4329          & -1.9142          \\  
    \multicolumn{1}{|l|}{}                                & Bartlett & \textbf{-0.7201} & -0.4815          & -2.3799          \\  
    \multicolumn{1}{|l|}{}                                & BS       & -1.2384          & -0.6058          & -2.8419          \\ \hline
    \multicolumn{1}{|l|}{\multirow{4}{*}{CVaR (99\%)}}    & Total    & -0.8692          & \textbf{-0.4627} & -2.7536          \\  
    \multicolumn{1}{|l|}{}                                & Local    & -1.0861          & -0.4670          & \textbf{-2.5194} \\  
    \multicolumn{1}{|l|}{}                                & Bartlett & \textbf{-0.8090}  & -0.5725          & -2.8797          \\  
    \multicolumn{1}{|l|}{}                                & BS       & -1.2864          & -0.7859          & -3.0839          \\ \hline
    \end{tabular}
	\caption{Summary of Monthly Hedging Call Options} \label{table:CallTotalM}
    \end{table}



\subsection{Put Option}
This is section for Put option


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[htp!]
	\centering
	\begin{tabular}{ll|l|l|l|}
	\cline{3-5}
														  &          & Near-The-Money   & In-The-Money     & Out-of-The-Money  \\ \hline
	\multicolumn{1}{|l|}{\multirow{4}{*}{Mean abs Error}} & Total    & 0.2535           & \textbf{0.0965}  & \textbf{1.5356}   \\ 
	\multicolumn{1}{|l|}{}                                & Local    & \textbf{0.2505}  & 0.1109           & 1.5424            \\ 
	\multicolumn{1}{|l|}{}                                & Bartlett & 0.2516           & 0.1140           & 1.6042            \\ 
	\multicolumn{1}{|l|}{}                                & BS       & 0.2763           & 0.1232           & 1.7074            \\ \hline
	\multicolumn{1}{|l|}{\multirow{4}{*}{VaR (95\%)}}     & Total    & -0.8124          & \textbf{-0.2364} & \textbf{-7.2478}  \\ 
	\multicolumn{1}{|l|}{}                                & Local    & \textbf{-0.7888} & -0.2995          & -8.0562           \\ 
	\multicolumn{1}{|l|}{}                                & Bartlett & -0.8229          & -0.3160          & -8.0506           \\  
	\multicolumn{1}{|l|}{}                                & BS       & -0.8683          & -0.4262          & -8.7359           \\ \hline
	\multicolumn{1}{|l|}{\multirow{4}{*}{CVaR (95\%)}}    & Total    & \textbf{-1.0475} & \textbf{-0.3452} & \textbf{-10.9438} \\ 
	\multicolumn{1}{|l|}{}                                & Local    & -1.2335          & -0.5405          & -11.8778          \\  
	\multicolumn{1}{|l|}{}                                & Bartlett & -1.2775          & -0.5981          & -13.2250          \\  
	\multicolumn{1}{|l|}{}                                & BS       & -1.4724          & -0.7182          & -13.3229          \\ \hline
	\multicolumn{1}{|l|}{\multirow{4}{*}{VaR (99\%)}}     & Total    & \textbf{-1.1138} & \textbf{-0.3763} & \textbf{-11.7573} \\  
	\multicolumn{1}{|l|}{}                                & Local    & -1.4584          & -0.6686          & -13.2533          \\ 
	\multicolumn{1}{|l|}{}                                & Bartlett & -1.4361          & -0.7996          & -14.5369          \\  
	\multicolumn{1}{|l|}{}                                & BS       & -1.7616          & -0.7969          & -19.0709          \\ \hline
	\multicolumn{1}{|l|}{\multirow{4}{*}{CVaR(99\%)}}     & Total    & \textbf{-1.3597} & \textbf{-0.4616} & \textbf{-15.1555} \\  
	\multicolumn{1}{|l|}{}                                & Local    & -1.7104          & -0.8854          & -16.1956          \\ 
	\multicolumn{1}{|l|}{}                                & Bartlett & -1.7732          & -0.9739          & -17.0642          \\ 
	\multicolumn{1}{|l|}{}                                & BS       & -2.2754          & -1.1338          & -20.6381          \\ \hline
	\end{tabular}
	\caption{Summary of Weekly Hedging Put Options} \label{table:putTotalW}
	\end{table}

\newpage
\begin{figure}[htp!]
	\centering

	\subfigure[]{
	   \includegraphics[width=0.32\textwidth]{./figures/LocalVersusTotalNTM.pdf}}
	\subfigure[]{
		\includegraphics[width=0.32\textwidth]{./figures/LocalVersusTotalITM.pdf}}
	\subfigure[]{
	   \includegraphics[width=0.32\textwidth]{./figures/LocalVersusTotalOTM.pdf}}
	\caption{Comparing Total Model and Local Model on Weekly Hedging Put Options} \label{fig:putTotalW1}
	\end{figure}



	\begin{figure}[htp!]
		\centering
		\subfigure[]{
		   \includegraphics[width=0.32\textwidth]{./figures/BartlettVersusTotalNTM.pdf}}
		\subfigure[]{
			\includegraphics[width=0.32\textwidth]{./figures/BartlettVersusTotalITM.pdf}}
		\subfigure[]{
				\includegraphics[width=0.32\textwidth]{./figures/BartlettVersusTotalOTM.pdf}}
		\caption{Comparing Total Model and Bartlett Model on Weekly Hedging Put Options} \label{fig:putTotalW2}
		\end{figure}

		\begin{figure}[htp!]
			\centering
	
			\subfigure[]{
			   \includegraphics[width=0.32\textwidth]{./figures/BSVersusTotalNTM.pdf}}
			\subfigure[]{
				\includegraphics[width=0.32\textwidth]{./figures/BSVersusTotalITM.pdf}}
				\subfigure[]{
					\includegraphics[width=0.32\textwidth]{./figures/BSVersusTotalOTM.pdf}}
			\caption{Comparing Total Model and Black–Scholes Model on Weekly Hedging Put Options} \label{fig:putTotalW3}
			\end{figure}



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[htp!]
	\centering
	\begin{tabular}{ll|l|l|l|}
	\cline{3-5}
														  &          & Near-The-Money   & In-The-Money     & Out-of-The-Money  \\ \hline
	\multicolumn{1}{|l|}{\multirow{4}{*}{Mean abs Error}} & Total    & \textbf{0.2986}  & \textbf{0.1240}  & 1.7639            \\  
	\multicolumn{1}{|l|}{}                                & Local    & 0.3485           & 0.1394           & \textbf{1.6849}   \\  
	\multicolumn{1}{|l|}{}                                & Bartlett & 0.3205           & 0.1583           & 1.7383            \\  
	\multicolumn{1}{|l|}{}                                & BS       & 0.3224           & 0.1342           & 1.8482            \\ \hline
	\multicolumn{1}{|l|}{\multirow{4}{*}{VaR (95\%)}}     & Total    & \textbf{-0.7395} & \textbf{-0.2562} & -8.5602           \\  
	\multicolumn{1}{|l|}{}                                & Local    & -0.7558          & -0.3268          & \textbf{-8.1812}  \\  
	\multicolumn{1}{|l|}{}                                & Bartlett & -0.8370          & -0.3583          & -9.0303           \\  
	\multicolumn{1}{|l|}{}                                & BS       & -0.7768          & -0.3088          & -9.7018           \\ \hline
	\multicolumn{1}{|l|}{\multirow{4}{*}{CVaR (95\%)}}    & Total    & -1.7761          & \textbf{-0.3577} & \textbf{-13.3160} \\  
	\multicolumn{1}{|l|}{}                                & Local    & -1.8144          & -0.4898          & -14.6857          \\  
	\multicolumn{1}{|l|}{}                                & Bartlett & \textbf{-1.5710} & -0.6016          & -14.4425          \\  
	\multicolumn{1}{|l|}{}                                & BS       & -1.8682          & -0.5401          & -16.1024          \\ \hline
	\multicolumn{1}{|l|}{\multirow{4}{*}{VaR (99\%)}}     & Total    & -2.1792          & \textbf{-0.4121} & -15.2323          \\  
	\multicolumn{1}{|l|}{}                                & Local    & \textbf{-2.1577} & -0.6454          & -16.5192          \\  
	\multicolumn{1}{|l|}{}                                & Bartlett & -2.9164          & -0.7728          & \textbf{-15.0144} \\  
	\multicolumn{1}{|l|}{}                                & BS       & -2.5569          & -0.7583          & -15.8393          \\ \hline
	\multicolumn{1}{|l|}{\multirow{4}{*}{CVaR (99\%)}}    & Total    & -3.4001          & \textbf{0.4509}  & \textbf{-20.6503} \\  
	\multicolumn{1}{|l|}{}                                & Local    & -3.3717          & -0.6910          & -21.7928          \\  
	\multicolumn{1}{|l|}{}                                & Bartlett & \textbf{-2.9164} & -0.8463          & -24.1757          \\  
	\multicolumn{1}{|l|}{}                                & BS       & -3.5486          & -0.8109          & -26.4941          \\ \hline
	\end{tabular}
	\caption{Summary of Monthly Hedging Put Options}
	\label{table:putTotalM}
	\end{table}


\begin{figure}[htp!]
	\centering

	\subfigure[]{
	   \includegraphics[width=0.32\textwidth]{./figures/MLocalVersusTotalNTM.pdf}}
	\subfigure[]{
		\includegraphics[width=0.32\textwidth]{./figures/MLocalVersusTotalITM.pdf}}
	\subfigure[]{
	   \includegraphics[width=0.32\textwidth]{./figures/MLocalVersusTotalOTM.pdf}}
	\caption{Comparing Total Model and Local Model on Monthly Hedging Put Options} \label{fig:putTotalM1}
	\end{figure}



	\begin{figure}[htp!]
		\centering
		\subfigure[]{
		   \includegraphics[width=0.32\textwidth]{./figures/MBartlettVersusTotalNTM.pdf}}
		\subfigure[]{
			\includegraphics[width=0.32\textwidth]{./figures/MBartlettVersusTotalITM.pdf}}
		\subfigure[]{
				\includegraphics[width=0.32\textwidth]{./figures/BartlettVersusTotalOTM.pdf}}
		\caption{Comparing Total Model and Bartlett Model on Monthly Hedging Put Options} \label{fig:putTotalM2}
		\end{figure}

		\begin{figure}[htp!]
			\centering
	
			\subfigure[]{
			   \includegraphics[width=0.32\textwidth]{./figures/MBSVersusTotalNTM.pdf}}
			\subfigure[]{
				\includegraphics[width=0.32\textwidth]{./figures/MBSVersusTotalITM.pdf}}
				\subfigure[]{
					\includegraphics[width=0.32\textwidth]{./figures/MBSVersusTotalOTM.pdf}}
			\caption{Comparing Total Model and Black–Scholes Model on Monthly Hedging Put Options} \label{fig:putTotalM3}
			\end{figure}
\chapter{Conclusion and Future Work}


\label{sec:Conclusion}
%----------------------------------------------------------------------
% END MATERIAL
%----------------------------------------------------------------------

% B I B L I O G R A P H Y
% -----------------------

% The following statement selects the style to use for references.  It controls the sort order of the entries in the bibliography and also the formatting for the in-text labels.
\bibliographystyle{plainnat}
% This specifies the location of the file containing the bibliographic information.
% It assumes you're using BibTeX (if not, why not?).
\cleardoublepage % This is needed if the book class is used, to place the anchor in the correct page,
                 % because the bibliography will start on its own page.
                 % Use \clearpage instead if the document class uses the "oneside" argument
\phantomsection  % With hyperref package, enables hyperlinking from the table of contents to bibliography
% The following statement causes the title "References" to be used for the bibliography section:
\renewcommand*{\bibname}{References}

% Add the References to the Table of Contents
\addcontentsline{toc}{chapter}{\textbf{References}}

\bibliography{uw-ethesis}
% Tip 5: You can create multiple .bib files to organize your references.
% Just list them all in the \bibliogaphy command, separated by commas (no spaces).

% The following statement causes the specified references to be added to the bibliography% even if they were not
% cited in the text. The asterisk is a wildcard that causes all entries in the bibliographic database to be included (optional).
\nocite{*}

% The \appendix statement indicates the beginning of the appendices.
\appendix
% Add a title page before the appendices and a line in the Table of Contents
\chapter*{APPENDICES}

\section{Proposed Model For Synthetic Experiments}
In this section, we propose a model that is mainly used for synthetic cases. We assume that we are rebalance the hedging portfolio at discrete time periods.

For a rebalance time steps, $t_i$, we assume the following features are available:
\begin{itemize}
  \item Local features: $\vy_{t_i}$ which records the information for current time steps. Typical features can the: the current underlying $S_{t_i}$. time to expiry $\tau$, and  moneyness $S_{t_i}$
  \item Sequential features $\mathbf{X}_{t_i}=\{x^{t_i}_1,\dots,x^{t_i}_K \}$: which records information in between the two adjacent rebalancing time periods. For example, if we rebalance every 20 business days. Then the time difference between two adjacent rebalancing time periods is $\Delta t= t_i-t_{i-1}=20/250$. $\mathbf{X}_{t_i}=\{\vx^{t_i}_1,\dots,\vx^{t_i}_K \}$ will record the price changes of underlying of the 20 business days. In this case K=20. At the initial rebalance time step $t_0$, $X_{t_0}$ is the sequential feature set recording the past history before $t_0$.
\end{itemize}
We again have a encoder to encode the information contained in  $X_{t_i}$ as $\widehat{\mathbf{h}}_E^{t_i}$. The overall structure is similar to what we have for local hedging cases. However, the decoder now is a RNN network too. The hidden states from previous relancing time $c_{t_{i-1}}$ is passed to current relancing time. The initial option price is computed in the following ways:
\begin{itemize}
	\item  $V_{t_0}$ can be supplied as one the parameter and optimized together with the parameters in the hedging position function.
	\item $V_{t_0}$ can be given by another NN or RNN model based on features we supplied. The option pricing function is optimized together with the the hedging position function.
\end{itemize}

\begin{figure}[htp!]
	\centering
	\resizebox{0.65\textwidth}{!}{
		\begin{tikzpicture}[
		prod/.style={circle, draw, inner sep=0pt},
		ct/.style={circle, draw, inner sep=5pt, ultra thick, minimum width=10mm},
		ft/.style={circle, draw, minimum width=8mm, inner sep=1pt},
		filter/.style={circle, draw, minimum width=7mm, inner sep=1pt, path picture={\draw[thick, rounded corners] (path picture bounding box.center)--++(65:2mm)--++(0:1mm);
				\draw[thick, rounded corners] (path picture bounding box.center)--++(245:2mm)--++(180:1mm);}},
		mylabel/.style={font=\scriptsize\sffamily},
		>=LaTeX
		]
\node[draw,rectangle]  (s1) at (4.5, -1) {softmax};
		\node[draw,rectangle]  (s3) at (7.5, -1) {softmax};
		\node [draw,circle,inner sep=0pt] (rp1) at (3*1, -1) {$\times$};
		\node  (rp2) at (3*2, -1.5) {$\dots$};
		\node [draw,circle,inner sep=0pt] (rp3) at (3*3, -1) {$\times$};
		
		\foreach \i [count=\step from 1] in {$\vx_1^{t_i}$,$\dots$,$\vx_K^{t_i}$}
		\node (ri\step) at (3*\step, -2) {\i};
		\node  (sw1) at (4.5, -2) {SW};
		\node  (sw3) at (7.5, -2) {SW};
		\draw[->] (s1.west) to node[below]{$\mathbf{\alpha}$} (rp1.east);
		\draw[->] (s3.east) to node[below]{$\mathbf{\alpha}$} (rp3.west);
		\draw[->] (ri1.north) to (rp1.south);
		\draw[->] (ri3.north) to (rp3.south);
		\draw[->] (sw1.north) to (s1.south);
		\draw[->] (sw3.north) to (s3.south);
		\node (h2) at (3*2, 0.0) {$\dots$};
		\foreach \step in {1,3} {
			\node[draw,rectangle] (h\step) at (3*\step, 0.0) {GRU};
		}
		\draw[->] (rp1.north) to node[left]{$\widehat{\vx}_1^{t_i}$} (h1.south);
		\draw[->] (rp3.north) to node[left]{$\widehat{\vx}_K^{t_i}$} (h3.south);
				
		\foreach \step in {1,...,2} {
			\pgfmathtruncatemacro{\next}{add(\step,1)}
			\draw[->] (h\step.east) -> (h\next.west);
		}
		\node[fit=(ri1) (ri3) (s1) (s3) (h1) (h3), draw, inner sep=0pt] (fit4) { };
        \node[align=center, outer sep=0] (encoder) [right=of fit4] {Hedging \\Encoder};
		\node[draw,circle,inner sep=0pt]  (l1) at (6, 2) {$\times$};
		\node[draw,rectangle]  (l3) at (6, 1) {softmax};
		\node  (l2) at (4.5, 2) {$\vy_{t_i}$};
		\node  (l4) at (4.5, 1) {$LW$};
		\draw[->] (l4.east) to (l3.west);
		\draw[->] (l2.east) to (l1.west);
		\draw[->] (l3.north) to node[left]{$\omega$} (l1.south);
		\node (he2) at (6, 3) {$\widehat{\vy}_{t_i}$};
		\node (he3) at (9, 3) {$\widehat{\mathbf{h}}_E^{t_i}$};

		\node[draw,rectangle]  (hedge) at (7.5, 4) {NN};
        \node[filter](output) at (7.5, 5) {};
        \node[draw,rectangle] (loss) at (7.5, 7) {Loss};
        \node (V0) at (7.5, 8) {$V_{t_0}$};
        \node  (outputPre) at (6, 5) {...};
        \node  (outputNext) at (9, 5) {...};
        \node  (hedgePre) at (6, 4) {...};
        \node (hedgeNext) at (9, 4) {...};
        \node[draw,rectangle]  (hedge0) at (4.5, 4) {NN};
        \node[filter](output0) at (4.5, 5) {};
        \node[draw,rectangle]  (hedgeF) at (10.5, 4) {NN};
        \node[filter](outputF) at (10.5, 5) {};
\draw[->] (l1.north) -> (he2.south);
\draw[->] (h3.north) -> (he3.south);
\draw[->] (he2.north) -> (hedge.south);
\draw[->] (he3.north) -> (hedge.south);
\draw[->] (V0.south) -> (loss.north);
\draw[->] (hedge0.east) -> (hedgePre.west);
\draw[->] (hedgeNext.east) -> (hedgeF.west);

\draw[->] (hedge.north) to node[left]{${\mathbf{c}}_{t_i}$} (output.south);
\draw[->] (hedge.north) to node[left]{${\mathbf{c}}_{t_i}$} (output.south);

\draw[->] (hedge0.north) to node[left]{${\mathbf{c}}_{t_0}$} (output0.south);
\draw[->] (hedgeF.north) to node[left]{${\mathbf{c}}_{t_{N-1}}$} (outputF.south);

\draw[->] (hedgePre.east) to node[above]{${\mathbf{c}}_{t_{i-1}}$} (hedge.west);
\draw[->] (hedge.east) to node[above]{${\mathbf{c}}_{t_{i+1}}$}(hedgeNext.west);

\node[fit=(hedge) (hedge0) (hedgeF) (output0) (output) (outputF), draw, inner sep=0pt] (fit5) { };
\node[align=center, outer sep=0] (encoder) [right=of fit5] {Hedging\\ Decoder};
 \node[rectangle]  (delta0) at (4.5, 6) {$\delta_{t_0}$};
 \node[rectangle]  (deltaP) at (6, 6) {...};
 \node[rectangle]  (delta) at (7.5, 6) {$\delta_{t_i}$};
 \node[rectangle]  (deltaN) at (9, 6) {...};
 \node[rectangle]  (deltaF) at (10.5, 6) {$\delta_{t_{N-1}}$};
\draw[->] (delta0.north) -> (loss.south);
\draw[->] (deltaP.north) -> (loss.south);
\draw[->] (delta.north) -> (loss.south);
\draw[->] (deltaN.north) -> (loss.south);
\draw[->] (deltaF.north) -> (loss.south);

\draw[->] (output0.north) -> (delta0.south);
\draw[->] (output.north) -> (delta.south);
\draw[->] (outputF.north) -> (deltaF.south);		
\end{tikzpicture}
	}
\caption{Proposed Model For Synthetic Cases}
	\label{fig:RNNModel}
\end{figure}

For synthetic data, we can fix the dynamic of the underlying and generate training and testing paths based on the same dynamic. We then train and test the total hedging performance of the model. We will compare the performance of the model with the performance of using analytical delta and BS delta with implied volatility. Furthermore, we will test the model with and without the hedging encoder.

In \cite{coleman2007total}, the authors proposed that they can achieve better or similar hedging performance using quadratic or linear risk measures. At each rebalancing time, there is a separate function computing the hedging position based on the current asset price $S_{t_i}$ and past history $S^H_{t_i}=[S_{t_0},\dots,S_{t_{i-1}}]$ :
\[
\delta_{t_i}=f_{t_i}(S_{t_i},S^H_{t_i})
\]
The current proposed model is different from the model in \cite{coleman2007total} in the following ways:
\begin{enumerate}
  \item Hedging position not only depends on the asset price. The feature we includes can contain many other features
  \item A single function is used for all hedging periods.
  That is to say:
  \[
  \delta_{t_i}=f(\mathbf{X}_{t_j},\vy_{t_j})
  \]
  \item Note that, time to expire is also one of the features, so potentially, $f(\mathbf{X}_{t_j},\vy_{t_j})$ can replace the roles of $f_{t_i}(S_{t_i},S^H_{t_i})$.
\end{enumerate}

From \cite{coleman2007total}, we can already know the importance of using the entire path of stock instead of the current stock in determining the hedging position. Even the analytical formula for quadratic formulation clearly demonstrate the dependence of the current hedging position on the past information.
Therefore, we want to demonstrate the following things:

\begin{figure}[htp!]
	\centering
	\resizebox{0.90\textwidth}{!}{
		\begin{tikzpicture}[
		prod/.style={circle, draw, inner sep=0pt},
		ct/.style={circle, draw, inner sep=5pt, ultra thick, minimum width=10mm},
		ft/.style={circle, draw, minimum width=8mm, inner sep=1pt},
		filter/.style={circle, draw, minimum width=7mm, inner sep=1pt, path picture={\draw[thick, rounded corners] (path picture bounding box.center)--++(65:2mm)--++(0:1mm);
				\draw[thick, rounded corners] (path picture bounding box.center)--++(245:2mm)--++(180:1mm);}},
		mylabel/.style={font=\scriptsize\sffamily},
		>=LaTeX
		]
		\node[draw,rectangle]  (ps1) at (4.5-8, -1) {softmax};
		\node[draw,rectangle]  (ps3) at (7.5-8, -1) {softmax};
		\node [draw,circle,inner sep=0pt] (prp1) at (3*1-8, -1) {$\times$};
		\node  (prp2) at (3*2-8, -1.5) {$\dots$};
		\node [draw,circle,inner sep=0pt] (prp3) at (3*3-8, -1) {$\times$};
		
		\foreach \i [count=\step from 1] in {$\vx_1^P$,$\dots$,$\vx_O^P$}
		\node (pri\step) at (3*\step-8, -2) {\i};
		\node  (psw1) at (4.5-8, -2) {$SW_P$};
		\node  (psw3) at (7.5-8, -2) {$SW_P$};
		\draw[->] (ps1.west) to node[below]{$\mathbf{\beta}$} (prp1.east);
		\draw[->] (ps3.east) to node[below]{$\mathbf{\beta}$} (prp3.west);
		\draw[->] (pri1.north) to (prp1.south);
		\draw[->] (pri3.north) to (prp3.south);
		\draw[->] (psw1.north) to (ps1.south);
		\draw[->] (psw3.north) to (ps3.south);
		\node (ph2) at (3*2-8, 0.0) {$\dots$};
		\foreach \step in {1,3} {
			\node[draw,rectangle] (ph\step) at (3*\step-8, 0.0) {GRU};
		}
		\draw[->] (prp1.north) to node[left]{$\widehat{\vx}_1^P$} (ph1.south);
		\draw[->] (prp3.north) to node[left]{$\widehat{\vx}_O^P$} (ph3.south);
				
		\foreach \step in {1,...,2} {
			\pgfmathtruncatemacro{\next}{add(\step,1)}
			\draw[->] (ph\step.east) -> (ph\next.west);
		}
		\node[fit=(pri1) (pri3) (ps1) (ps3) (ph1) (ph3), draw, inner sep=0pt] (fit1) { };
		\node[align=center, outer sep=0] (encoder) [left=of fit1] {Pricing \\ Encoder};
		
		
		
		\node[filter] (poe2) at (9-8, 4.5) {};
		\node[filter] (poe3) at (6-8, 4.5) {};
		\node [draw,circle,inner sep=0pt] (poe4) at (6-8, 5.5) {$\times$};
		\node [draw,circle,inner sep=0pt] (poe5) at (9-8, 5.5) {$\times$};
		\node [draw,circle,inner sep=0pt] (poe6) at (7.5-8, 6) {$+$};
		\node (poe7) at (7.5-8, 7) {$V^M_{t_0}$};
		\node  (pbs) at (4.5-8, 5.5) {$V^{mkt}_{t_0}$};
		\draw[->] (poe3.north) to node[left]{$1-o$} (poe4.south);
		\draw[->] (poe3.north) to node[above]{$o$} (poe5.west);
		\draw[->] (poe2.north) to node[left]{$\widehat{V}^M_{t_0}$} (poe5.south);
		\draw[->] (poe4.north) to node[left]{} (poe6.west);
		\draw[->] (poe5.north) to node[left]{} (poe6.east);
		\draw[->] (poe6.north) to node[left]{} (poe7.south);
		\draw[->] (pbs.east) to node[left]{} (poe4.west);
		
		
	    \draw[->] (poe7.east) to node[left]{} (loss.west);

		
		\node[draw,circle,inner sep=0pt]  (pl1) at (6-8, 2) {$\times$};
		\node[draw,rectangle]  (pl3) at (6-8, 1) {softmax};
		\node  (pl2) at (4.5-8, 2) {$\vy_P$};
		\node  (pl4) at (4.5-8, 1) {$LW_P$};
		\draw[->] (pl4.east) to (pl3.west);
		\draw[->] (pl2.east) to (pl1.west);
		\draw[->] (pl3.north) to node[left]{$\eta$} (pl1.south);
		
		
		\node (phe2) at (6-8, 3) {$\widehat{\vy}_P$};
		\node (phe3) at (9-8, 3) {$\widehat{\mathbf{h}}_E^P$};
		\draw[->] (pl1.north) to (phe2.south);
		\draw[->] (ph3.north) to  (phe3.south);
		\draw[->] (phe3.north) to  (poe2.south);
		\draw[->] (phe3.north) to  (poe3.south);
		
		\draw[->] (phe2.north) to (poe2.south);
		\draw[->] (phe2.north) to (poe3.south);
		
		\node[fit=(poe2) (poe3) (poe5) (poe6)  (pbs) (phe2) (phe3), draw, inner sep=0pt] (fit3) { };
		\node[align=center, outer sep=0] (encoder) [left=of fit3] {Pricing \\ Decoder};






		\node[draw,rectangle]  (s1) at (4.5, -1) {softmax};
		\node[draw,rectangle]  (s3) at (7.5, -1) {softmax};
		\node [draw,circle,inner sep=0pt] (rp1) at (3*1, -1) {$\times$};
		\node  (rp2) at (3*2, -1.5) {$\dots$};
		\node [draw,circle,inner sep=0pt] (rp3) at (3*3, -1) {$\times$};
		
		\foreach \i [count=\step from 1] in {$\vx_1^{t_i}$,$\dots$,$\vx_K^{t_i}$}
		\node (ri\step) at (3*\step, -2) {\i};
		\node  (sw1) at (4.5, -2) {SW};
		\node  (sw3) at (7.5, -2) {SW};
		\draw[->] (s1.west) to node[below]{$\mathbf{\alpha}$} (rp1.east);
		\draw[->] (s3.east) to node[below]{$\mathbf{\alpha}$} (rp3.west);
		\draw[->] (ri1.north) to (rp1.south);
		\draw[->] (ri3.north) to (rp3.south);
		\draw[->] (sw1.north) to (s1.south);
		\draw[->] (sw3.north) to (s3.south);
		\node (h2) at (3*2, 0.0) {$\dots$};
		\foreach \step in {1,3} {
			\node[draw,rectangle] (h\step) at (3*\step, 0.0) {GRU};
		}
		\draw[->] (rp1.north) to node[left]{$\widehat{\vx}_1^{t_i}$} (h1.south);
		\draw[->] (rp3.north) to node[left]{$\widehat{\vx}_K^{t_i}$} (h3.south);
				
		\foreach \step in {1,...,2} {
			\pgfmathtruncatemacro{\next}{add(\step,1)}
			\draw[->] (h\step.east) -> (h\next.west);
		}
		\node[fit=(ri1) (ri3) (s1) (s3) (h1) (h3), draw, inner sep=0pt] (fit4) { };
        \node[align=center, outer sep=0] (encoder) [right=of fit4] {Hedging \\Encoder};
		\node[draw,circle,inner sep=0pt]  (l1) at (6, 2) {$\times$};
		\node[draw,rectangle]  (l3) at (6, 1) {softmax};
		\node  (l2) at (4.5, 2) {$\vy_{t_i}$};
		\node  (l4) at (4.5, 1) {$LW$};
		\draw[->] (l4.east) to (l3.west);
		\draw[->] (l2.east) to (l1.west);
		\draw[->] (l3.north) to node[left]{$\omega$} (l1.south);
		\node (he2) at (6, 3) {$\widehat{\vy}_{t_i}$};
		\node (he3) at (9, 3) {$\widehat{\mathbf{h}}_E^{t_i}$};

		\node[draw,rectangle]  (hedge) at (7.5, 4) {GRU};
        \node[filter](output) at (7.5, 5) {};
        \node[draw,rectangle] (loss) at (7.5, 7) {Loss};
%        \node (V0) at (7.5, 8) {$V_{t_0}$};
        \node  (outputPre) at (6, 5) {...};
        \node  (outputNext) at (9, 5) {...};
        \node  (hedgePre) at (6, 4) {...};
        \node (hedgeNext) at (9, 4) {...};




        \node[draw,rectangle]  (hedge0) at (4.5, 4) {GRU};
        \node[filter](output0) at (4.5, 5) {};
        \node[draw,rectangle]  (hedgeF) at (10.5, 4) {GRU};
        \node[filter](outputF) at (10.5, 5) {};
\draw[->] (l1.north) -> (he2.south);
\draw[->] (h3.north) -> (he3.south);
\draw[->] (he2.north) -> (hedge.south);
\draw[->] (he3.north) -> (hedge.south);
%\draw[->] (V0.south) -> (loss.north);
\draw[->] (hedge0.east) -> (hedgePre.west);
\draw[->] (hedgeNext.east) -> (hedgeF.west);

\draw[->] (hedge.north) to node[left]{${\mathbf{c}}_{t_i}$} (output.south);
\draw[->] (hedge.north) to node[left]{${\mathbf{c}}_{t_i}$} (output.south);

\draw[->] (hedge0.north) to node[left]{${\mathbf{c}}_{t_0}$} (output0.south);
\draw[->] (hedgeF.north) to node[left]{${\mathbf{c}}_{t_{N-1}}$} (outputF.south);

\draw[->] (hedgePre.east) to node[above]{${\mathbf{c}}_{t_{i-1}}$} (hedge.west);
\draw[->] (hedge.east) to node[above]{${\mathbf{c}}_{t_{i+1}}$}(hedgeNext.west);

\node[fit=(hedge) (hedge0) (hedgeF) (output0) (output) (outputF), draw, inner sep=0pt] (fit5) { };
\node[align=center, outer sep=0] (encoder) [right=of fit5] {Hedging\\ Decoder};
 \node[rectangle]  (delta0) at (4.5, 6) {$\delta_{t_0}$};
 \node[rectangle]  (deltaP) at (6, 6) {...};
 \node[rectangle]  (delta) at (7.5, 6) {$\delta_{t_i}$};
 \node[rectangle]  (deltaN) at (9, 6) {...};
 \node[rectangle]  (deltaF) at (10.5, 6) {$\delta_{t_{N-1}}$};
\draw[->] (delta0.north) -> (loss.south);
\draw[->] (deltaP.north) -> (loss.south);
\draw[->] (delta.north) -> (loss.south);
\draw[->] (deltaN.north) -> (loss.south);
\draw[->] (deltaF.north) -> (loss.south);
\draw[->] (output0.north) -> (delta0.south);
\draw[->] (output.north) -> (delta.south);
\draw[->] (outputF.north) -> (deltaF.south);
	\end{tikzpicture}
	}
\caption{Refined Model For Real Cases}
	\label{fig:RNNModel}
\end{figure}

\section{Testing with Synthetic Data}

We have conducted primitive experiments with the Black-Scholes Model, Heston Model, GARCH model.

The input is $\vy_{t_i}=\{S_{t_i}/K,t,\delta_{BS}^{t_i}\}$. The previous output from the previous rebalancing time is also added as one of the input. In terms of performance, the performance of the model is also similar to the spline model proposed in \cite{coleman2007total} and the analytical solution of the total risk minimization. The model we proposed is the simple model in Figure 3.
The model we proposed is the simple model in Figure 3.
The evaluation criteria are same as in \cite{coleman2007total}:
\begin{itemize}
\item Total risk: Average of the absolute value of the final portfolio value:
\[ \begin{split}
|P_{t_N^-}|&=\left|e^{-r T}\left( e^{r \Delta t}B_{t_{N-1}}- V_{t_N}+ S_{t_N} \delta_{t_{N-1}} \right)\right| \\
&=\left|e^{-r T}\left(\sum_{j=0}^{N-1}\left\{ \left[e^{r (N-j-1) \Delta t} S_{t_{j+1}}-e^{r (N-j) \Delta t}S_{t_{j}}\right] \delta_{t_j} \right\}+e^{r N \Delta t} V_{t_0}-V_{t_N}\right)\right|
\end{split}
\]
\item Total cost: Average of the total cost in rebalancing the portfolio:
\[
e^{-r T}\left(V_{t_N}-\sum_{j=0}^{N-1}\left\{ \left[e^{r (N-j-1) \Delta t} S_{t_{j+1}}-e^{r (N-j) \Delta t}S_{t_{j}}\right] \delta_{t_j} \right\}\right)
\]
\end{itemize}

\subsection{Black-Scholes Model}
We adopt the same experimental setting as \cite{coleman2007total}. Specifically, we assume $S_0=100$,$K=100$, $T=1$, $\mu=0.15$,$r=0.04$, and $\sigma=0.2$.
We have 600 time steps with $\Delta t=1/600$. We can hedge every 25,50,100,300 time steps.
\begin{table}[htp!]
\centering
\begin{tabular}{|c|c|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  method       &25 &50&100&300 \\
  GRU          & 0.8448& 1.1845& 1.6518 & 2.7989 \\
  GRU (Refined)& 0.8376& 1.1426& 1.6896 & 2.8038 \\
  Spline       & 0.8563& 1.1789& 1.6518 & 2.7843 \\
  Analytical   & 0.8295& 1.1636& 1.6479 & 2.7914 \\
  Delta        & 0.9481&1.3385 & 1.9128 & 3.4582 \\
  \hline
\end{tabular}
\caption{Total Risk}
\end{table}


\begin{table}[htp!]
\centering
\begin{tabular}{|c|c|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  method &25 &50&100&300 \\
  GRU          & 5.8943& 5.8812& 5.7378 & 5.2507 \\
  GRU (Refined)& 5.9682& 5.8370& 5.8549 & 5.1759 \\
  Spline    & 5.9118& 5.8445& 5.7119 & 5.2530 \\
  Analytical& 5.9413& 5.8773& 5.7399 & 5.2565 \\
  Delta     & 6.0483& 6.0897& 6.1734 & 6.5382 \\
  \hline
\end{tabular}
\caption{Total Cost}
\end{table}
\addcontentsline{toc}{chapter}{APPENDICES}
%======================================================================


\end{document}
